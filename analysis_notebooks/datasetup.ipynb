{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "588754c4",
   "metadata": {},
   "source": [
    "# Colorado Oil Spills Analysis - 2020\n",
    "## Redux Attempt\n",
    "### Journal Aim: Energy Research and Social Science\n",
    "#### David P. Adams\n",
    "#### 2025 December 23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5060d7",
   "metadata": {},
   "source": [
    "## Environment & Dependencies\n",
    "This notebook expects a local virtual environment plus a `.env` file at the repo root.\n",
    "\n",
    "- Create `.env` by copying `.env.example` and filling values.\n",
    "- Install Python packages from `requirements.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffb4f943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas>=2.2 in /home/dadams/Repos/colorado_redux/.venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 2)) (2.3.3)\n",
      "Requirement already satisfied: numpy>=2.0 in /home/dadams/Repos/colorado_redux/.venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 3)) (2.4.0)\n",
      "Requirement already satisfied: geopandas>=1.0 in /home/dadams/Repos/colorado_redux/.venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 6)) (1.1.2)\n",
      "Requirement already satisfied: shapely>=2.0 in /home/dadams/Repos/colorado_redux/.venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 7)) (2.1.2)\n",
      "Requirement already satisfied: pyproj>=3.6 in /home/dadams/Repos/colorado_redux/.venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 8)) (3.7.2)\n",
      "Requirement already satisfied: fiona>=1.10 in /home/dadams/Repos/colorado_redux/.venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 9)) (1.10.1)\n",
      "Requirement already satisfied: SQLAlchemy>=2.0 in /home/dadams/Repos/colorado_redux/.venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 12)) (2.0.45)\n",
      "Requirement already satisfied: psycopg>=3.2 in /home/dadams/Repos/colorado_redux/.venv/lib/python3.13/site-packages (from psycopg[binary]>=3.2->-r ../requirements.txt (line 13)) (3.3.2)\n",
      "Requirement already satisfied: python-dotenv>=1.0 in /home/dadams/Repos/colorado_redux/.venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 16)) (1.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.9 in /home/dadams/Repos/colorado_redux/.venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 19)) (2.9.0.post0)\n",
      "Requirement already satisfied: matplotlib>=3.9 in /home/dadams/Repos/colorado_redux/.venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 22)) (3.10.8)\n",
      "Requirement already satisfied: pytest>=7.0 in /home/dadams/Repos/colorado_redux/.venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 25)) (9.0.2)\n",
      "Requirement already satisfied: statsmodels>=0.14 in /home/dadams/Repos/colorado_redux/.venv/lib/python3.13/site-packages (from -r ../requirements.txt (line 26)) (0.14.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/dadams/Repos/colorado_redux/.venv/lib/python3.13/site-packages (from pandas>=2.2->-r ../requirements.txt (line 2)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/dadams/Repos/colorado_redux/.venv/lib/python3.13/site-packages (from pandas>=2.2->-r ../requirements.txt (line 2)) (2025.3)\n",
      "Requirement already satisfied: pyogrio>=0.7.2 in /home/dadams/Repos/colorado_redux/.venv/lib/python3.13/site-packages (from geopandas>=1.0->-r ../requirements.txt (line 6)) (0.12.1)\n",
      "Requirement already satisfied: packaging in /home/dadams/Repos/colorado_redux/.venv/lib/python3.13/site-packages (from geopandas>=1.0->-r ../requirements.txt (line 6)) (25.0)\n",
      "Requirement already satisfied: certifi in /home/dadams/Repos/colorado_redux/.venv/lib/python3.13/site-packages (from pyproj>=3.6->-r ../requirements.txt (line 8)) (2025.11.12)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /home/dadams/Repos/colorado_redux/.venv/lib/python3.13/site-packages (from fiona>=1.10->-r ../requirements.txt (line 9)) (25.4.0)\n",
      "Requirement already satisfied: click~=8.0 in /home/dadams/Repos/colorado_redux/.venv/lib/python3.13/site-packages (from fiona>=1.10->-r ../requirements.txt (line 9)) (8.3.1)\n",
      "Requirement already satisfied: click-plugins>=1.0 in /home/dadams/Repos/colorado_redux/.venv/lib/python3.13/site-packages (from fiona>=1.10->-r ../requirements.txt (line 9)) (1.1.1.2)\n",
      "Requirement already satisfied: cligj>=0.5 in /home/dadams/Repos/colorado_redux/.venv/lib/python3.13/site-packages (from fiona>=1.10->-r ../requirements.txt (line 9)) (0.7.2)\n",
      "Requirement already satisfied: greenlet>=1 in /home/dadams/Repos/colorado_redux/.venv/lib/python3.13/site-packages (from SQLAlchemy>=2.0->-r ../requirements.txt (line 12)) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /home/dadams/Repos/colorado_redux/.venv/lib/python3.13/site-packages (from SQLAlchemy>=2.0->-r ../requirements.txt (line 12)) (4.15.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/dadams/Repos/colorado_redux/.venv/lib/python3.13/site-packages (from python-dateutil>=2.9->-r ../requirements.txt (line 19)) (1.17.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/dadams/Repos/colorado_redux/.venv/lib/python3.13/site-packages (from matplotlib>=3.9->-r ../requirements.txt (line 22)) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/dadams/Repos/colorado_redux/.venv/lib/python3.13/site-packages (from matplotlib>=3.9->-r ../requirements.txt (line 22)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/dadams/Repos/colorado_redux/.venv/lib/python3.13/site-packages (from matplotlib>=3.9->-r ../requirements.txt (line 22)) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/dadams/Repos/colorado_redux/.venv/lib/python3.13/site-packages (from matplotlib>=3.9->-r ../requirements.txt (line 22)) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in /home/dadams/Repos/colorado_redux/.venv/lib/python3.13/site-packages (from matplotlib>=3.9->-r ../requirements.txt (line 22)) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /home/dadams/Repos/colorado_redux/.venv/lib/python3.13/site-packages (from matplotlib>=3.9->-r ../requirements.txt (line 22)) (3.3.1)\n",
      "Requirement already satisfied: iniconfig>=1.0.1 in /home/dadams/Repos/colorado_redux/.venv/lib/python3.13/site-packages (from pytest>=7.0->-r ../requirements.txt (line 25)) (2.3.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /home/dadams/Repos/colorado_redux/.venv/lib/python3.13/site-packages (from pytest>=7.0->-r ../requirements.txt (line 25)) (1.6.0)\n",
      "Requirement already satisfied: pygments>=2.7.2 in /home/dadams/Repos/colorado_redux/.venv/lib/python3.13/site-packages (from pytest>=7.0->-r ../requirements.txt (line 25)) (2.19.2)\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /home/dadams/Repos/colorado_redux/.venv/lib/python3.13/site-packages (from statsmodels>=0.14->-r ../requirements.txt (line 26)) (1.16.3)\n",
      "Requirement already satisfied: patsy>=0.5.6 in /home/dadams/Repos/colorado_redux/.venv/lib/python3.13/site-packages (from statsmodels>=0.14->-r ../requirements.txt (line 26)) (1.0.2)\n",
      "Requirement already satisfied: psycopg-binary==3.3.2 in /home/dadams/Repos/colorado_redux/.venv/lib/python3.13/site-packages (from psycopg[binary]>=3.2->-r ../requirements.txt (line 13)) (3.3.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# If you're using the repo's .venv, install once per environment.\n",
    "# In VS Code, pick the interpreter from .venv, then run this cell.\n",
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbba0487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Engine(postgresql+psycopg://postgres:***@localhost:5432/colorado_spills)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.engine import URL\n",
    "\n",
    "# Optional: load repo-root .env (doesn't override existing shell env vars)\n",
    "repo_root = Path.cwd().resolve().parent\n",
    "load_dotenv(repo_root / '.env', override=False)\n",
    "\n",
    "# Prefer DATABASE_URL if provided; otherwise build from PG* vars.\n",
    "database_url = os.getenv('DATABASE_URL')\n",
    "if not database_url:\n",
    "    host = os.getenv('PGHOST', 'localhost')\n",
    "    port = int(os.getenv('PGPORT', '5432'))\n",
    "    dbname = os.getenv('colorado_spills', 'colorado_spills')\n",
    "    user = os.getenv('PGUSER', os.getenv('USER', 'postgres'))\n",
    "    password = os.getenv('PGPASSWORD')  # can be None if using .pgpass/peer auth\n",
    "    database_url = URL.create(\n",
    "        drivername='postgresql+psycopg',\n",
    "        username=user,\n",
    "        password=password,\n",
    "        host=host,\n",
    "        port=port,\n",
    "        database=dbname,\n",
    "    )\n",
    "\n",
    "engine = create_engine(database_url, future=True)\n",
    "engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbbd8c14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('PostgreSQL 18.1 on x86_64-pc-linux-gnu, compiled by gcc (GCC) 15.2.1 20251112, 64-bit',\n",
       " 'POSTGIS=\"3.6.0 POSTGIS_REVISION\" [EXTENSION] PGSQL=\"180\" GEOS=\"3.14.1-CAPI-1.20.5\" (compiled against GEOS 3.13.1) PROJ=\"')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick connectivity + PostGIS sanity check\n",
    "with engine.connect() as conn:\n",
    "    postgres_version = conn.execute(text('select version()')).scalar_one()\n",
    "    postgis_version = conn.execute(text('select postgis_full_version()')).scalar_one()\n",
    "\n",
    "postgres_version[:120], postgis_version[:120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b9c472b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.3'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scratch / sanity checks live here. (Keeps initial setup cells above clean.)\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c15598cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Document #",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Report",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Operator",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Operator #",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Tracking #",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Initial Report Date",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Date of Discovery",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Spill Type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Qtr Qtr",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Section",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Township",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "range",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "meridian",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Latitude",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Longitude",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Municipality",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "county",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Facility Type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Facility ID",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "API County Code",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "API Sequence Number",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Spilled outside of berms",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "More than five barrels spilled",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Oil Spill Volume",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Condensate Spill Volume",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Flow Back Spill Volume",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Produced Water Spill Volume",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "E&P Waste Spill Volume",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Other Waste",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Drilling Fluid Spill Volume",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Current Land Use",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Other Land Use",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Weather Conditions",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Surface Owner",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Surface Owner Other",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Waters of the State",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Residence / Occupied Structure",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "livestock",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Public Byway",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Surface Water Supply Area",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Spill Description",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Supplemental Report Date",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Oil BBLs Spilled",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Oil BBLs Recovered",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Oil Unknown",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Condensate BBLs Spilled",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Condensate BBLs Recovered",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Condensate Unknown",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Produced Water BBLs Spilled",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Produced Water BBLs Recovered",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Produced Water Unknown",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Drilling Fluid BBLs Spilled",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Drilling Fluid BBLs Recovered",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Drilling Fluid Unknown",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Flow Back Fluid BBLs Spilled",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Flow Back Fluid BBLs Recovered",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Flow Back Fluid Unkown",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Other E&P Waste BBLS Spilled",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Other E&P Waste BBLS Recovered",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Other E&P Waste Unknown",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Other E&P Waste",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Spill Contained within Berm",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Emergency Pit Constructed",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "soil",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "groundwater",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Surface Water",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Dry Drainage Feature",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Surface Area Length",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Surface Area Width",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Depth of Impact in Feet",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Depth of Impact in Inches",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Area Depth Determined",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Geology Description",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Depth to Groundwater",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Water wells in area",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Water Wells",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Water Wells None",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Surface Water Near",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Surface Water None",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Wetlands",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Wetlands None",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Springs",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Springs None",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Livestock Near",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Livestock None",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Occupied Buildings",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Occupied Buildings None",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Additional Spill Details",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Supplemental Report Date CA",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "Human Error",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Equipment Failure",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Historical Unkown",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Other",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Other Description",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Root Cause",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Preventative Measures",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Soil Excavated",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Offsite Disposal",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Onsite Treatment",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Other Disposition",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Other Disposition Description",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Ground Water Removed",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Surface Water Removed",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Corrective Actions Completed",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "Approved Form 27",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Form 27 Project Number",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "GEOID",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "TRACT_NAME",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "total_population",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "white_population",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "hispanic_population",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "median_household_income",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "poverty_population",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "unemployed_population",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "percent_white",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "percent_hispanic",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "percent_poverty",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "unemployment_rate",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "geometry",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "ruca_code",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ruca_description",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "rurality",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "9437aaa7-aa75-4532-abac-d8a4fe1edf6f",
       "rows": [
        [
         "0",
         "400827079",
         "S",
         "NOBLE ENERGY INC",
         "100322",
         "400823757",
         "04/10/2015",
         "04/09/2015",
         "Historical",
         "NWNW  ",
         "12",
         "4N    ",
         "67W    ",
         "6",
         "40.332614",
         "-104.846698",
         null,
         "WELD           ",
         "TANK BATTERY",
         "327199.0",
         "   ",
         "     ",
         "N",
         "Y",
         "0",
         "0",
         "0",
         "Unknown",
         "0",
         null,
         "0",
         "CROP LAND                          ",
         null,
         "Clear, 50 degrees and windy        ",
         "FEE",
         null,
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "Historic relaease was discovered during plug and abandoning activities.",
         "04/17/2015",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         null,
         null,
         "1.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         null,
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "0.0",
         "0.0",
         null,
         null,
         null,
         null,
         "The extent of impacts will be determined through excavation or site assessment activities.  Confirmation soil samples will confirm extent.",
         "Nunn Loam, 1-3 percent slopes",
         "9.0",
         "20.0",
         "1200.0",
         "0.0",
         "90.0",
         "0.0",
         null,
         "1.0",
         null,
         "1.0",
         null,
         "1.0",
         "600.0",
         "0.0",
         "No additonal spill details",
         "2015-04-17 00:00:00",
         "0.0",
         "1.0",
         "0.0",
         "0.0",
         null,
         "The produced water vault developed a leak at the base",
         "The facility will be plugged and abandoned",
         null,
         "0.0",
         "0.0",
         "0.0",
         null,
         null,
         null,
         "False",
         "0.0",
         null,
         "08123002101",
         "21.01",
         "11173.0",
         "9194.0",
         "3065.0",
         "83193.0",
         "247.0",
         "245.0",
         "82.28765774635282",
         "27.432202631343415",
         "2.2106864763268597",
         "2.192786180971986",
         "0101000020E6100000ACE5CE4C30365AC08B187618932A4440",
         "3",
         "Metropolitan low commuting",
         "Urban"
        ],
        [
         "1",
         "400827243",
         "I",
         "NOBLE ENERGY INC",
         "100322",
         "400827243",
         "04/17/2015",
         "04/17/2015",
         "Historical",
         "SESW  ",
         "34",
         "4N    ",
         "67W    ",
         "6",
         "40.26231",
         "-104.88036",
         null,
         "WELD           ",
         "TANK BATTERY",
         "329996.0",
         "   ",
         "     ",
         "Y",
         "N",
         "0",
         ">=1 and <5",
         "0",
         "0",
         "0",
         null,
         "0",
         "CROP LAND                          ",
         null,
         "Snow 40                            ",
         "FEE",
         null,
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "During operations the flowline leading from the separator to the oil tank developed a leak.  The facility was shut in and remediation will be scheduled.",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "False",
         "0.0",
         "200438233.0",
         "08123002101",
         "21.01",
         "11173.0",
         "9194.0",
         "3065.0",
         "83193.0",
         "247.0",
         "245.0",
         "82.28765774635282",
         "27.432202631343415",
         "2.2106864763268597",
         "2.192786180971986",
         "0101000020E6100000382D78D157385AC0F6B4C35F93214440",
         "3",
         "Metropolitan low commuting",
         "Urban"
        ],
        [
         "2",
         "400827326",
         "I",
         "KINDER MORGAN CO2 CO LP",
         "46685",
         "400827326",
         "04/18/2015",
         "04/17/2015",
         "Recent",
         "NWSW  ",
         "23",
         "37N   ",
         "18W    ",
         "N",
         "37.447045",
         "-108.807532",
         null,
         "MONTEZUMA      ",
         "TANK BATTERY",
         null,
         "   ",
         "     ",
         "N",
         "Y",
         "0",
         "0",
         "0",
         ">=5 and <100",
         "0",
         "21 bbl Produced Water",
         "0",
         "OTHER                              ",
         "Tank battery containment area",
         "Light snow, cloudy, temp 40 F      ",
         "FEDERAL",
         "BLM",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "Tank high level alarm & ESD rendered non-functional after becoming impacted with debris (sand blast media from recent tank cleaning event) causing overtopping condition. Pumps activated to transfer produced water to disposal well. Devices cleaned and now fully functional, 9 bbl of 21 bbl release were recovered, roustabout crew in process of cleanup inside containment area.",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "False",
         "0.0",
         null,
         "08083969200",
         "9692",
         "2459.0",
         "2404.0",
         "81.0",
         "66683.0",
         "330.0",
         "26.0",
         "97.76331842212281",
         "3.294021960146401",
         "13.420089467263116",
         "1.0573403822692151",
         "0101000020E61000004D9EB29AAE335BC0906B43C538B94240",
         "10",
         "Rural area",
         "Rural"
        ],
        [
         "3",
         "400834096",
         "I",
         "SMITH OIL PROPERTIES INC",
         "79905",
         "400834096",
         "04/30/2015",
         "03/26/2015",
         "Historical",
         "NENW  ",
         "4",
         "2N    ",
         "62W    ",
         "6",
         "40.173326",
         "-104.331767",
         null,
         "WELD           ",
         "TANK BATTERY",
         "441278.0",
         "   ",
         "     ",
         "Y",
         "Y",
         "0",
         "0",
         "0",
         "Unknown",
         "0",
         null,
         "0",
         "NON-CROP LAND                      ",
         null,
         "Variable over the project life.    ",
         "FEE",
         null,
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "All production has been shut-in. The produced water storage tank was removed (sent to a scrap metal recycle yard) and holes were noted in the tank bottom. No emergency pits were constructed. The extent of impacts will be determined through excavation and/or site assessment. Further remediation will be discussed in a subsequent Form 27.",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "False",
         "0.0",
         "2314759.0",
         "08123002502",
         "25.02",
         "7335.0",
         "6302.0",
         "2011.0",
         "71440.0",
         "831.0",
         "166.0",
         "85.91683708248125",
         "27.41649625085208",
         "11.32924335378323",
         "2.2631220177232447",
         "0101000020E610000017B9A7AB3B155AC0F4C5DE8B2F164440",
         "2",
         "Metropolitan high commuting",
         "Urban"
        ],
        [
         "4",
         "400834131",
         "I",
         "LINN OPERATING INC",
         "10516",
         "400834131",
         "05/01/2015",
         "04/30/2015",
         "Recent",
         "NESW  ",
         "15",
         "6S    ",
         "96W    ",
         "6",
         "39.522333",
         "-108.097129",
         null,
         "GARFIELD       ",
         "WELL PAD",
         "335991.0",
         "   ",
         "     ",
         "Y",
         "N",
         "0",
         "0",
         "0",
         ">=1 and <5",
         "0",
         "less than three (3) barrels of produced water.",
         "0",
         "NON-CROP LAND                      ",
         null,
         "clear, sunshine                    ",
         "FEE",
         null,
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "During pumping activities on the K15 696 Pad, Facility ID#335991 on 4/30/15 to remove and filter water and sediment from the bottom of the pit to allow removal of the liner, the filter bag failed and less than three (3) barrels of produced water was released outside of the containment unto the pad surface.  The water was evacuated with a vacuum truck and the soil was picked up.",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "False",
         "0.0",
         null,
         "08045952100",
         "9521",
         "7240.0",
         "5646.0",
         "1659.0",
         "64573.0",
         "693.0",
         "240.0",
         "77.98342541436463",
         "22.914364640883978",
         "9.57182320441989",
         "3.314917127071823",
         "0101000020E6100000909F8D5C37065BC0904FC8CEDBC24340",
         "7",
         "Small town core",
         "Rural"
        ]
       ],
       "shape": {
        "columns": 122,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document #</th>\n",
       "      <th>Report</th>\n",
       "      <th>Operator</th>\n",
       "      <th>Operator #</th>\n",
       "      <th>Tracking #</th>\n",
       "      <th>Initial Report Date</th>\n",
       "      <th>Date of Discovery</th>\n",
       "      <th>Spill Type</th>\n",
       "      <th>Qtr Qtr</th>\n",
       "      <th>Section</th>\n",
       "      <th>...</th>\n",
       "      <th>poverty_population</th>\n",
       "      <th>unemployed_population</th>\n",
       "      <th>percent_white</th>\n",
       "      <th>percent_hispanic</th>\n",
       "      <th>percent_poverty</th>\n",
       "      <th>unemployment_rate</th>\n",
       "      <th>geometry</th>\n",
       "      <th>ruca_code</th>\n",
       "      <th>ruca_description</th>\n",
       "      <th>rurality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>400827079</td>\n",
       "      <td>S</td>\n",
       "      <td>NOBLE ENERGY INC</td>\n",
       "      <td>100322</td>\n",
       "      <td>400823757</td>\n",
       "      <td>04/10/2015</td>\n",
       "      <td>04/09/2015</td>\n",
       "      <td>Historical</td>\n",
       "      <td>NWNW</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>247.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>82.287658</td>\n",
       "      <td>27.432203</td>\n",
       "      <td>2.210686</td>\n",
       "      <td>2.192786</td>\n",
       "      <td>0101000020E6100000ACE5CE4C30365AC08B187618932A...</td>\n",
       "      <td>3</td>\n",
       "      <td>Metropolitan low commuting</td>\n",
       "      <td>Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>400827243</td>\n",
       "      <td>I</td>\n",
       "      <td>NOBLE ENERGY INC</td>\n",
       "      <td>100322</td>\n",
       "      <td>400827243</td>\n",
       "      <td>04/17/2015</td>\n",
       "      <td>04/17/2015</td>\n",
       "      <td>Historical</td>\n",
       "      <td>SESW</td>\n",
       "      <td>34</td>\n",
       "      <td>...</td>\n",
       "      <td>247.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>82.287658</td>\n",
       "      <td>27.432203</td>\n",
       "      <td>2.210686</td>\n",
       "      <td>2.192786</td>\n",
       "      <td>0101000020E6100000382D78D157385AC0F6B4C35F9321...</td>\n",
       "      <td>3</td>\n",
       "      <td>Metropolitan low commuting</td>\n",
       "      <td>Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>400827326</td>\n",
       "      <td>I</td>\n",
       "      <td>KINDER MORGAN CO2 CO LP</td>\n",
       "      <td>46685</td>\n",
       "      <td>400827326</td>\n",
       "      <td>04/18/2015</td>\n",
       "      <td>04/17/2015</td>\n",
       "      <td>Recent</td>\n",
       "      <td>NWSW</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>330.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>97.763318</td>\n",
       "      <td>3.294022</td>\n",
       "      <td>13.420089</td>\n",
       "      <td>1.057340</td>\n",
       "      <td>0101000020E61000004D9EB29AAE335BC0906B43C538B9...</td>\n",
       "      <td>10</td>\n",
       "      <td>Rural area</td>\n",
       "      <td>Rural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>400834096</td>\n",
       "      <td>I</td>\n",
       "      <td>SMITH OIL PROPERTIES INC</td>\n",
       "      <td>79905</td>\n",
       "      <td>400834096</td>\n",
       "      <td>04/30/2015</td>\n",
       "      <td>03/26/2015</td>\n",
       "      <td>Historical</td>\n",
       "      <td>NENW</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>831.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>85.916837</td>\n",
       "      <td>27.416496</td>\n",
       "      <td>11.329243</td>\n",
       "      <td>2.263122</td>\n",
       "      <td>0101000020E610000017B9A7AB3B155AC0F4C5DE8B2F16...</td>\n",
       "      <td>2</td>\n",
       "      <td>Metropolitan high commuting</td>\n",
       "      <td>Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>400834131</td>\n",
       "      <td>I</td>\n",
       "      <td>LINN OPERATING INC</td>\n",
       "      <td>10516</td>\n",
       "      <td>400834131</td>\n",
       "      <td>05/01/2015</td>\n",
       "      <td>04/30/2015</td>\n",
       "      <td>Recent</td>\n",
       "      <td>NESW</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>693.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>77.983425</td>\n",
       "      <td>22.914365</td>\n",
       "      <td>9.571823</td>\n",
       "      <td>3.314917</td>\n",
       "      <td>0101000020E6100000909F8D5C37065BC0904FC8CEDBC2...</td>\n",
       "      <td>7</td>\n",
       "      <td>Small town core</td>\n",
       "      <td>Rural</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 122 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document # Report                  Operator  Operator #  Tracking #  \\\n",
       "0   400827079      S          NOBLE ENERGY INC      100322   400823757   \n",
       "1   400827243      I          NOBLE ENERGY INC      100322   400827243   \n",
       "2   400827326      I   KINDER MORGAN CO2 CO LP       46685   400827326   \n",
       "3   400834096      I  SMITH OIL PROPERTIES INC       79905   400834096   \n",
       "4   400834131      I        LINN OPERATING INC       10516   400834131   \n",
       "\n",
       "  Initial Report Date Date of Discovery  Spill Type Qtr Qtr  Section  ...  \\\n",
       "0          04/10/2015        04/09/2015  Historical  NWNW         12  ...   \n",
       "1          04/17/2015        04/17/2015  Historical  SESW         34  ...   \n",
       "2          04/18/2015        04/17/2015      Recent  NWSW         23  ...   \n",
       "3          04/30/2015        03/26/2015  Historical  NENW          4  ...   \n",
       "4          05/01/2015        04/30/2015      Recent  NESW         15  ...   \n",
       "\n",
       "  poverty_population unemployed_population percent_white  percent_hispanic  \\\n",
       "0              247.0                 245.0     82.287658         27.432203   \n",
       "1              247.0                 245.0     82.287658         27.432203   \n",
       "2              330.0                  26.0     97.763318          3.294022   \n",
       "3              831.0                 166.0     85.916837         27.416496   \n",
       "4              693.0                 240.0     77.983425         22.914365   \n",
       "\n",
       "   percent_poverty unemployment_rate  \\\n",
       "0         2.210686          2.192786   \n",
       "1         2.210686          2.192786   \n",
       "2        13.420089          1.057340   \n",
       "3        11.329243          2.263122   \n",
       "4         9.571823          3.314917   \n",
       "\n",
       "                                            geometry ruca_code  \\\n",
       "0  0101000020E6100000ACE5CE4C30365AC08B187618932A...         3   \n",
       "1  0101000020E6100000382D78D157385AC0F6B4C35F9321...         3   \n",
       "2  0101000020E61000004D9EB29AAE335BC0906B43C538B9...        10   \n",
       "3  0101000020E610000017B9A7AB3B155AC0F4C5DE8B2F16...         2   \n",
       "4  0101000020E6100000909F8D5C37065BC0904FC8CEDBC2...         7   \n",
       "\n",
       "              ruca_description rurality  \n",
       "0   Metropolitan low commuting    Urban  \n",
       "1   Metropolitan low commuting    Urban  \n",
       "2                   Rural area    Rural  \n",
       "3  Metropolitan high commuting    Urban  \n",
       "4              Small town core    Rural  \n",
       "\n",
       "[5 rows x 122 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a sample from spills_with_ruca\n",
    "df_spills = pd.read_sql(text('select * from spills_with_ruca limit 5'), con=engine)\n",
    "df_spills.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e52f4cfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document #</th>\n",
       "      <th>Report</th>\n",
       "      <th>Operator</th>\n",
       "      <th>Operator #</th>\n",
       "      <th>Tracking #</th>\n",
       "      <th>Initial Report Date</th>\n",
       "      <th>Date of Discovery</th>\n",
       "      <th>Spill Type</th>\n",
       "      <th>Qtr Qtr</th>\n",
       "      <th>Section</th>\n",
       "      <th>...</th>\n",
       "      <th>poverty_population</th>\n",
       "      <th>unemployed_population</th>\n",
       "      <th>percent_white</th>\n",
       "      <th>percent_hispanic</th>\n",
       "      <th>percent_poverty</th>\n",
       "      <th>unemployment_rate</th>\n",
       "      <th>geometry</th>\n",
       "      <th>ruca_code</th>\n",
       "      <th>ruca_description</th>\n",
       "      <th>rurality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>400827079</td>\n",
       "      <td>S</td>\n",
       "      <td>NOBLE ENERGY INC</td>\n",
       "      <td>100322</td>\n",
       "      <td>400823757</td>\n",
       "      <td>04/10/2015</td>\n",
       "      <td>04/09/2015</td>\n",
       "      <td>Historical</td>\n",
       "      <td>NWNW</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>247.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>82.287658</td>\n",
       "      <td>27.432203</td>\n",
       "      <td>2.210686</td>\n",
       "      <td>2.192786</td>\n",
       "      <td>POINT (-104.8467 40.33261)</td>\n",
       "      <td>3</td>\n",
       "      <td>Metropolitan low commuting</td>\n",
       "      <td>Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>400827243</td>\n",
       "      <td>I</td>\n",
       "      <td>NOBLE ENERGY INC</td>\n",
       "      <td>100322</td>\n",
       "      <td>400827243</td>\n",
       "      <td>04/17/2015</td>\n",
       "      <td>04/17/2015</td>\n",
       "      <td>Historical</td>\n",
       "      <td>SESW</td>\n",
       "      <td>34</td>\n",
       "      <td>...</td>\n",
       "      <td>247.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>82.287658</td>\n",
       "      <td>27.432203</td>\n",
       "      <td>2.210686</td>\n",
       "      <td>2.192786</td>\n",
       "      <td>POINT (-104.88036 40.26231)</td>\n",
       "      <td>3</td>\n",
       "      <td>Metropolitan low commuting</td>\n",
       "      <td>Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>400827326</td>\n",
       "      <td>I</td>\n",
       "      <td>KINDER MORGAN CO2 CO LP</td>\n",
       "      <td>46685</td>\n",
       "      <td>400827326</td>\n",
       "      <td>04/18/2015</td>\n",
       "      <td>04/17/2015</td>\n",
       "      <td>Recent</td>\n",
       "      <td>NWSW</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>330.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>97.763318</td>\n",
       "      <td>3.294022</td>\n",
       "      <td>13.420089</td>\n",
       "      <td>1.057340</td>\n",
       "      <td>POINT (-108.80753 37.44704)</td>\n",
       "      <td>10</td>\n",
       "      <td>Rural area</td>\n",
       "      <td>Rural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>400834096</td>\n",
       "      <td>I</td>\n",
       "      <td>SMITH OIL PROPERTIES INC</td>\n",
       "      <td>79905</td>\n",
       "      <td>400834096</td>\n",
       "      <td>04/30/2015</td>\n",
       "      <td>03/26/2015</td>\n",
       "      <td>Historical</td>\n",
       "      <td>NENW</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>831.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>85.916837</td>\n",
       "      <td>27.416496</td>\n",
       "      <td>11.329243</td>\n",
       "      <td>2.263122</td>\n",
       "      <td>POINT (-104.33177 40.17333)</td>\n",
       "      <td>2</td>\n",
       "      <td>Metropolitan high commuting</td>\n",
       "      <td>Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>400834131</td>\n",
       "      <td>I</td>\n",
       "      <td>LINN OPERATING INC</td>\n",
       "      <td>10516</td>\n",
       "      <td>400834131</td>\n",
       "      <td>05/01/2015</td>\n",
       "      <td>04/30/2015</td>\n",
       "      <td>Recent</td>\n",
       "      <td>NESW</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>693.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>77.983425</td>\n",
       "      <td>22.914365</td>\n",
       "      <td>9.571823</td>\n",
       "      <td>3.314917</td>\n",
       "      <td>POINT (-108.09713 39.52233)</td>\n",
       "      <td>7</td>\n",
       "      <td>Small town core</td>\n",
       "      <td>Rural</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 122 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document # Report                  Operator  Operator #  Tracking #  \\\n",
       "0   400827079      S          NOBLE ENERGY INC      100322   400823757   \n",
       "1   400827243      I          NOBLE ENERGY INC      100322   400827243   \n",
       "2   400827326      I   KINDER MORGAN CO2 CO LP       46685   400827326   \n",
       "3   400834096      I  SMITH OIL PROPERTIES INC       79905   400834096   \n",
       "4   400834131      I        LINN OPERATING INC       10516   400834131   \n",
       "\n",
       "  Initial Report Date Date of Discovery  Spill Type Qtr Qtr  Section  ...  \\\n",
       "0          04/10/2015        04/09/2015  Historical  NWNW         12  ...   \n",
       "1          04/17/2015        04/17/2015  Historical  SESW         34  ...   \n",
       "2          04/18/2015        04/17/2015      Recent  NWSW         23  ...   \n",
       "3          04/30/2015        03/26/2015  Historical  NENW          4  ...   \n",
       "4          05/01/2015        04/30/2015      Recent  NESW         15  ...   \n",
       "\n",
       "  poverty_population unemployed_population percent_white  percent_hispanic  \\\n",
       "0              247.0                 245.0     82.287658         27.432203   \n",
       "1              247.0                 245.0     82.287658         27.432203   \n",
       "2              330.0                  26.0     97.763318          3.294022   \n",
       "3              831.0                 166.0     85.916837         27.416496   \n",
       "4              693.0                 240.0     77.983425         22.914365   \n",
       "\n",
       "   percent_poverty unemployment_rate                     geometry ruca_code  \\\n",
       "0         2.210686          2.192786   POINT (-104.8467 40.33261)         3   \n",
       "1         2.210686          2.192786  POINT (-104.88036 40.26231)         3   \n",
       "2        13.420089          1.057340  POINT (-108.80753 37.44704)        10   \n",
       "3        11.329243          2.263122  POINT (-104.33177 40.17333)         2   \n",
       "4         9.571823          3.314917  POINT (-108.09713 39.52233)         7   \n",
       "\n",
       "              ruca_description rurality  \n",
       "0   Metropolitan low commuting    Urban  \n",
       "1   Metropolitan low commuting    Urban  \n",
       "2                   Rural area    Rural  \n",
       "3  Metropolitan high commuting    Urban  \n",
       "4              Small town core    Rural  \n",
       "\n",
       "[5 rows x 122 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a GeoDataFrame from PostGIS table spills_with_ruca\n",
    "# (Requires a geometry column; this auto-detects it.)\n",
    "\n",
    "# Find geometry column name\n",
    "geom_col = None\n",
    "geom_col_sql = (\n",
    "    \"select column_name \"\n",
    "\n",
    "    \"from information_schema.columns \"\n",
    "\n",
    "    \"where table_schema = 'public' \"\n",
    "\n",
    "    \"  and table_name = 'spills_with_ruca' \"\n",
    "\n",
    "    \"  and udt_name = 'geometry' \"\n",
    "\n",
    "    \"order by ordinal_position \"\n",
    "\n",
    "    \"limit 1\"\n",
    " )\n",
    "with engine.connect() as conn:\n",
    "    geom_col = conn.execute(text(geom_col_sql)).scalar_one_or_none()\n",
    "\n",
    "if not geom_col:\n",
    "    raise ValueError('No geometry column found on public.spills_with_ruca')\n",
    "\n",
    "# Detect SRID from the first non-null geometry (optional but nice to have)\n",
    "with engine.connect() as conn:\n",
    "    srid_sql = (\n",
    "        f'select ST_SRID(\"{geom_col}\") '\n",
    "        f'from public.spills_with_ruca '\n",
    "        f'where \"{geom_col}\" is not null '\n",
    "        f'limit 1'\n",
    "    )\n",
    "    srid = conn.execute(text(srid_sql)).scalar_one_or_none()\n",
    "\n",
    "crs = f'EPSG:{srid}' if srid else None\n",
    "spills_gdf = gpd.read_postgis('select * from public.spills_with_ruca', con=engine, geom_col=geom_col, crs=crs)\n",
    "\n",
    "spills_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc00f6a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Date of Discovery",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "Initial Report Date",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "Report Year",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "Discovery Year",
         "rawType": "int32",
         "type": "integer"
        }
       ],
       "ref": "19f2ad7b-c853-4e2a-a95c-0d8afc21d9eb",
       "rows": [
        [
         "0",
         "2015-04-09 00:00:00",
         "2015-04-10 00:00:00",
         "2015",
         "2015"
        ],
        [
         "1",
         "2015-04-17 00:00:00",
         "2015-04-17 00:00:00",
         "2015",
         "2015"
        ],
        [
         "2",
         "2015-04-17 00:00:00",
         "2015-04-18 00:00:00",
         "2015",
         "2015"
        ],
        [
         "3",
         "2015-03-26 00:00:00",
         "2015-04-30 00:00:00",
         "2015",
         "2015"
        ],
        [
         "4",
         "2015-04-30 00:00:00",
         "2015-05-01 00:00:00",
         "2015",
         "2015"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date of Discovery</th>\n",
       "      <th>Initial Report Date</th>\n",
       "      <th>Report Year</th>\n",
       "      <th>Discovery Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-04-09</td>\n",
       "      <td>2015-04-10</td>\n",
       "      <td>2015</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-04-17</td>\n",
       "      <td>2015-04-17</td>\n",
       "      <td>2015</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-04-17</td>\n",
       "      <td>2015-04-18</td>\n",
       "      <td>2015</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-03-26</td>\n",
       "      <td>2015-04-30</td>\n",
       "      <td>2015</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-04-30</td>\n",
       "      <td>2015-05-01</td>\n",
       "      <td>2015</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Date of Discovery Initial Report Date  Report Year  Discovery Year\n",
       "0        2015-04-09          2015-04-10         2015            2015\n",
       "1        2015-04-17          2015-04-17         2015            2015\n",
       "2        2015-04-17          2015-04-18         2015            2015\n",
       "3        2015-03-26          2015-04-30         2015            2015\n",
       "4        2015-04-30          2015-05-01         2015            2015"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize key date columns + derive year fields\n",
    "date_cols = ['Date of Discovery', 'Initial Report Date']\n",
    "missing = [c for c in date_cols if c not in spills_gdf.columns]\n",
    "if missing:\n",
    "    raise KeyError(f\"Missing expected columns: {missing}. Available columns include: {list(spills_gdf.columns)[:20]}\")\n",
    "\n",
    "for col in date_cols:\n",
    "    spills_gdf[col] = pd.to_datetime(spills_gdf[col], errors='coerce')\n",
    "\n",
    "spills_gdf['Report Year'] = spills_gdf['Initial Report Date'].dt.year\n",
    "spills_gdf['Discovery Year'] = spills_gdf['Date of Discovery'].dt.year\n",
    "\n",
    "spills_gdf[date_cols + ['Report Year', 'Discovery Year']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c02293a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "Report Year",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Discovery Year",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "db8d48ea-505e-4ae3-bed1-1c0cfb6d9d6f",
       "rows": [
        [
         "2015",
         "2406",
         "2384"
        ],
        [
         "2016",
         "2758",
         "2752"
        ],
        [
         "2017",
         "3224",
         "3206"
        ],
        [
         "2018",
         "3172",
         "3168"
        ],
        [
         "2019",
         "3120",
         "3122"
        ],
        [
         "2020",
         "2354",
         "2342"
        ],
        [
         "2021",
         "3568",
         "3588"
        ],
        [
         "2022",
         "4314",
         "4340"
        ],
        [
         "2023",
         "4268",
         "4344"
        ],
        [
         "2024",
         "5456",
         "5400"
        ],
        [
         "2025",
         "1000",
         "894"
        ],
        [
         "1994",
         "0",
         "2"
        ],
        [
         "2004",
         "0",
         "2"
        ],
        [
         "2009",
         "0",
         "2"
        ],
        [
         "2011",
         "0",
         "8"
        ],
        [
         "2012",
         "0",
         "14"
        ],
        [
         "2013",
         "0",
         "18"
        ],
        [
         "2014",
         "0",
         "54"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 18
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Report Year</th>\n",
       "      <th>Discovery Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>2406</td>\n",
       "      <td>2384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>2758</td>\n",
       "      <td>2752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>3224</td>\n",
       "      <td>3206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>3172</td>\n",
       "      <td>3168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>3120</td>\n",
       "      <td>3122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>2354</td>\n",
       "      <td>2342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>3568</td>\n",
       "      <td>3588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022</th>\n",
       "      <td>4314</td>\n",
       "      <td>4340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023</th>\n",
       "      <td>4268</td>\n",
       "      <td>4344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>5456</td>\n",
       "      <td>5400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025</th>\n",
       "      <td>1000</td>\n",
       "      <td>894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Report Year  Discovery Year\n",
       "2015         2406            2384\n",
       "2016         2758            2752\n",
       "2017         3224            3206\n",
       "2018         3172            3168\n",
       "2019         3120            3122\n",
       "2020         2354            2342\n",
       "2021         3568            3588\n",
       "2022         4314            4340\n",
       "2023         4268            4344\n",
       "2024         5456            5400\n",
       "2025         1000             894\n",
       "1994            0               2\n",
       "2004            0               2\n",
       "2009            0               2\n",
       "2011            0               8\n",
       "2012            0              14\n",
       "2013            0              18\n",
       "2014            0              54"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Frequency tables for derived year fields\n",
    "year_cols = ['Report Year', 'Discovery Year']\n",
    "missing = [c for c in year_cols if c not in spills_gdf.columns]\n",
    "if missing:\n",
    "    raise KeyError(f\"Missing expected columns: {missing}\")\n",
    "\n",
    "report_year_freq = spills_gdf['Report Year'].value_counts(dropna=False).sort_index()\n",
    "discovery_year_freq = spills_gdf['Discovery Year'].value_counts(dropna=False).sort_index()\n",
    "\n",
    "year_freq = (\n",
    "    pd.concat(\n",
    "        {'Report Year': report_year_freq, 'Discovery Year': discovery_year_freq},\n",
    "        axis=1,\n",
    "    )\n",
    "    .fillna(0)\n",
    "    .astype('int64')\n",
    ")\n",
    "\n",
    "year_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02cf34b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "0",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "3c01ed06-fd8b-4562-a052-e801f57fd212",
       "rows": [
        [
         "Report Year",
         "35640"
        ],
        [
         "Discovery Year",
         "35640"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 2
       }
      },
      "text/plain": [
       "Report Year       35640\n",
       "Discovery Year    35640\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sum totals for both frequency columns\n",
    "year_freq[['Report Year', 'Discovery Year']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52f43c1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "first_date",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "last_date",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        }
       ],
       "ref": "4841df63-b3af-4178-a361-12841325fdf6",
       "rows": [
        [
         "Initial Report Date",
         "2015-03-19 00:00:00",
         "2025-03-17 00:00:00"
        ],
        [
         "Date of Discovery",
         "1994-11-14 00:00:00",
         "2025-03-16 00:00:00"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_date</th>\n",
       "      <th>last_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Initial Report Date</th>\n",
       "      <td>2015-03-19</td>\n",
       "      <td>2025-03-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date of Discovery</th>\n",
       "      <td>1994-11-14</td>\n",
       "      <td>2025-03-16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    first_date  last_date\n",
       "Initial Report Date 2015-03-19 2025-03-17\n",
       "Date of Discovery   1994-11-14 2025-03-16"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First/last (min/max) dates for each date column\n",
    "date_cols = ['Initial Report Date', 'Date of Discovery']\n",
    "missing = [c for c in date_cols if c not in spills_gdf.columns]\n",
    "if missing:\n",
    "    raise KeyError(f\"Missing expected columns: {missing}\")\n",
    "\n",
    "date_range = pd.DataFrame({\n",
    "    'first_date': [spills_gdf[c].min(skipna=True) for c in date_cols],\n",
    "    'last_date': [spills_gdf[c].max(skipna=True) for c in date_cols],\n",
    "}, index=date_cols)\n",
    "\n",
    "date_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6999c79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35640, 34540)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep only rows with years 2015â€“2024 (inclusive) for BOTH year fields\n",
    "required_cols = ['Report Year', 'Discovery Year']\n",
    "missing = [c for c in required_cols if c not in spills_gdf.columns]\n",
    "if missing:\n",
    "    raise KeyError(f\"Missing columns {missing}; run the year-derivation cell first.\")\n",
    "\n",
    "before_n = len(spills_gdf)\n",
    "mask = spills_gdf['Report Year'].between(2015, 2024) & spills_gdf['Discovery Year'].between(2015, 2024)\n",
    "spills_gdf = spills_gdf[mask].copy()\n",
    "after_n = len(spills_gdf)\n",
    "\n",
    "before_n, after_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49a692fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "Report Year",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Discovery Year",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "b74ffb4f-9b7f-46de-b417-c3b7e5f741fc",
       "rows": [
        [
         "2015",
         "2346",
         "2384"
        ],
        [
         "2016",
         "2744",
         "2752"
        ],
        [
         "2017",
         "3220",
         "3206"
        ],
        [
         "2018",
         "3172",
         "3168"
        ],
        [
         "2019",
         "3104",
         "3122"
        ],
        [
         "2020",
         "2352",
         "2342"
        ],
        [
         "2021",
         "3568",
         "3588"
        ],
        [
         "2022",
         "4314",
         "4338"
        ],
        [
         "2023",
         "4268",
         "4322"
        ],
        [
         "2024",
         "5452",
         "5318"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Report Year</th>\n",
       "      <th>Discovery Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>2346</td>\n",
       "      <td>2384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>2744</td>\n",
       "      <td>2752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>3220</td>\n",
       "      <td>3206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>3172</td>\n",
       "      <td>3168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>3104</td>\n",
       "      <td>3122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>2352</td>\n",
       "      <td>2342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>3568</td>\n",
       "      <td>3588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022</th>\n",
       "      <td>4314</td>\n",
       "      <td>4338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023</th>\n",
       "      <td>4268</td>\n",
       "      <td>4322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>5452</td>\n",
       "      <td>5318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Report Year  Discovery Year\n",
       "2015         2346            2384\n",
       "2016         2744            2752\n",
       "2017         3220            3206\n",
       "2018         3172            3168\n",
       "2019         3104            3122\n",
       "2020         2352            2342\n",
       "2021         3568            3588\n",
       "2022         4314            4338\n",
       "2023         4268            4322\n",
       "2024         5452            5318"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check: frequency tables after filtering to 2015â€“2024 for BOTH year fields\n",
    "year_cols = ['Report Year', 'Discovery Year']\n",
    "missing = [c for c in year_cols if c not in spills_gdf.columns]\n",
    "if missing:\n",
    "    raise KeyError(f\"Missing expected columns: {missing}\")\n",
    "\n",
    "report_year_freq_post = spills_gdf['Report Year'].value_counts(dropna=False).sort_index()\n",
    "discovery_year_freq_post = spills_gdf['Discovery Year'].value_counts(dropna=False).sort_index()\n",
    "\n",
    "year_freq_post = (\n",
    "    pd.concat(\n",
    "        {'Report Year': report_year_freq_post, 'Discovery Year': discovery_year_freq_post},\n",
    "        axis=1,\n",
    "    )\n",
    "    .fillna(0)\n",
    "    .astype('int64')\n",
    ")\n",
    "\n",
    "year_freq_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe097def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Report Year vs Discovery Year frequencies (post-filter)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use the post-filter table if present; otherwise compute quickly from spills_gdf\n",
    "if 'year_freq_post' in globals():\n",
    "    plot_df = year_freq_post.copy()\n",
    "else:\n",
    "    plot_df = pd.DataFrame({\n",
    "        'Report Year': spills_gdf['Report Year'].value_counts(dropna=False),\n",
    "        'Discovery Year': spills_gdf['Discovery Year'].value_counts(dropna=False),\n",
    "    }).fillna(0).astype('int64').sort_index()\n",
    "\n",
    "plot_df = plot_df.sort_index()\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(10, 6), sharex=True)\n",
    "plot_df['Report Year'].plot(kind='bar', ax=axes[0], title='Report Year frequency')\n",
    "axes[0].set_ylabel('Count')\n",
    "plot_df['Discovery Year'].plot(kind='bar', ax=axes[1], title='Discovery Year frequency')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_xlabel('Year')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53dc3383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13106, 21434)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate Historical vs. Recent Spills\n",
    "if 'Spill Type' not in spills_gdf.columns:\n",
    "    raise KeyError(\"'Spill Type' column not found on spills_gdf\")\n",
    "\n",
    "historical_spills = spills_gdf[spills_gdf['Spill Type'] == 'Historical'].copy()\n",
    "recent_spills = spills_gdf[spills_gdf['Spill Type'] == 'Recent'].copy()\n",
    "\n",
    "len(historical_spills), len(recent_spills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19961419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['WELD', 'GARFIELD', 'RIO BLANCO'], 26376, 11294, 15082)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only use the top 3 counties by number of spills (after trimming county names)\n",
    "required = ['spills_gdf', 'historical_spills', 'recent_spills']\n",
    "missing_vars = [v for v in required if v not in globals()]\n",
    "if missing_vars:\n",
    "    raise NameError(f\"Missing variables: {missing_vars}. Run the earlier cells first.\")\n",
    "\n",
    "for name, df in [('spills_gdf', spills_gdf), ('historical_spills', historical_spills), ('recent_spills', recent_spills)]:\n",
    "    if 'county' not in df.columns:\n",
    "        raise KeyError(f\"'county' column not found on {name}\")\n",
    "\n",
    "# Normalize county strings (some values are space-padded in the source table)\n",
    "spills_gdf['county'] = spills_gdf['county'].astype('string').str.strip()\n",
    "historical_spills['county'] = historical_spills['county'].astype('string').str.strip()\n",
    "recent_spills['county'] = recent_spills['county'].astype('string').str.strip()\n",
    "\n",
    "top_counties = spills_gdf['county'].value_counts().nlargest(3).index.tolist()\n",
    "\n",
    "# Filter spills_gdf to only include top counties\n",
    "spills_gdf = spills_gdf[spills_gdf['county'].isin(top_counties)].copy()\n",
    "\n",
    "# Filter historical spills to only include top counties\n",
    "historical_spills = historical_spills[historical_spills['county'].isin(top_counties)].copy()\n",
    "\n",
    "# Filter recent spills to only include top counties\n",
    "recent_spills = recent_spills[recent_spills['county'].isin(top_counties)].copy()\n",
    "\n",
    "top_counties, len(spills_gdf), len(historical_spills), len(recent_spills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3a45e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "Before Report Year 2020",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "f1788ca1-9156-4eb3-882a-4eca4e911985",
       "rows": [
        [
         "False",
         "15704"
        ],
        [
         "True",
         "10672"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 2
       }
      },
      "text/plain": [
       "Before Report Year 2020\n",
       "False    15704\n",
       "True     10672\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Policy indicator: before 1/1/2020 (using Report Year)\n",
    "if 'Report Year' not in spills_gdf.columns:\n",
    "    raise KeyError(\"'Report Year' column not found on spills_gdf\")\n",
    "\n",
    "spills_gdf['Before Report Year 2020'] = spills_gdf['Report Year'] < 2020\n",
    "historical_spills['Before Report Year 2020'] = historical_spills['Report Year'] < 2020\n",
    "recent_spills['Before Report Year 2020'] = recent_spills['Report Year'] < 2020\n",
    "\n",
    "spills_gdf['Before Report Year 2020'].value_counts(dropna=False).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f35d7bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "report_delay_days",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "fcfa3855-8399-4bc2-9bb7-cb4ffa9a7e63",
       "rows": [
        [
         "count",
         "26376.0"
        ],
        [
         "mean",
         "3.347361237488626"
        ],
        [
         "std",
         "25.067149611081554"
        ],
        [
         "min",
         "0.0"
        ],
        [
         "1%",
         "0.0"
        ],
        [
         "5%",
         "0.0"
        ],
        [
         "50%",
         "1.0"
        ],
        [
         "95%",
         "5.0"
        ],
        [
         "99%",
         "55.0"
        ],
        [
         "max",
         "1329.0"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 10
       }
      },
      "text/plain": [
       "count    26376.000000\n",
       "mean         3.347361\n",
       "std         25.067150\n",
       "min          0.000000\n",
       "1%           0.000000\n",
       "5%           0.000000\n",
       "50%          1.000000\n",
       "95%          5.000000\n",
       "99%         55.000000\n",
       "max       1329.000000\n",
       "Name: report_delay_days, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Report delay: Initial Report Date minus Date of Discovery\n",
    "required_cols = ['Initial Report Date', 'Date of Discovery']\n",
    "missing = [c for c in required_cols if c not in spills_gdf.columns]\n",
    "if missing:\n",
    "    raise KeyError(f\"Missing required columns for report_delay: {missing}\")\n",
    "\n",
    "# Timedelta (can be negative if dates are inconsistent)\n",
    "spills_gdf['report_delay'] = spills_gdf['Initial Report Date'] - spills_gdf['Date of Discovery']\n",
    "historical_spills['report_delay'] = historical_spills['Initial Report Date'] - historical_spills['Date of Discovery']\n",
    "recent_spills['report_delay'] = recent_spills['Initial Report Date'] - recent_spills['Date of Discovery']\n",
    "\n",
    "# Convenience numeric version\n",
    "spills_gdf['report_delay_days'] = spills_gdf['report_delay'].dt.total_seconds() / 86400.0\n",
    "historical_spills['report_delay_days'] = historical_spills['report_delay'].dt.total_seconds() / 86400.0\n",
    "recent_spills['report_delay_days'] = recent_spills['report_delay'].dt.total_seconds() / 86400.0\n",
    "\n",
    "spills_gdf['report_delay_days'].describe(percentiles=[0.01, 0.05, 0.5, 0.95, 0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "850ed30a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(rows_total              26376.0\n",
       " missing_delay_days          0.0\n",
       " negative_delay_days         0.0\n",
       " delay_days_ge_30          412.0\n",
       " delay_days_ge_90          200.0\n",
       " q1_days                     0.0\n",
       " q3_days                     1.0\n",
       " iqr_days                    1.0\n",
       " iqr_upper_fence_days        2.5\n",
       " iqr_lower_fence_days       -1.5\n",
       " iqr_hi_outliers_n        3618.0\n",
       " iqr_lo_outliers_n           0.0\n",
       " dtype: float64,\n",
       "       county  Spill Type  Report Year  Discovery Year  \\\n",
       " 27284   WELD      Recent         2021            2018   \n",
       " 9285    WELD      Recent         2021            2018   \n",
       " 33702   WELD  Historical         2024            2022   \n",
       " 33707   WELD  Historical         2024            2022   \n",
       " 15956   WELD  Historical         2024            2022   \n",
       " 15952   WELD  Historical         2024            2022   \n",
       " 14624   WELD  Historical         2024            2022   \n",
       " 32401   WELD  Historical         2024            2022   \n",
       " 15055   WELD  Historical         2024            2022   \n",
       " 32824   WELD  Historical         2024            2022   \n",
       " 11853   WELD  Historical         2022            2021   \n",
       " 29646   WELD  Historical         2022            2021   \n",
       " 32051   WELD  Historical         2023            2022   \n",
       " 14272   WELD  Historical         2023            2022   \n",
       " 34188   WELD  Historical         2023            2022   \n",
       " 16442   WELD  Historical         2023            2022   \n",
       " 32821   WELD  Historical         2024            2023   \n",
       " 15051   WELD  Historical         2024            2023   \n",
       " 7628    WELD      Recent         2020            2019   \n",
       " 7596    WELD      Recent         2020            2019   \n",
       " 25399   WELD      Recent         2020            2019   \n",
       " 25431   WELD      Recent         2020            2019   \n",
       " 30577   WELD      Recent         2023            2022   \n",
       " 30107   WELD      Recent         2023            2022   \n",
       " 12311   WELD      Recent         2023            2022   \n",
       " \n",
       "        Before Report Year 2020 Initial Report Date Date of Discovery  \\\n",
       " 27284                    False          2021-08-30        2018-01-09   \n",
       " 9285                     False          2021-08-30        2018-01-09   \n",
       " 33702                    False          2024-08-09        2022-10-24   \n",
       " 33707                    False          2024-08-09        2022-10-24   \n",
       " 15956                    False          2024-08-09        2022-10-24   \n",
       " 15952                    False          2024-08-09        2022-10-24   \n",
       " 14624                    False          2024-02-16        2022-05-09   \n",
       " 32401                    False          2024-02-16        2022-05-09   \n",
       " 15055                    False          2024-04-17        2022-10-24   \n",
       " 32824                    False          2024-04-17        2022-10-24   \n",
       " 11853                    False          2022-10-26        2021-07-21   \n",
       " 29646                    False          2022-10-26        2021-07-21   \n",
       " 32051                    False          2023-12-29        2022-10-27   \n",
       " 14272                    False          2023-12-29        2022-10-27   \n",
       " 34188                    False          2023-12-29        2022-10-27   \n",
       " 16442                    False          2023-12-29        2022-10-27   \n",
       " 32821                    False          2024-04-16        2023-02-27   \n",
       " 15051                    False          2024-04-16        2023-02-27   \n",
       " 7628                     False          2020-05-07        2019-04-08   \n",
       " 7596                     False          2020-05-07        2019-04-08   \n",
       " 25399                    False          2020-05-07        2019-04-08   \n",
       " 25431                    False          2020-05-07        2019-04-08   \n",
       " 30577                    False          2023-01-20        2022-01-20   \n",
       " 30107                    False          2023-01-20        2022-01-20   \n",
       " 12311                    False          2023-01-20        2022-01-20   \n",
       " \n",
       "        report_delay_days report_delay                         Operator  \\\n",
       " 27284             1329.0    1329 days          KP KAUFFMAN COMPANY INC   \n",
       " 9285              1329.0    1329 days          KP KAUFFMAN COMPANY INC   \n",
       " 33702              655.0     655 days          KP KAUFFMAN COMPANY INC   \n",
       " 33707              655.0     655 days          KP KAUFFMAN COMPANY INC   \n",
       " 15956              655.0     655 days          KP KAUFFMAN COMPANY INC   \n",
       " 15952              655.0     655 days          KP KAUFFMAN COMPANY INC   \n",
       " 14624              648.0     648 days  KERR MCGEE OIL & GAS ONSHORE LP   \n",
       " 32401              648.0     648 days  KERR MCGEE OIL & GAS ONSHORE LP   \n",
       " 15055              541.0     541 days          KP KAUFFMAN COMPANY INC   \n",
       " 32824              541.0     541 days          KP KAUFFMAN COMPANY INC   \n",
       " 11853              462.0     462 days          KP KAUFFMAN COMPANY INC   \n",
       " 29646              462.0     462 days          KP KAUFFMAN COMPANY INC   \n",
       " 32051              428.0     428 days      PETRO OPERATING COMPANY LLC   \n",
       " 14272              428.0     428 days      PETRO OPERATING COMPANY LLC   \n",
       " 34188              428.0     428 days      PETRO OPERATING COMPANY LLC   \n",
       " 16442              428.0     428 days      PETRO OPERATING COMPANY LLC   \n",
       " 32821              414.0     414 days                 NOBLE ENERGY INC   \n",
       " 15051              414.0     414 days                 NOBLE ENERGY INC   \n",
       " 7628               395.0     395 days          KP KAUFFMAN COMPANY INC   \n",
       " 7596               395.0     395 days          KP KAUFFMAN COMPANY INC   \n",
       " 25399              395.0     395 days          KP KAUFFMAN COMPANY INC   \n",
       " 25431              395.0     395 days          KP KAUFFMAN COMPANY INC   \n",
       " 30577              365.0     365 days  KERR MCGEE OIL & GAS ONSHORE LP   \n",
       " 30107              365.0     365 days  KERR MCGEE OIL & GAS ONSHORE LP   \n",
       " 12311              365.0     365 days  KERR MCGEE OIL & GAS ONSHORE LP   \n",
       " \n",
       "         Latitude   Longitude  \n",
       " 27284  40.035140 -104.917660  \n",
       " 9285   40.035140 -104.917660  \n",
       " 33702  40.083751 -105.039722  \n",
       " 33707  40.082903 -105.039918  \n",
       " 15956  40.082903 -105.039918  \n",
       " 15952  40.083751 -105.039722  \n",
       " 14624  40.069483 -104.948097  \n",
       " 32401  40.069483 -104.948097  \n",
       " 15055  40.084742 -105.039550  \n",
       " 32824  40.084742 -105.039550  \n",
       " 11853  40.129190 -104.788576  \n",
       " 29646  40.129190 -104.788576  \n",
       " 32051  40.110909 -104.824469  \n",
       " 14272  40.110909 -104.824469  \n",
       " 34188  40.110909 -104.824469  \n",
       " 16442  40.110909 -104.824469  \n",
       " 32821  40.395440 -104.555204  \n",
       " 15051  40.395440 -104.555204  \n",
       " 7628   40.375604 -104.923776  \n",
       " 7596   40.375604 -104.923776  \n",
       " 25399  40.375604 -104.923776  \n",
       " 25431  40.375604 -104.923776  \n",
       " 30577  40.261072 -104.715610  \n",
       " 30107  40.261072 -104.715610  \n",
       " 12311  40.261072 -104.715610  ,\n",
       "       county Spill Type  Report Year  Discovery Year Before Report Year 2020  \\\n",
       " 35626   <NA>        NaN          NaN             NaN                     NaN   \n",
       " 35623   <NA>        NaN          NaN             NaN                     NaN   \n",
       " 35621   <NA>        NaN          NaN             NaN                     NaN   \n",
       " 35614   <NA>        NaN          NaN             NaN                     NaN   \n",
       " 24      <NA>        NaN          NaN             NaN                     NaN   \n",
       " 20      <NA>        NaN          NaN             NaN                     NaN   \n",
       " 16      <NA>        NaN          NaN             NaN                     NaN   \n",
       " 6       <NA>        NaN          NaN             NaN                     NaN   \n",
       " 14018   <NA>        NaN          NaN             NaN                     NaN   \n",
       " 14017   <NA>        NaN          NaN             NaN                     NaN   \n",
       " 14016   <NA>        NaN          NaN             NaN                     NaN   \n",
       " 14012   <NA>        NaN          NaN             NaN                     NaN   \n",
       " 14010   <NA>        NaN          NaN             NaN                     NaN   \n",
       " 14009   <NA>        NaN          NaN             NaN                     NaN   \n",
       " 14006   <NA>        NaN          NaN             NaN                     NaN   \n",
       " 14003   <NA>        NaN          NaN             NaN                     NaN   \n",
       " 35584   <NA>        NaN          NaN             NaN                     NaN   \n",
       " 35579   <NA>        NaN          NaN             NaN                     NaN   \n",
       " 35575   <NA>        NaN          NaN             NaN                     NaN   \n",
       " 35574   <NA>        NaN          NaN             NaN                     NaN   \n",
       " 35604   <NA>        NaN          NaN             NaN                     NaN   \n",
       " 35602   <NA>        NaN          NaN             NaN                     NaN   \n",
       " 35597   <NA>        NaN          NaN             NaN                     NaN   \n",
       " 35593   <NA>        NaN          NaN             NaN                     NaN   \n",
       " 47      <NA>        NaN          NaN             NaN                     NaN   \n",
       " \n",
       "       Initial Report Date Date of Discovery  report_delay_days report_delay  \\\n",
       " 35626                 NaT               NaT                0.0          NaT   \n",
       " 35623                 NaT               NaT                0.0          NaT   \n",
       " 35621                 NaT               NaT                0.0          NaT   \n",
       " 35614                 NaT               NaT                0.0          NaT   \n",
       " 24                    NaT               NaT                0.0          NaT   \n",
       " 20                    NaT               NaT                0.0          NaT   \n",
       " 16                    NaT               NaT                0.0          NaT   \n",
       " 6                     NaT               NaT                0.0          NaT   \n",
       " 14018                 NaT               NaT                0.0          NaT   \n",
       " 14017                 NaT               NaT                0.0          NaT   \n",
       " 14016                 NaT               NaT                0.0          NaT   \n",
       " 14012                 NaT               NaT                0.0          NaT   \n",
       " 14010                 NaT               NaT                0.0          NaT   \n",
       " 14009                 NaT               NaT                0.0          NaT   \n",
       " 14006                 NaT               NaT                0.0          NaT   \n",
       " 14003                 NaT               NaT                0.0          NaT   \n",
       " 35584                 NaT               NaT                0.0          NaT   \n",
       " 35579                 NaT               NaT                0.0          NaT   \n",
       " 35575                 NaT               NaT                0.0          NaT   \n",
       " 35574                 NaT               NaT                0.0          NaT   \n",
       " 35604                 NaT               NaT                0.0          NaT   \n",
       " 35602                 NaT               NaT                0.0          NaT   \n",
       " 35597                 NaT               NaT                0.0          NaT   \n",
       " 35593                 NaT               NaT                0.0          NaT   \n",
       " 47                    NaT               NaT                0.0          NaT   \n",
       " \n",
       "       Operator  Latitude  Longitude  \n",
       " 35626      NaN       NaN        NaN  \n",
       " 35623      NaN       NaN        NaN  \n",
       " 35621      NaN       NaN        NaN  \n",
       " 35614      NaN       NaN        NaN  \n",
       " 24         NaN       NaN        NaN  \n",
       " 20         NaN       NaN        NaN  \n",
       " 16         NaN       NaN        NaN  \n",
       " 6          NaN       NaN        NaN  \n",
       " 14018      NaN       NaN        NaN  \n",
       " 14017      NaN       NaN        NaN  \n",
       " 14016      NaN       NaN        NaN  \n",
       " 14012      NaN       NaN        NaN  \n",
       " 14010      NaN       NaN        NaN  \n",
       " 14009      NaN       NaN        NaN  \n",
       " 14006      NaN       NaN        NaN  \n",
       " 14003      NaN       NaN        NaN  \n",
       " 35584      NaN       NaN        NaN  \n",
       " 35579      NaN       NaN        NaN  \n",
       " 35575      NaN       NaN        NaN  \n",
       " 35574      NaN       NaN        NaN  \n",
       " 35604      NaN       NaN        NaN  \n",
       " 35602      NaN       NaN        NaN  \n",
       " 35597      NaN       NaN        NaN  \n",
       " 35593      NaN       NaN        NaN  \n",
       " 47         NaN       NaN        NaN  )"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Outliers: reporting delay (report_delay_days)\n",
    "if 'report_delay_days' not in spills_gdf.columns:\n",
    "    raise KeyError(\"'report_delay_days' not found; run the report_delay cell first.\")\n",
    "\n",
    "s = pd.to_numeric(spills_gdf['report_delay_days'], errors='coerce')\n",
    "q1, q3 = s.quantile([0.25, 0.75])\n",
    "iqr = q3 - q1\n",
    "upper_iqr = q3 + 1.5 * iqr\n",
    "lower_iqr = q1 - 1.5 * iqr\n",
    "\n",
    "n_total = len(spills_gdf)\n",
    "n_missing = int(s.isna().sum())\n",
    "n_negative = int((s < 0).sum())\n",
    "n_ge_30 = int((s >= 30).sum())\n",
    "n_ge_90 = int((s >= 90).sum())\n",
    "n_outliers_iqr_hi = int((s > upper_iqr).sum())\n",
    "n_outliers_iqr_lo = int((s < lower_iqr).sum())\n",
    "\n",
    "summary = pd.Series({\n",
    "    'rows_total': n_total,\n",
    "    'missing_delay_days': n_missing,\n",
    "    'negative_delay_days': n_negative,\n",
    "    'delay_days_ge_30': n_ge_30,\n",
    "    'delay_days_ge_90': n_ge_90,\n",
    "    'q1_days': float(q1),\n",
    "    'q3_days': float(q3),\n",
    "    'iqr_days': float(iqr),\n",
    "    'iqr_upper_fence_days': float(upper_iqr),\n",
    "    'iqr_lower_fence_days': float(lower_iqr),\n",
    "    'iqr_hi_outliers_n': n_outliers_iqr_hi,\n",
    "    'iqr_lo_outliers_n': n_outliers_iqr_lo,\n",
    "})\n",
    "\n",
    "# Show the most extreme delays with helpful context columns (only those that exist)\n",
    "candidate_cols = [\n",
    "    'county',\n",
    "    'Spill Type',\n",
    "    'Report Year',\n",
    "    'Discovery Year',\n",
    "    'Before Report Year 2020',\n",
    "    'Initial Report Date',\n",
    "    'Date of Discovery',\n",
    "    'report_delay_days',\n",
    "    'report_delay',\n",
    "    'Operator',\n",
    "    'Operator Name',\n",
    "    'Facility',\n",
    "    'Facility Name',\n",
    "    'Location',\n",
    "    'Incident ID',\n",
    "    'Spill ID',\n",
    "    'Document ID',\n",
    "    'Form',\n",
    "    'Cause',\n",
    "    'Cause of Spill',\n",
    "    'Spilled Material',\n",
    "    'Spill Volume',\n",
    "    'Volume',\n",
    "    'volume',\n",
    "    'gallons',\n",
    "    'Gallons',\n",
    "    'barrels',\n",
    "    'Barrels',\n",
    "    'Lat',\n",
    "    'Lon',\n",
    "    'Latitude',\n",
    "    'Longitude',\n",
    "    'api',\n",
    "    'API',\n",
    " ]\n",
    "display_cols = [c for c in candidate_cols if c in spills_gdf.columns]\n",
    "\n",
    "top_delays = (\n",
    "    spills_gdf.loc[s.notna()].assign(report_delay_days=s).sort_values('report_delay_days', ascending=False).head(25)\n",
    " )\n",
    "neg_delays = (\n",
    "    spills_gdf.loc[s.notna() & (s < 0)].assign(report_delay_days=s).sort_values('report_delay_days', ascending=True).head(25)\n",
    " )\n",
    "\n",
    "summary, top_delays[display_cols], neg_delays[display_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832515ab",
   "metadata": {},
   "source": [
    "## Policy comparison: reporting delay (pre/post 2020)\n",
    "Weâ€™ll summarize `report_delay_days` before vs. after 2020 using robust stats (median/p90/p95/p99) and tail rates (%â‰¥30 days, %â‰¥90 days)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0817d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   Before Report Year 2020        n  mean_days  median_days  p90_days  \\\n",
       " 0                    False  15704.0       3.93          1.0       3.0   \n",
       " 1                     True  10672.0       2.50          1.0       3.0   \n",
       " \n",
       "    p95_days  p99_days  pct_ge_30_days  pct_ge_90_days  \n",
       " 0       4.0      99.0            1.92            1.08  \n",
       " 1       6.0      31.0            1.03            0.28  ,\n",
       "        county  Before Report Year 2020        n  mean_days  median_days  \\\n",
       " 0    GARFIELD                    False   2094.0       2.01          1.0   \n",
       " 1    GARFIELD                     True   1384.0       1.91          1.0   \n",
       " 2  RIO BLANCO                    False    854.0       7.42          1.0   \n",
       " 3  RIO BLANCO                     True   1170.0       3.48          2.0   \n",
       " 4        WELD                    False  12756.0       4.01          0.0   \n",
       " 5        WELD                     True   8118.0       2.45          1.0   \n",
       " \n",
       "    p90_days  p95_days  p99_days  pct_ge_30_days  pct_ge_90_days  \n",
       " 0       3.0      6.00     21.00            0.67            0.29  \n",
       " 1       3.0      6.85     18.36            0.43            0.00  \n",
       " 2       8.0     36.00    202.00            5.62            2.34  \n",
       " 3       4.0     10.00     39.65            3.42            0.51  \n",
       " 4       3.0      3.00    101.45            1.88            1.13  \n",
       " 5       3.0      5.00     27.00            0.79            0.30  ,\n",
       "    Spill Type  Before Report Year 2020       n  mean_days  median_days  \\\n",
       " 0  Historical                    False  7518.0       5.56          0.0   \n",
       " 1  Historical                     True  3776.0       3.59          1.0   \n",
       " 2      Recent                    False  8186.0       2.42          1.0   \n",
       " 3      Recent                     True  6896.0       1.90          1.0   \n",
       " \n",
       "    p90_days  p95_days  p99_days  pct_ge_30_days  pct_ge_90_days  \n",
       " 0       3.0       7.0    165.00            3.17            1.86  \n",
       " 1       3.0       8.0     52.75            2.01            0.74  \n",
       " 2       3.0       3.0     21.00            0.78            0.37  \n",
       " 3       3.0       5.0     21.00            0.49            0.03  )"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pre/post-2020 reporting delay summaries\n",
    "required_cols = ['report_delay_days', 'Before Report Year 2020', 'county', 'Spill Type']\n",
    "missing = [c for c in required_cols if c not in spills_gdf.columns]\n",
    "if missing:\n",
    "    raise KeyError(f\"Missing columns needed for policy comparison: {missing}\")\n",
    "\n",
    "df = spills_gdf.copy()\n",
    "df['Before Report Year 2020'] = df['Before Report Year 2020'].astype('boolean')\n",
    "df['county'] = df['county'].astype('string')\n",
    "df['Spill Type'] = df['Spill Type'].astype('string')\n",
    "df['report_delay_days'] = pd.to_numeric(df['report_delay_days'], errors='coerce')\n",
    "\n",
    "def _summarize(series: pd.Series) -> pd.Series:\n",
    "    s = pd.to_numeric(series, errors='coerce').dropna()\n",
    "    if len(s) == 0:\n",
    "        return pd.Series({\n",
    "            'n': 0,\n",
    "            'mean_days': pd.NA,\n",
    "            'median_days': pd.NA,\n",
    "            'p90_days': pd.NA,\n",
    "            'p95_days': pd.NA,\n",
    "            'p99_days': pd.NA,\n",
    "            'pct_ge_30_days': pd.NA,\n",
    "            'pct_ge_90_days': pd.NA,\n",
    "        })\n",
    "    return pd.Series({\n",
    "        'n': int(s.shape[0]),\n",
    "        'mean_days': float(s.mean()),\n",
    "        'median_days': float(s.median()),\n",
    "        'p90_days': float(s.quantile(0.90)),\n",
    "        'p95_days': float(s.quantile(0.95)),\n",
    "        'p99_days': float(s.quantile(0.99)),\n",
    "        'pct_ge_30_days': float((s >= 30).mean() * 100.0),\n",
    "        'pct_ge_90_days': float((s >= 90).mean() * 100.0),\n",
    "    })\n",
    "\n",
    "def delay_summary(data: pd.DataFrame, group_cols: list[str]) -> pd.DataFrame:\n",
    "    out = (\n",
    "        data.groupby(group_cols, dropna=False)['report_delay_days']\n",
    "        .apply(_summarize)\n",
    "        .unstack()\n",
    "        .reset_index()\n",
    "    )\n",
    "    # consistent ordering: Before (True) then After (False) can be confusing; make it explicit\n",
    "    if 'Before Report Year 2020' in out.columns:\n",
    "        out = out.sort_values(['Before Report Year 2020'] + [c for c in group_cols if c != 'Before Report Year 2020'])\n",
    "    return out\n",
    "\n",
    "overall = delay_summary(df, ['Before Report Year 2020'])\n",
    "by_county = delay_summary(df, ['county', 'Before Report Year 2020']).sort_values(['county', 'Before Report Year 2020'])\n",
    "by_spill_type = delay_summary(df, ['Spill Type', 'Before Report Year 2020']).sort_values(['Spill Type', 'Before Report Year 2020'])\n",
    "\n",
    "# A little rounding for readability\n",
    "for t in (overall, by_county, by_spill_type):\n",
    "    for c in ['mean_days', 'median_days', 'p90_days', 'p95_days', 'p99_days', 'pct_ge_30_days', 'pct_ge_90_days']:\n",
    "        if c in t.columns:\n",
    "            t[c] = t[c].round(2)\n",
    "\n",
    "overall, by_county, by_spill_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6f4a0f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(      period  Spill Type     n  pct_within_period\n",
       " 0  post_2020  Historical  7518              47.87\n",
       " 1  post_2020      Recent  8186              52.13\n",
       " 2   pre_2020  Historical  3776              35.38\n",
       " 3   pre_2020      Recent  6896              64.62,\n",
       "    n__post_2020  n__pre_2020  median_days__post_2020  median_days__pre_2020  \\\n",
       " 0       15704.0      10672.0                     1.0                    1.0   \n",
       " \n",
       "    p95_days__post_2020  p95_days__pre_2020  p99_days__post_2020  \\\n",
       " 0                  4.0                 6.0                 99.0   \n",
       " \n",
       "    p99_days__pre_2020  pct_ge_30_days__post_2020  pct_ge_30_days__pre_2020  \\\n",
       " 0                31.0                       1.92                      1.03   \n",
       " \n",
       "    pct_ge_90_days__post_2020  pct_ge_90_days__pre_2020  \\\n",
       " 0                       1.08                      0.28   \n",
       " \n",
       "    delta_n__post_minus_pre  delta_median_days__post_minus_pre  \\\n",
       " 0                   5032.0                                0.0   \n",
       " \n",
       "    delta_p95_days__post_minus_pre  delta_p99_days__post_minus_pre  \\\n",
       " 0                            -2.0                            68.0   \n",
       " \n",
       "    delta_pct_ge_30_days__post_minus_pre  delta_pct_ge_90_days__post_minus_pre  \n",
       " 0                                  0.89                                   0.8  ,\n",
       "        county  n__post_2020  n__pre_2020  median_days__post_2020  \\\n",
       " 0    GARFIELD        2094.0       1384.0                     1.0   \n",
       " 1  RIO BLANCO         854.0       1170.0                     1.0   \n",
       " 2        WELD       12756.0       8118.0                     0.0   \n",
       " \n",
       "    median_days__pre_2020  p95_days__post_2020  p95_days__pre_2020  \\\n",
       " 0                    1.0                  6.0                6.85   \n",
       " 1                    2.0                 36.0               10.00   \n",
       " 2                    1.0                  3.0                5.00   \n",
       " \n",
       "    p99_days__post_2020  p99_days__pre_2020  pct_ge_30_days__post_2020  \\\n",
       " 0                21.00               18.36                       0.67   \n",
       " 1               202.00               39.65                       5.62   \n",
       " 2               101.45               27.00                       1.88   \n",
       " \n",
       "    pct_ge_30_days__pre_2020  pct_ge_90_days__post_2020  \\\n",
       " 0                      0.43                       0.29   \n",
       " 1                      3.42                       2.34   \n",
       " 2                      0.79                       1.13   \n",
       " \n",
       "    pct_ge_90_days__pre_2020  delta_n__post_minus_pre  \\\n",
       " 0                      0.00                    710.0   \n",
       " 1                      0.51                   -316.0   \n",
       " 2                      0.30                   4638.0   \n",
       " \n",
       "    delta_median_days__post_minus_pre  delta_p95_days__post_minus_pre  \\\n",
       " 0                                0.0                           -0.85   \n",
       " 1                               -1.0                           26.00   \n",
       " 2                               -1.0                           -2.00   \n",
       " \n",
       "    delta_p99_days__post_minus_pre  delta_pct_ge_30_days__post_minus_pre  \\\n",
       " 0                            2.64                                  0.24   \n",
       " 1                          162.35                                  2.20   \n",
       " 2                           74.45                                  1.09   \n",
       " \n",
       "    delta_pct_ge_90_days__post_minus_pre  \n",
       " 0                                  0.29  \n",
       " 1                                  1.83  \n",
       " 2                                  0.83  ,\n",
       "    Spill Type  n__post_2020  n__pre_2020  median_days__post_2020  \\\n",
       " 0  Historical        7518.0       3776.0                     0.0   \n",
       " 1      Recent        8186.0       6896.0                     1.0   \n",
       " \n",
       "    median_days__pre_2020  p95_days__post_2020  p95_days__pre_2020  \\\n",
       " 0                    1.0                  7.0                 8.0   \n",
       " 1                    1.0                  3.0                 5.0   \n",
       " \n",
       "    p99_days__post_2020  p99_days__pre_2020  pct_ge_30_days__post_2020  \\\n",
       " 0                165.0               52.75                       3.17   \n",
       " 1                 21.0               21.00                       0.78   \n",
       " \n",
       "    pct_ge_30_days__pre_2020  pct_ge_90_days__post_2020  \\\n",
       " 0                      2.01                       1.86   \n",
       " 1                      0.49                       0.37   \n",
       " \n",
       "    pct_ge_90_days__pre_2020  delta_n__post_minus_pre  \\\n",
       " 0                      0.74                   3742.0   \n",
       " 1                      0.03                   1290.0   \n",
       " \n",
       "    delta_median_days__post_minus_pre  delta_p95_days__post_minus_pre  \\\n",
       " 0                               -1.0                            -1.0   \n",
       " 1                                0.0                            -2.0   \n",
       " \n",
       "    delta_p99_days__post_minus_pre  delta_pct_ge_30_days__post_minus_pre  \\\n",
       " 0                          112.25                                  1.15   \n",
       " 1                            0.00                                  0.29   \n",
       " \n",
       "    delta_pct_ge_90_days__post_minus_pre  \n",
       " 0                                  1.12  \n",
       " 1                                  0.34  )"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Composition shift + post-minus-pre deltas (pre/post 2020)\n",
    "required_cols = ['report_delay_days', 'Before Report Year 2020', 'county', 'Spill Type']\n",
    "missing = [c for c in required_cols if c not in spills_gdf.columns]\n",
    "if missing:\n",
    "    raise KeyError(f\"Missing columns needed for delta/composition report: {missing}\")\n",
    "\n",
    "data = spills_gdf.copy()\n",
    "data['Before Report Year 2020'] = data['Before Report Year 2020'].astype('boolean')\n",
    "data['period'] = data['Before Report Year 2020'].map({True: 'pre_2020', False: 'post_2020'}).astype('string')\n",
    "data['report_delay_days'] = pd.to_numeric(data['report_delay_days'], errors='coerce')\n",
    "data['county'] = data['county'].astype('string')\n",
    "data['Spill Type'] = data['Spill Type'].astype('string')\n",
    "\n",
    "# 1) Composition table: Spill Type shares pre vs post\n",
    "composition = (\n",
    "    data.groupby(['period', 'Spill Type'], dropna=False)\n",
    "    .size()\n",
    "    .rename('n')\n",
    "    .reset_index()\n",
    ")\n",
    "composition['pct_within_period'] = (\n",
    "    composition['n'] / composition.groupby('period', dropna=False)['n'].transform('sum') * 100.0\n",
    ").round(2)\n",
    "composition = composition.sort_values(['period', 'Spill Type'])\n",
    "\n",
    "# 2) Delta helper: compute post-minus-pre on robust metrics\n",
    "def _metrics(series: pd.Series) -> pd.Series:\n",
    "    s = pd.to_numeric(series, errors='coerce').dropna()\n",
    "    if len(s) == 0:\n",
    "        return pd.Series({\n",
    "            'n': 0,\n",
    "            'median_days': pd.NA,\n",
    "            'p95_days': pd.NA,\n",
    "            'p99_days': pd.NA,\n",
    "            'pct_ge_30_days': pd.NA,\n",
    "            'pct_ge_90_days': pd.NA,\n",
    "        })\n",
    "    return pd.Series({\n",
    "        'n': int(s.shape[0]),\n",
    "        'median_days': float(s.median()),\n",
    "        'p95_days': float(s.quantile(0.95)),\n",
    "        'p99_days': float(s.quantile(0.99)),\n",
    "        'pct_ge_30_days': float((s >= 30).mean() * 100.0),\n",
    "        'pct_ge_90_days': float((s >= 90).mean() * 100.0),\n",
    "    })\n",
    "\n",
    "def deltas(df: pd.DataFrame, group_cols: list[str]) -> pd.DataFrame:\n",
    "    value_cols = ['n', 'median_days', 'p95_days', 'p99_days', 'pct_ge_30_days', 'pct_ge_90_days']\n",
    "    tmp = df.copy()\n",
    "    grp = list(group_cols)\n",
    "    # If no grouping, create a dummy group so unstack always yields one-row output\n",
    "    if len(grp) == 0:\n",
    "        tmp['__all__'] = 'all'\n",
    "        grp = ['__all__']\n",
    "\n",
    "    stats = tmp.groupby(grp + ['period'], dropna=False)['report_delay_days'].apply(_metrics).unstack()\n",
    "    # stats index: grp + period; columns: value_cols. Unstack period into columns (metric, period).\n",
    "    wide = stats.unstack('period')\n",
    "    wide.columns = [f\"{metric}__{period}\" for metric, period in wide.columns]\n",
    "    wide = wide.reset_index()\n",
    "\n",
    "    if '__all__' in wide.columns:\n",
    "        wide = wide.drop(columns=['__all__'])\n",
    "\n",
    "    # Compute deltas: post - pre (where both exist)\n",
    "    for metric in value_cols:\n",
    "        pre = f\"{metric}__pre_2020\"\n",
    "        post = f\"{metric}__post_2020\"\n",
    "        if pre in wide.columns and post in wide.columns:\n",
    "            wide[f\"delta_{metric}__post_minus_pre\"] = wide[post] - wide[pre]\n",
    "\n",
    "    # Round for readability\n",
    "    round_cols = [c for c in wide.columns if c.startswith('delta_') or c.endswith('__pre_2020') or c.endswith('__post_2020')]\n",
    "    for c in round_cols:\n",
    "        if c.startswith('n__') or c.startswith('delta_n__'):\n",
    "            continue\n",
    "        wide[c] = wide[c].round(2)\n",
    "    return wide\n",
    "\n",
    "delta_overall = deltas(data, [])\n",
    "delta_by_county = deltas(data, ['county']).sort_values('county')\n",
    "delta_by_spill_type = deltas(data, ['Spill Type']).sort_values('Spill Type')\n",
    "\n",
    "composition, delta_overall, delta_by_county, delta_by_spill_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b6bbb44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(     group  threshold_days  n_ge  pct_ge\n",
       " 0  overall              30   412    1.56\n",
       " 1  overall              60   252    0.96\n",
       " 2  overall              90   200    0.76\n",
       " 3  overall              99   188    0.71\n",
       " 4  overall             180    96    0.36\n",
       " 5  overall             365    30    0.11,\n",
       "                group  threshold_days  n_ge  pct_ge\n",
       " 0   period=post_2020              30   302    1.92\n",
       " 1   period=post_2020              60   208    1.32\n",
       " 2   period=post_2020              90   170    1.08\n",
       " 3   period=post_2020              99   166    1.06\n",
       " 4   period=post_2020             180    82    0.52\n",
       " 5   period=post_2020             365    30    0.19\n",
       " 6    period=pre_2020              30   110    1.03\n",
       " 7    period=pre_2020              60    44    0.41\n",
       " 8    period=pre_2020              90    30    0.28\n",
       " 9    period=pre_2020              99    22    0.21\n",
       " 10   period=pre_2020             180    14    0.13\n",
       " 11   period=pre_2020             365     0    0.00)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Frequency check: counts/percents above key delay thresholds\n",
    "required_cols = ['report_delay_days', 'Before Report Year 2020']\n",
    "missing = [c for c in required_cols if c not in spills_gdf.columns]\n",
    "if missing:\n",
    "    raise KeyError(f\"Missing columns for threshold frequency check: {missing}\")\n",
    "\n",
    "thresholds = [30, 60, 90, 99, 180, 365]\n",
    "\n",
    "df = spills_gdf.copy()\n",
    "df['Before Report Year 2020'] = df['Before Report Year 2020'].astype('boolean')\n",
    "df['period'] = df['Before Report Year 2020'].map({True: 'pre_2020', False: 'post_2020'}).astype('string')\n",
    "s = pd.to_numeric(df['report_delay_days'], errors='coerce')\n",
    "if s.isna().any():\n",
    "    # Shouldn't happen based on earlier work, but keep it safe\n",
    "    df = df.loc[s.notna()].copy()\n",
    "    s = pd.to_numeric(df['report_delay_days'], errors='coerce')\n",
    "\n",
    "def threshold_table(values: pd.Series, label: str) -> pd.DataFrame:\n",
    "    v = pd.to_numeric(values, errors='coerce')\n",
    "    n_total = int(v.notna().sum())\n",
    "    rows = []\n",
    "    for t in thresholds:\n",
    "        n_ge = int((v >= t).sum())\n",
    "        pct_ge = (n_ge / n_total * 100.0) if n_total else float('nan')\n",
    "        rows.append({'group': label, 'threshold_days': t, 'n_ge': n_ge, 'pct_ge': pct_ge})\n",
    "    out = pd.DataFrame(rows)\n",
    "    out['pct_ge'] = out['pct_ge'].round(2)\n",
    "    return out\n",
    "\n",
    "overall_thresh = threshold_table(s, 'overall')\n",
    "by_period_thresh = pd.concat(\n",
    "    [threshold_table(pd.to_numeric(g['report_delay_days'], errors='coerce'), f\"period={period}\") for period, g in df.groupby('period', dropna=False)],\n",
    "    ignore_index=True,\n",
    ")\n",
    "\n",
    "overall_thresh, by_period_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed041585",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_88141/1867456031.py:158: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  out = tmp.groupby(group_col, dropna=False).apply(_agg).reset_index()\n",
      "/tmp/ipykernel_88141/1867456031.py:158: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  out = tmp.groupby(group_col, dropna=False).apply(_agg).reset_index()\n",
      "/tmp/ipykernel_88141/1867456031.py:158: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  out = tmp.groupby(group_col, dropna=False).apply(_agg).reset_index()\n",
      "/tmp/ipykernel_88141/1867456031.py:368: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(_agg_rp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote report: /home/dadams/Repos/colorado_redux/step1_delay_audit.md\n",
      "Outputs dir: /home/dadams/Repos/colorado_redux/analysis_postgis/step1_delay_audit_outputs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(    n_all_rows  n_nonmissing_delay  delay_days_value  n_exact  \\\n",
       " 0        26376               26376                 0    10562   \n",
       " 1        26376               26376                 1     9300   \n",
       " 2        26376               26376                 2     2896   \n",
       " 3        26376               26376                 3     1894   \n",
       " 4        26376               26376                 4      354   \n",
       " 5        26376               26376                 5      190   \n",
       " 6        26376               26376                 6      110   \n",
       " 7        26376               26376                 7      128   \n",
       " 8        26376               26376                14       32   \n",
       " 9        26376               26376                21       22   \n",
       " 10       26376               26376                30        6   \n",
       " 11       26376               26376                60        0   \n",
       " 12       26376               26376                90        0   \n",
       " 13       26376               26376                98        2   \n",
       " 14       26376               26376                99       10   \n",
       " 15       26376               26376               100        2   \n",
       " 16       26376               26376               180        2   \n",
       " 17       26376               26376               365        8   \n",
       " \n",
       "     pct_of_all_rows  pct_of_nonmissing_delay  \n",
       " 0           40.0440                  40.0440  \n",
       " 1           35.2593                  35.2593  \n",
       " 2           10.9797                  10.9797  \n",
       " 3            7.1808                   7.1808  \n",
       " 4            1.3421                   1.3421  \n",
       " 5            0.7204                   0.7204  \n",
       " 6            0.4170                   0.4170  \n",
       " 7            0.4853                   0.4853  \n",
       " 8            0.1213                   0.1213  \n",
       " 9            0.0834                   0.0834  \n",
       " 10           0.0227                   0.0227  \n",
       " 11           0.0000                   0.0000  \n",
       " 12           0.0000                   0.0000  \n",
       " 13           0.0076                   0.0076  \n",
       " 14           0.0379                   0.0379  \n",
       " 15           0.0076                   0.0076  \n",
       " 16           0.0076                   0.0076  \n",
       " 17           0.0303                   0.0303  ,\n",
       "     delay_days_int  n_exact  pct_of_nonmissing_delay  pct_of_all_rows\n",
       " 0               90        0                   0.0000           0.0000\n",
       " 1               91        0                   0.0000           0.0000\n",
       " 2               92        6                   0.0227           0.0227\n",
       " 3               93        0                   0.0000           0.0000\n",
       " 4               94        0                   0.0000           0.0000\n",
       " 5               95        2                   0.0076           0.0076\n",
       " 6               96        2                   0.0076           0.0076\n",
       " 7               97        0                   0.0000           0.0000\n",
       " 8               98        2                   0.0076           0.0076\n",
       " 9               99       10                   0.0379           0.0379\n",
       " 10             100        2                   0.0076           0.0076\n",
       " 11             101        2                   0.0076           0.0076\n",
       " 12             102        2                   0.0076           0.0076\n",
       " 13             103        2                   0.0076           0.0076\n",
       " 14             104        0                   0.0000           0.0000\n",
       " 15             105        2                   0.0076           0.0076\n",
       " 16             106        0                   0.0000           0.0000\n",
       " 17             107        0                   0.0000           0.0000\n",
       " 18             108        0                   0.0000           0.0000\n",
       " 19             109        2                   0.0076           0.0076\n",
       " 20             110        0                   0.0000           0.0000,\n",
       "     delay_days_int      n  pct_of_all_rows\n",
       " 0                0  10562           40.044\n",
       " 1                1   9300          35.2593\n",
       " 2                2   2896          10.9797\n",
       " 3                3   1894           7.1808\n",
       " 4                4    354           1.3421\n",
       " 5                5    190           0.7204\n",
       " 6                7    128           0.4853\n",
       " 7                6    110            0.417\n",
       " 8                8     80           0.3033\n",
       " 9                9     52           0.1971\n",
       " 10              10     44           0.1668\n",
       " 11              12     42           0.1592\n",
       " 12              15     38           0.1441\n",
       " 13              14     32           0.1213\n",
       " 14              11     30           0.1137\n",
       " 15              17     26           0.0986\n",
       " 16              18     24            0.091\n",
       " 17              13     24            0.091\n",
       " 18              21     22           0.0834\n",
       " 19              33     18           0.0682,\n",
       "       period  n_total  n_true  pct_true_within_group  pct_true_of_total_rows\n",
       " 0  post_2020    15704      10                 0.0637                  0.0379\n",
       " 1   pre_2020    10672       0                 0.0000                  0.0000,\n",
       "       period  n_total  n_true  pct_true_within_group  pct_true_of_total_rows\n",
       " 0  post_2020    15704     166                 1.0571                  0.6294\n",
       " 1   pre_2020    10672      22                 0.2061                  0.0834,\n",
       "       period  spill_type  n_total  n_true  pct_true_within_group  \\\n",
       " 0  post_2020  Historical     7518       4                 0.0532   \n",
       " 1  post_2020      Recent     8186       6                 0.0733   \n",
       " 2   pre_2020  Historical     3776       0                 0.0000   \n",
       " 3   pre_2020      Recent     6896       0                 0.0000   \n",
       " \n",
       "    pct_true_of_total_rows  \n",
       " 0                  0.0152  \n",
       " 1                  0.0227  \n",
       " 2                  0.0000  \n",
       " 3                  0.0000  ,\n",
       "       period  spill_type  n_total  n_true  pct_true_within_group  \\\n",
       " 0  post_2020  Historical     7518     136                 1.8090   \n",
       " 1  post_2020      Recent     8186      30                 0.3665   \n",
       " 2   pre_2020  Historical     3776      22                 0.5826   \n",
       " 3   pre_2020      Recent     6896       0                 0.0000   \n",
       " \n",
       "    pct_true_of_total_rows  \n",
       " 0                  0.5156  \n",
       " 1                  0.1137  \n",
       " 2                  0.0834  \n",
       " 3                  0.0000  ,\n",
       "                                        operator  n_total  pct_ge_99  \\\n",
       " 89                             NOBLE ENERGY INC   5430.0     1.1786   \n",
       " 68              KERR MCGEE OIL & GAS ONSHORE LP   4714.0     0.4243   \n",
       " 96                               PDC ENERGY INC   2454.0     0.3260   \n",
       " 21                          CAERUS PICEANCE LLC   2194.0     0.1823   \n",
       " 20   BONANZA CREEK ENERGY OPERATING COMPANY LLC   1130.0     0.0000   \n",
       " 71                      KP KAUFFMAN COMPANY INC    940.0     2.3404   \n",
       " 28                              CHEVRON USA INC    872.0     1.6055   \n",
       " 61              HIGHPOINT OPERATING CORPORATION    856.0     0.0000   \n",
       " 34        CRESTONE PEAK RESOURCES OPERATING LLC    694.0     0.2882   \n",
       " 124                      TEP ROCKY MOUNTAIN LLC    592.0     0.0000   \n",
       " 48                     EXTRACTION OIL & GAS INC    494.0     1.6194   \n",
       " 137               WHITING OIL & GAS CORPORATION    446.0     0.0000   \n",
       " 39                     DCP OPERATING COMPANY LP    400.0     0.0000   \n",
       " 73                           LARAMIE ENERGY LLC    286.0     0.0000   \n",
       " 67                     KERR MCGEE GATHERING LLC    250.0     0.0000   \n",
       " 38                             DCP MIDSTREAM LP    208.0     0.0000   \n",
       " 57          GREAT WESTERN OPERATING COMPANY LLC    206.0     1.9417   \n",
       " 53      FUNDARE RESOURCES OPERATING COMPANY LLC    196.0     1.0204   \n",
       " 141                              XTO ENERGY INC    184.0     0.0000   \n",
       " 140               WPX ENERGY ROCKY MOUNTAIN LLC    164.0     0.0000   \n",
       " 51             FOUNDATION ENERGY MANAGEMENT LLC    160.0     2.5000   \n",
       " 117                              SRC ENERGY INC    158.0     2.5316   \n",
       " 133                        VERDAD RESOURCES LLC    144.0     0.0000   \n",
       " 9                     BARRETT CORPORATION* BILL    140.0     0.0000   \n",
       " 56                    GRAND RIVER GATHERING LLC    138.0     0.0000   \n",
       " \n",
       "      pct_is_99  median_delay  p95_delay  p99_delay  \n",
       " 89      0.0000           1.0       3.00     114.00  \n",
       " 68      0.0424           0.0       3.00      17.61  \n",
       " 96      0.0000           1.0       3.00      27.00  \n",
       " 21      0.0912           1.0       3.00      18.28  \n",
       " 20      0.0000           1.0       4.00       9.00  \n",
       " 71      0.6383           1.0      31.00     435.87  \n",
       " 28      0.0000           2.0       7.00     202.00  \n",
       " 61      0.0000           1.0       3.00       5.00  \n",
       " 34      0.0000           1.0       3.00      17.00  \n",
       " 124     0.0000           1.0       3.00       4.00  \n",
       " 48      0.0000           1.0       3.00     113.00  \n",
       " 137     0.0000           1.0       4.00      35.00  \n",
       " 39      0.0000           1.0      13.05      27.00  \n",
       " 73      0.0000           1.0      17.00      65.00  \n",
       " 67      0.0000           1.0       7.00      36.00  \n",
       " 38      0.0000           1.0       9.00      26.00  \n",
       " 57      0.0000           1.0       6.00     103.00  \n",
       " 53      0.0000           1.0       5.00      20.20  \n",
       " 141     0.0000           1.0       3.00      10.99  \n",
       " 140     0.0000           0.0       1.00       1.00  \n",
       " 51      0.0000           2.0       7.00     176.00  \n",
       " 117     0.0000           1.0       6.90     303.00  \n",
       " 133     0.0000           1.0       3.00       5.00  \n",
       " 9       0.0000           1.0      29.00      42.00  \n",
       " 56      0.0000           1.0       5.00      25.00  ,\n",
       "                                     operator  n_total  pct_ge_99  pct_is_99  \\\n",
       " 130        UTAH GAS OP LTD DBA UTAH GAS CORP    122.0     6.5574     0.0000   \n",
       " 15                    BISON IV OPERATING LLC     70.0     5.7143     0.0000   \n",
       " 117                           SRC ENERGY INC    158.0     2.5316     0.0000   \n",
       " 51          FOUNDATION ENERGY MANAGEMENT LLC    160.0     2.5000     0.0000   \n",
       " 71                   KP KAUFFMAN COMPANY INC    940.0     2.3404     0.6383   \n",
       " 57       GREAT WESTERN OPERATING COMPANY LLC    206.0     1.9417     0.0000   \n",
       " 12    BAYSWATER EXPLORATION & PRODUCTION LLC    116.0     1.7241     0.0000   \n",
       " 48                  EXTRACTION OIL & GAS INC    494.0     1.6194     0.0000   \n",
       " 28                           CHEVRON USA INC    872.0     1.6055     0.0000   \n",
       " 89                          NOBLE ENERGY INC   5430.0     1.1786     0.0000   \n",
       " 53   FUNDARE RESOURCES OPERATING COMPANY LLC    196.0     1.0204     0.0000   \n",
       " 68           KERR MCGEE OIL & GAS ONSHORE LP   4714.0     0.4243     0.0424   \n",
       " 96                            PDC ENERGY INC   2454.0     0.3260     0.0000   \n",
       " 34     CRESTONE PEAK RESOURCES OPERATING LLC    694.0     0.2882     0.0000   \n",
       " 21                       CAERUS PICEANCE LLC   2194.0     0.1823     0.0912   \n",
       " \n",
       "      median_delay  p95_delay  p99_delay  \n",
       " 130           1.0      130.0     167.37  \n",
       " 15            2.0      105.2     188.00  \n",
       " 117           1.0        6.9     303.00  \n",
       " 51            2.0        7.0     176.00  \n",
       " 71            1.0       31.0     435.87  \n",
       " 57            1.0        6.0     103.00  \n",
       " 12            2.0       13.0      90.10  \n",
       " 48            1.0        3.0     113.00  \n",
       " 28            2.0        7.0     202.00  \n",
       " 89            1.0        3.0     114.00  \n",
       " 53            1.0        5.0      20.20  \n",
       " 68            0.0        3.0      17.61  \n",
       " 96            1.0        3.0      27.00  \n",
       " 34            1.0        3.0      17.00  \n",
       " 21            1.0        3.0      18.28  ,\n",
       "   county_norm  n_total  pct_ge_99  pct_is_99  median_delay  p95_delay  \\\n",
       " 2        WELD  20874.0     0.7761     0.0383           1.0        4.0   \n",
       " 0    GARFIELD   3478.0     0.1725     0.0000           1.0        6.0   \n",
       " 1  RIO BLANCO   2024.0     0.9881     0.0988           1.0       18.0   \n",
       " \n",
       "    p99_delay  \n",
       " 2       58.0  \n",
       " 0       21.0  \n",
       " 1       92.0  ,\n",
       "   rurality_3  n_total  pct_ge_99  pct_is_99  median_delay  p95_delay  \\\n",
       " 2      Urban  13854.0     0.8084     0.0144           1.0        4.0   \n",
       " 0      Rural  10564.0     0.5869     0.0189           1.0        6.0   \n",
       " 1   Suburban   1958.0     0.7150     0.3064           1.0        8.0   \n",
       " \n",
       "    p99_delay  \n",
       " 2      64.00  \n",
       " 0      50.00  \n",
       " 1      37.45  ,\n",
       "   rurality_3     period  n_total  pct_ge_99  median_delay  p95_delay\n",
       " 0      Rural  post_2020   5900.0     1.0508           1.0        5.0\n",
       " 1      Rural   pre_2020   4664.0     0.0000           1.0        7.0\n",
       " 2   Suburban  post_2020   1230.0     0.9756           1.0        5.0\n",
       " 3   Suburban   pre_2020    728.0     0.2747           1.0        9.0\n",
       " 4      Urban  post_2020   8574.0     1.0730           0.0        3.0\n",
       " 5      Urban   pre_2020   5280.0     0.3788           1.0        4.0,\n",
       " Empty DataFrame\n",
       " Columns: [period, spill_type, county_norm, operator, Date of Discovery, Initial Report Date, delay_days, delay_recomputed_days, abs_diff_days]\n",
       " Index: [],\n",
       "       period  spill_type     n  n_disc_missing  n_rpt_missing  \\\n",
       " 0  post_2020  Historical  7518               0              0   \n",
       " 1  post_2020      Recent  8186               0              0   \n",
       " 2   pre_2020  Historical  3776               0              0   \n",
       " 3   pre_2020      Recent  6896               0              0   \n",
       " \n",
       "    pct_disc_missing  pct_rpt_missing  \n",
       " 0               0.0              0.0  \n",
       " 1               0.0              0.0  \n",
       " 2               0.0              0.0  \n",
       " 3               0.0              0.0  ,\n",
       "        group      n  pct_disc_missing  pct_rpt_missing\n",
       " 0      is_99     10               0.0              0.0\n",
       " 1  not_is_99  26366               0.0              0.0\n",
       " 2      ge_99    188               0.0              0.0\n",
       " 3      lt_99  26188               0.0              0.0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1 measurement audit for reporting delay (Colorado spills)\n",
    "# Goal: diagnose heaping/top-coding/placeholder behavior (esp. 99+) and where it concentrates.\n",
    "# NOTE: This step does NOT filter/cap/winsorize anything.\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "\n",
    "def _pick_col(df: pd.DataFrame, candidates: list[str]) -> str | None:\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "\n",
    "def _as_numeric(s: pd.Series) -> pd.Series:\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "\n",
    "def _to_markdown_safe(df: pd.DataFrame, index: bool = False) -> str:\n",
    "    try:\n",
    "        return df.to_markdown(index=index)\n",
    "    except Exception:\n",
    "        # Fallback that still embeds nicely in markdown\n",
    "        return \"```\\n\" + df.to_string(index=index) + \"\\n```\"\n",
    "\n",
    "\n",
    "def _ensure_period(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    if \"Before Report Year 2020\" in out.columns:\n",
    "        out[\"Before Report Year 2020\"] = out[\"Before Report Year 2020\"].astype(\"boolean\")\n",
    "        out[\"period\"] = out[\"Before Report Year 2020\"].map({True: \"pre_2020\", False: \"post_2020\"}).astype(\"string\")\n",
    "        return out\n",
    "\n",
    "    # Fallback if only year exists\n",
    "    if \"Report Year\" in out.columns:\n",
    "        out[\"period\"] = np.where(out[\"Report Year\"].astype(\"Int64\") < 2020, \"pre_2020\", \"post_2020\")\n",
    "        out[\"period\"] = pd.Series(out[\"period\"]).astype(\"string\")\n",
    "        return out\n",
    "\n",
    "    raise KeyError(\"Need either 'Before Report Year 2020' or 'Report Year' to define period.\")\n",
    "\n",
    "\n",
    "def _derive_delay_fields(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "\n",
    "    delay_col = _pick_col(out, [\"delay_days\", \"report_delay_days\", \"Delay Days\", \"delay\", \"report_delay\"])\n",
    "    if delay_col is None:\n",
    "        raise KeyError(\"Could not find a delay column. Expected one of: delay_days/report_delay_days.\")\n",
    "\n",
    "    out[\"delay_days\"] = _as_numeric(out[delay_col])\n",
    "\n",
    "    # Integer-ish version for exact-value heaping checks.\n",
    "    # Only treat as integer when it's extremely close to a whole number.\n",
    "    frac = (out[\"delay_days\"] - np.floor(out[\"delay_days\"])).abs()\n",
    "    out[\"delay_days_int\"] = pd.Series(pd.NA, index=out.index, dtype=\"Int64\")\n",
    "    mask_int = out[\"delay_days\"].notna() & (frac < 1e-9)\n",
    "    out.loc[mask_int, \"delay_days_int\"] = out.loc[mask_int, \"delay_days\"].astype(\"Int64\")\n",
    "\n",
    "    out[\"is_non_integer_delay\"] = out[\"delay_days\"].notna() & out[\"delay_days_int\"].isna()\n",
    "    return out\n",
    "\n",
    "\n",
    "def _heaping_table(df: pd.DataFrame, values: list[int]) -> pd.DataFrame:\n",
    "    n_rows = int(df.shape[0])\n",
    "    n_delay_nonmissing = int(df[\"delay_days\"].notna().sum())\n",
    "\n",
    "    rows = []\n",
    "    for v in values:\n",
    "        n_exact = int((df[\"delay_days_int\"] == v).sum())\n",
    "        rows.append(\n",
    "            {\n",
    "                \"delay_days_value\": v,\n",
    "                \"n_exact\": n_exact,\n",
    "                \"pct_of_all_rows\": round((n_exact / n_rows * 100.0) if n_rows else float(\"nan\"), 4),\n",
    "                \"pct_of_nonmissing_delay\": round((n_exact / n_delay_nonmissing * 100.0) if n_delay_nonmissing else float(\"nan\"), 4),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    out = pd.DataFrame(rows)\n",
    "    out.insert(0, \"n_all_rows\", n_rows)\n",
    "    out.insert(1, \"n_nonmissing_delay\", n_delay_nonmissing)\n",
    "    return out\n",
    "\n",
    "\n",
    "def _top_k_values(df: pd.DataFrame, k: int = 20) -> pd.DataFrame:\n",
    "    vc = df[\"delay_days_int\"].value_counts(dropna=False).rename(\"n\").reset_index()\n",
    "    vc = vc.rename(columns={\"index\": \"delay_days_int\"})\n",
    "    n_rows = int(df.shape[0])\n",
    "    vc[\"pct_of_all_rows\"] = (vc[\"n\"] / n_rows * 100.0).round(4) if n_rows else float(\"nan\")\n",
    "    return vc.head(k)\n",
    "\n",
    "\n",
    "def _crosstab_indicator(df: pd.DataFrame, group_cols: list[str], indicator_col: str) -> pd.DataFrame:\n",
    "    if indicator_col not in df.columns:\n",
    "        raise KeyError(f\"Missing indicator column: {indicator_col}\")\n",
    "\n",
    "    tmp = df.copy()\n",
    "    tmp[indicator_col] = tmp[indicator_col].fillna(False).astype(bool)\n",
    "\n",
    "    total_n = int(tmp.shape[0])\n",
    "    grp = tmp.groupby(group_cols, dropna=False)\n",
    "\n",
    "    out = grp.agg(\n",
    "        n_total=(indicator_col, \"size\"),\n",
    "        n_true=(indicator_col, \"sum\"),\n",
    "    ).reset_index()\n",
    "\n",
    "    out[\"pct_true_within_group\"] = (out[\"n_true\"] / out[\"n_total\"] * 100.0).round(4)\n",
    "    out[\"pct_true_of_total_rows\"] = (out[\"n_true\"] / total_n * 100.0).round(4) if total_n else float(\"nan\")\n",
    "\n",
    "    # Keep it easy to scan\n",
    "    out = out.sort_values(group_cols)\n",
    "    return out\n",
    "\n",
    "\n",
    "def _summary_by_group(df: pd.DataFrame, group_col: str) -> pd.DataFrame:\n",
    "    # Summary requested: n_total, pct_ge_99, pct_is_99, median_delay, p95_delay, p99_delay\n",
    "    tmp = df.copy()\n",
    "    tmp[\"delay_days\"] = _as_numeric(tmp[\"delay_days\"])\n",
    "\n",
    "    def _agg(g: pd.DataFrame) -> pd.Series:\n",
    "        s = _as_numeric(g[\"delay_days\"]).dropna()\n",
    "        n = int(g.shape[0])\n",
    "        if n == 0:\n",
    "            return pd.Series(\n",
    "                {\n",
    "                    \"n_total\": 0,\n",
    "                    \"pct_ge_99\": pd.NA,\n",
    "                    \"pct_is_99\": pd.NA,\n",
    "                    \"median_delay\": pd.NA,\n",
    "                    \"p95_delay\": pd.NA,\n",
    "                    \"p99_delay\": pd.NA,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return pd.Series(\n",
    "            {\n",
    "                \"n_total\": n,\n",
    "                \"pct_ge_99\": round(float(g[\"ge_99\"].mean() * 100.0), 4),\n",
    "                \"pct_is_99\": round(float(g[\"is_99\"].mean() * 100.0), 4),\n",
    "                \"median_delay\": round(float(s.median()) if len(s) else float(\"nan\"), 4),\n",
    "                \"p95_delay\": round(float(s.quantile(0.95)) if len(s) else float(\"nan\"), 4),\n",
    "                \"p99_delay\": round(float(s.quantile(0.99)) if len(s) else float(\"nan\"), 4),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    out = tmp.groupby(group_col, dropna=False).apply(_agg).reset_index()\n",
    "    out = out.sort_values(\"n_total\", ascending=False)\n",
    "    return out\n",
    "\n",
    "\n",
    "def _missingness_table(df: pd.DataFrame, group_cols: list[str], discovery_col: str, report_col: str) -> pd.DataFrame:\n",
    "    tmp = df.copy()\n",
    "    tmp[\"_disc_missing\"] = tmp[discovery_col].isna()\n",
    "    tmp[\"_rpt_missing\"] = tmp[report_col].isna()\n",
    "\n",
    "    drop_dummy = False\n",
    "    grp_cols = list(group_cols)\n",
    "    if len(grp_cols) == 0:\n",
    "        tmp[\"__all__\"] = \"all\"\n",
    "        grp_cols = [\"__all__\"]\n",
    "        drop_dummy = True\n",
    "\n",
    "    total = tmp.groupby(grp_cols, dropna=False).size().rename(\"n\").reset_index()\n",
    "    miss = tmp.groupby(grp_cols, dropna=False).agg(\n",
    "        n_disc_missing=(\"_disc_missing\", \"sum\"),\n",
    "        n_rpt_missing=(\"_rpt_missing\", \"sum\"),\n",
    "    ).reset_index()\n",
    "\n",
    "    out = total.merge(miss, on=grp_cols, how=\"left\")\n",
    "    out[\"pct_disc_missing\"] = (out[\"n_disc_missing\"] / out[\"n\"] * 100.0).round(4)\n",
    "    out[\"pct_rpt_missing\"] = (out[\"n_rpt_missing\"] / out[\"n\"] * 100.0).round(4)\n",
    "\n",
    "    if drop_dummy:\n",
    "        out = out.drop(columns=[\"__all__\"])\n",
    "    else:\n",
    "        out = out.sort_values(grp_cols)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Inputs / assumptions\n",
    "# -----------------------------\n",
    "\n",
    "if \"spills_gdf\" not in globals():\n",
    "    raise NameError(\"spills_gdf is not defined. Run the earlier cells to load and engineer report_delay_days first.\")\n",
    "\n",
    "base = spills_gdf.copy()\n",
    "base = _ensure_period(base)\n",
    "base = _derive_delay_fields(base)\n",
    "\n",
    "spill_type_col = _pick_col(base, [\"Spill Type\", \"spill_type\", \"spilltype\"])\n",
    "operator_col = _pick_col(base, [\"Operator Name\", \"Operator\", \"operator_name\", \"operator\", \"operator_id\"])\n",
    "county_col = _pick_col(base, [\"county\", \"County\"])  # county already normalized earlier\n",
    "\n",
    "# Optional rurality/RUCA handling\n",
    "ruca_col = _pick_col(base, [\"rurality\", \"Rurality\", \"RUCA\", \"ruca\", \"ruca_code\", \"ruca_primary\", \"ruca1\", \"ruca_prim\"])\n",
    "\n",
    "base[\"spill_type\"] = base[spill_type_col].astype(\"string\") if spill_type_col else pd.Series(pd.NA, index=base.index, dtype=\"string\")\n",
    "base[\"operator\"] = base[operator_col].astype(\"string\") if operator_col else pd.Series(pd.NA, index=base.index, dtype=\"string\")\n",
    "base[\"county_norm\"] = base[county_col].astype(\"string\") if county_col else pd.Series(pd.NA, index=base.index, dtype=\"string\")\n",
    "\n",
    "# If RUCA exists and looks numeric 1â€“10, create a 3-level rurality for requested summary.\n",
    "base[\"rurality_3\"] = pd.Series(pd.NA, index=base.index, dtype=\"string\")\n",
    "if ruca_col:\n",
    "    ruca_raw = base[ruca_col]\n",
    "    ruca_num = pd.to_numeric(ruca_raw, errors=\"coerce\")\n",
    "    # Heuristic: if values mostly in 1â€“10, map to (urban/suburban/rural)\n",
    "    if ruca_num.notna().mean() > 0.5 and ruca_num.dropna().between(1, 10).mean() > 0.9:\n",
    "        # Simple, explicit mapping assumption (documented in report)\n",
    "        # 1â€“3: urban; 4â€“6: suburban; 7â€“10: rural\n",
    "        base[\"rurality_3\"] = pd.cut(\n",
    "            ruca_num,\n",
    "            bins=[0, 3, 6, 10],\n",
    "            labels=[\"urban\", \"suburban\", \"rural\"],\n",
    "            include_lowest=True,\n",
    "        ).astype(\"string\")\n",
    "    else:\n",
    "        # Treat as already categorical if itâ€™s non-numeric (e.g., \"Rural/Suburban/Urban\")\n",
    "        if ruca_raw.dtype == object or str(ruca_raw.dtype).startswith(\"string\"):\n",
    "            base[\"rurality_3\"] = ruca_raw.astype(\"string\")\n",
    "\n",
    "# Create indicators requested\n",
    "base[\"is_99\"] = base[\"delay_days_int\"] == 99\n",
    "base[\"ge_99\"] = base[\"delay_days\"] >= 99\n",
    "base[\"ge_90\"] = base[\"delay_days\"] >= 90\n",
    "base[\"ge_30\"] = base[\"delay_days\"] >= 30\n",
    "\n",
    "n_all = int(base.shape[0])\n",
    "n_nonmissing_delay = int(base[\"delay_days\"].notna().sum())\n",
    "n_non_integer_delay = int(base[\"is_non_integer_delay\"].sum())\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# A) Heaping / spikes at key values\n",
    "# -----------------------------\n",
    "\n",
    "key_values = [0, 1, 2, 3, 4, 5, 6, 7, 14, 21, 30, 60, 90, 98, 99, 100, 180, 365]\n",
    "heaping_key = _heaping_table(base, key_values)\n",
    "\n",
    "neighborhood = pd.DataFrame(\n",
    "    {\n",
    "        \"delay_days_int\": list(range(90, 111)),\n",
    "        \"n_exact\": [int((base[\"delay_days_int\"] == v).sum()) for v in range(90, 111)],\n",
    "    }\n",
    ")\n",
    "neighborhood[\"pct_of_nonmissing_delay\"] = (\n",
    "    (neighborhood[\"n_exact\"] / n_nonmissing_delay * 100.0).round(4) if n_nonmissing_delay else float(\"nan\")\n",
    ")\n",
    "neighborhood[\"pct_of_all_rows\"] = ((neighborhood[\"n_exact\"] / n_all * 100.0).round(4) if n_all else float(\"nan\"))\n",
    "\n",
    "# Top 20 most frequent integer delay values overall (includes <NA> bucket if many non-integers)\n",
    "top20_values = _top_k_values(base, k=20)\n",
    "\n",
    "# Plot: neighborhood around 99 and top 20 values\n",
    "OUTPUT_DIR = (Path.cwd().resolve().parent / \"analysis_postgis\" / \"step1_delay_audit_outputs\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "plot_neighborhood_path = OUTPUT_DIR / \"delay_days_neighborhood_90_110.png\"\n",
    "plot_top20_path = OUTPUT_DIR / \"delay_days_top20.png\"\n",
    "\n",
    "try:\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    ax.bar(neighborhood[\"delay_days_int\"].astype(int), neighborhood[\"n_exact\"].astype(int))\n",
    "    ax.axvline(99, color=\"black\", linewidth=1)\n",
    "    ax.set_title(\"Delay days exact counts (90â€“110), integer-only\")\n",
    "    ax.set_xlabel(\"delay_days\")\n",
    "    ax.set_ylabel(\"count\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(plot_neighborhood_path, dpi=150)\n",
    "    plt.close(fig)\n",
    "except Exception:\n",
    "    plot_neighborhood_path = None\n",
    "\n",
    "try:\n",
    "    # Drop NA bucket for nicer plotting\n",
    "    top20_plot = top20_values.copy()\n",
    "    top20_plot = top20_plot[top20_plot[\"delay_days_int\"].notna()].copy()\n",
    "    top20_plot[\"delay_days_int\"] = top20_plot[\"delay_days_int\"].astype(int)\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.bar(top20_plot[\"delay_days_int\"].astype(str), top20_plot[\"n\"].astype(int))\n",
    "    ax.set_title(\"Top 20 most frequent delay_days (integer-only)\")\n",
    "    ax.set_xlabel(\"delay_days\")\n",
    "    ax.set_ylabel(\"count\")\n",
    "    ax.tick_params(axis=\"x\", rotation=45)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(plot_top20_path, dpi=150)\n",
    "    plt.close(fig)\n",
    "except Exception:\n",
    "    plot_top20_path = None\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# B) Where the 99+ values live (concentration)\n",
    "# -----------------------------\n",
    "\n",
    "# Cross-tabs for is_99 and ge_99 by:\n",
    "# - period\n",
    "# - spill_type\n",
    "# - period x spill_type\n",
    "\n",
    "ct_is99_by_period = _crosstab_indicator(base, [\"period\"], \"is_99\")\n",
    "ct_ge99_by_period = _crosstab_indicator(base, [\"period\"], \"ge_99\")\n",
    "\n",
    "ct_is99_by_type = _crosstab_indicator(base, [\"spill_type\"], \"is_99\")\n",
    "ct_ge99_by_type = _crosstab_indicator(base, [\"spill_type\"], \"ge_99\")\n",
    "\n",
    "ct_is99_by_period_type = _crosstab_indicator(base, [\"period\", \"spill_type\"], \"is_99\")\n",
    "ct_ge99_by_period_type = _crosstab_indicator(base, [\"period\", \"spill_type\"], \"ge_99\")\n",
    "\n",
    "# Operator summaries\n",
    "operator_summary = pd.DataFrame()\n",
    "operator_top25 = pd.DataFrame()\n",
    "operator_top15_by_pct_ge99 = pd.DataFrame()\n",
    "\n",
    "if operator_col:\n",
    "    operator_summary = _summary_by_group(base, \"operator\")\n",
    "    operator_top25 = operator_summary.head(25).copy()\n",
    "\n",
    "    operator_top15_by_pct_ge99 = operator_summary[operator_summary[\"n_total\"] >= 50].copy()\n",
    "    operator_top15_by_pct_ge99 = operator_top15_by_pct_ge99.sort_values(\"pct_ge_99\", ascending=False).head(15)\n",
    "\n",
    "# Geography summaries (county)\n",
    "county_summary = pd.DataFrame()\n",
    "county_top = pd.DataFrame()\n",
    "\n",
    "if county_col:\n",
    "    county_summary = _summary_by_group(base, \"county_norm\")\n",
    "    # \"top counties by n\" â€“ keep 25 for scanability\n",
    "    county_top = county_summary.head(25).copy()\n",
    "\n",
    "# Rurality summaries\n",
    "rurality_summary = pd.DataFrame()\n",
    "rurality_by_period = pd.DataFrame()\n",
    "\n",
    "if base[\"rurality_3\"].notna().any():\n",
    "    rurality_summary = _summary_by_group(base, \"rurality_3\").sort_values(\"n_total\", ascending=False)\n",
    "\n",
    "    # rurality x period\n",
    "    tmp = base.copy()\n",
    "\n",
    "    def _agg_rp(g: pd.DataFrame) -> pd.Series:\n",
    "        s = _as_numeric(g[\"delay_days\"]).dropna()\n",
    "        n = int(g.shape[0])\n",
    "        return pd.Series(\n",
    "            {\n",
    "                \"n_total\": n,\n",
    "                \"pct_ge_99\": round(float(g[\"ge_99\"].mean() * 100.0), 4) if n else float(\"nan\"),\n",
    "                \"median_delay\": round(float(s.median()) if len(s) else float(\"nan\"), 4),\n",
    "                \"p95_delay\": round(float(s.quantile(0.95)) if len(s) else float(\"nan\"), 4),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    rurality_by_period = (\n",
    "        tmp.groupby([\"rurality_3\", \"period\"], dropna=False)\n",
    "        .apply(_agg_rp)\n",
    "        .reset_index()\n",
    "        .sort_values([\"rurality_3\", \"period\"])\n",
    "    )\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# C) Sanity checks on date construction\n",
    "# -----------------------------\n",
    "\n",
    "discovery_col = _pick_col(base, [\"Date of Discovery\", \"discovery_date\", \"Discovery Date\", \"date_of_discovery\"])\n",
    "report_col = _pick_col(base, [\"Initial Report Date\", \"report_date\", \"Report Date\", \"initial_report_date\"])\n",
    "\n",
    "date_sample_n = 0\n",
    "tol = 1e-6\n",
    "\n",
    "date_mismatch_rate = None\n",
    "mismatch_examples = pd.DataFrame()\n",
    "missingness_overall = pd.DataFrame()\n",
    "missingness_by_period_type = pd.DataFrame()\n",
    "missing_correlation = pd.DataFrame()\n",
    "\n",
    "if discovery_col and report_col:\n",
    "    tmp = base.copy()\n",
    "    tmp[discovery_col] = pd.to_datetime(tmp[discovery_col], errors=\"coerce\")\n",
    "    tmp[report_col] = pd.to_datetime(tmp[report_col], errors=\"coerce\")\n",
    "\n",
    "    tmp[\"delay_recomputed_days\"] = (tmp[report_col] - tmp[discovery_col]).dt.total_seconds() / 86400.0\n",
    "\n",
    "    # Compare existing delay_days (numeric) vs recomputed, for rows with both dates and non-missing delay\n",
    "    comparable = tmp[tmp[\"delay_days\"].notna() & tmp[\"delay_recomputed_days\"].notna()].copy()\n",
    "\n",
    "    # Random sample of up to 200 rows\n",
    "    date_sample_n = min(200, comparable.shape[0])\n",
    "    sample = comparable.sample(n=date_sample_n, random_state=42) if date_sample_n > 0 else comparable.head(0)\n",
    "\n",
    "    # Tolerance: allow tiny float differences\n",
    "    mism = (sample[\"delay_days\"] - sample[\"delay_recomputed_days\"]).abs() > tol\n",
    "    date_mismatch_rate = float(mism.mean()) if date_sample_n > 0 else None\n",
    "\n",
    "    mismatch_examples = sample.loc[mism].copy()\n",
    "    mismatch_examples[\"abs_diff_days\"] = (mismatch_examples[\"delay_days\"] - mismatch_examples[\"delay_recomputed_days\"]).abs()\n",
    "\n",
    "    # Keep only a few context columns if present\n",
    "    context_cols = [\n",
    "        \"period\",\n",
    "        \"spill_type\",\n",
    "        \"county_norm\",\n",
    "        \"operator\",\n",
    "        discovery_col,\n",
    "        report_col,\n",
    "        \"delay_days\",\n",
    "        \"delay_recomputed_days\",\n",
    "        \"abs_diff_days\",\n",
    "    ]\n",
    "    context_cols = [c for c in context_cols if c in mismatch_examples.columns]\n",
    "    mismatch_examples = mismatch_examples.sort_values(\"abs_diff_days\", ascending=False)[context_cols].head(20)\n",
    "\n",
    "    # Missingness rates overall and by period/spill_type\n",
    "    missingness_overall = _missingness_table(tmp, [], discovery_col, report_col)\n",
    "    missingness_by_period_type = _missingness_table(tmp, [\"period\", \"spill_type\"], discovery_col, report_col)\n",
    "\n",
    "    # Missingness correlation with is_99/ge_99\n",
    "    tmp[\"disc_missing\"] = tmp[discovery_col].isna()\n",
    "    tmp[\"rpt_missing\"] = tmp[report_col].isna()\n",
    "\n",
    "    def _miss_rate(mask: pd.Series) -> dict:\n",
    "        g = tmp.loc[mask]\n",
    "        n = int(g.shape[0])\n",
    "        return {\n",
    "            \"n\": n,\n",
    "            \"pct_disc_missing\": round(float(g[\"disc_missing\"].mean() * 100.0), 4) if n else float(\"nan\"),\n",
    "            \"pct_rpt_missing\": round(float(g[\"rpt_missing\"].mean() * 100.0), 4) if n else float(\"nan\"),\n",
    "        }\n",
    "\n",
    "    missing_correlation = pd.DataFrame(\n",
    "        [\n",
    "            {\"group\": \"is_99\", **_miss_rate(tmp[\"is_99\"] == True)},\n",
    "            {\"group\": \"not_is_99\", **_miss_rate(tmp[\"is_99\"] == False)},\n",
    "            {\"group\": \"ge_99\", **_miss_rate(tmp[\"ge_99\"] == True)},\n",
    "            {\"group\": \"lt_99\", **_miss_rate(tmp[\"ge_99\"] == False)},\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# D) Narrative bullets (computed, explicit)\n",
    "# -----------------------------\n",
    "\n",
    "# Spike test around 99: compare count at 99 to average of neighbors 95â€“105 excluding 99\n",
    "count_99 = int((base[\"delay_days_int\"] == 99).sum())\n",
    "count_98 = int((base[\"delay_days_int\"] == 98).sum())\n",
    "count_100 = int((base[\"delay_days_int\"] == 100).sum())\n",
    "\n",
    "window_95_105 = neighborhood[(neighborhood[\"delay_days_int\"] >= 95) & (neighborhood[\"delay_days_int\"] <= 105)].copy()\n",
    "mean_neighbors_ex_99 = float(window_95_105.loc[window_95_105[\"delay_days_int\"] != 99, \"n_exact\"].mean()) if len(window_95_105) else float(\"nan\")\n",
    "spike_ratio = (count_99 / mean_neighbors_ex_99) if (mean_neighbors_ex_99 and not math.isnan(mean_neighbors_ex_99) and mean_neighbors_ex_99 > 0) else float(\"nan\")\n",
    "\n",
    "# Pre/post change\n",
    "pre = base[base[\"period\"] == \"pre_2020\"]\n",
    "post = base[base[\"period\"] == \"post_2020\"]\n",
    "\n",
    "pct_is99_pre = float(pre[\"is_99\"].mean() * 100.0) if len(pre) else float(\"nan\")\n",
    "pct_is99_post = float(post[\"is_99\"].mean() * 100.0) if len(post) else float(\"nan\")\n",
    "\n",
    "pct_ge99_pre = float(pre[\"ge_99\"].mean() * 100.0) if len(pre) else float(\"nan\")\n",
    "pct_ge99_post = float(post[\"ge_99\"].mean() * 100.0) if len(post) else float(\"nan\")\n",
    "\n",
    "# Historical vs Recent concentration\n",
    "hist = base[base[\"spill_type\"] == \"Historical\"]\n",
    "rec = base[base[\"spill_type\"] == \"Recent\"]\n",
    "\n",
    "pct_ge99_hist = float(hist[\"ge_99\"].mean() * 100.0) if len(hist) else float(\"nan\")\n",
    "pct_ge99_rec = float(rec[\"ge_99\"].mean() * 100.0) if len(rec) else float(\"nan\")\n",
    "\n",
    "bullets = []\n",
    "bullets.append(f\"Rows total: {n_all:,}. Non-missing delay_days: {n_nonmissing_delay:,}. Non-integer delay_days (not exactly whole-day): {n_non_integer_delay:,}.\")\n",
    "bullets.append(f\"Exact 99 count (integer-only): {count_99:,}. Neighborhood 98={count_98:,}, 100={count_100:,}. Spike ratio vs 95â€“105 (excluding 99): {spike_ratio:.2f} (higher suggests heaping at 99).\")\n",
    "bullets.append(f\"Pre/post: pct_is_99 pre={pct_is99_pre:.3f}% vs post={pct_is99_post:.3f}%. pct_ge_99 pre={pct_ge99_pre:.3f}% vs post={pct_ge99_post:.3f}%.\")\n",
    "if len(hist) and len(rec):\n",
    "    bullets.append(f\"Spill Type concentration: pct_ge_99 Historical={pct_ge99_hist:.3f}% vs Recent={pct_ge99_rec:.3f}% (big gap suggests tail is classification-driven).\")\n",
    "\n",
    "if not operator_top15_by_pct_ge99.empty:\n",
    "    top_op = operator_top15_by_pct_ge99.iloc[0]\n",
    "    bullets.append(\n",
    "        f\"Operator concentration: highest pct_ge_99 among operators with n>=50 is '{top_op['operator']}' at {top_op['pct_ge_99']:.3f}% (n={int(top_op['n_total']):,}).\"\n",
    "    )\n",
    "\n",
    "if not county_top.empty:\n",
    "    top_cty = county_top.iloc[0]\n",
    "    bullets.append(\n",
    "        f\"County concentration: largest-count county is '{top_cty['county_norm']}' (n={int(top_cty['n_total']):,}); its pct_ge_99={top_cty['pct_ge_99']:.3f}%.\"\n",
    "    )\n",
    "\n",
    "if base[\"rurality_3\"].notna().any() and not rurality_summary.empty:\n",
    "    # Identify max pct_ge_99 rurality bucket with decent size\n",
    "    tmp_r = rurality_summary[rurality_summary[\"n_total\"] >= 100].copy()\n",
    "    if not tmp_r.empty:\n",
    "        row = tmp_r.sort_values(\"pct_ge_99\", ascending=False).iloc[0]\n",
    "        bullets.append(\n",
    "            f\"Rurality: bucket '{row['rurality_3']}' has highest pct_ge_99 among n>=100 at {row['pct_ge_99']:.3f}% (n={int(row['n_total']):,}).\"\n",
    "        )\n",
    "\n",
    "if date_mismatch_rate is not None:\n",
    "    bullets.append(f\"Date sanity (random sample n={date_sample_n}): mismatch rate vs recomputed (tol {tol}): {date_mismatch_rate:.3%}.\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Save markdown report (+ optional CSV exports)\n",
    "# -----------------------------\n",
    "\n",
    "REPORT_PATH = Path.cwd().resolve().parent / \"step1_delay_audit.md\"\n",
    "\n",
    "EXPORT_CSV = True\n",
    "\n",
    "csv_paths = {}\n",
    "\n",
    "def _write_csv(name: str, df: pd.DataFrame):\n",
    "    if not EXPORT_CSV:\n",
    "        return\n",
    "    path = OUTPUT_DIR / f\"{name}.csv\"\n",
    "    df.to_csv(path, index=False)\n",
    "    csv_paths[name] = path\n",
    "\n",
    "\n",
    "# Write CSVs (optional)\n",
    "_write_csv(\"A1_heaping_key_values\", heaping_key)\n",
    "_write_csv(\"A2_neighborhood_90_110\", neighborhood)\n",
    "_write_csv(\"A3_top20_delay_values\", top20_values)\n",
    "\n",
    "_write_csv(\"B5_is99_by_period\", ct_is99_by_period)\n",
    "_write_csv(\"B5_ge99_by_period\", ct_ge99_by_period)\n",
    "_write_csv(\"B5_is99_by_spill_type\", ct_is99_by_type)\n",
    "_write_csv(\"B5_ge99_by_spill_type\", ct_ge99_by_type)\n",
    "_write_csv(\"B5_is99_by_period_x_type\", ct_is99_by_period_type)\n",
    "_write_csv(\"B5_ge99_by_period_x_type\", ct_ge99_by_period_type)\n",
    "\n",
    "if operator_col:\n",
    "    _write_csv(\"B6_operator_top25_by_n\", operator_top25)\n",
    "    _write_csv(\"B6_operator_top15_by_pct_ge99_min_n50\", operator_top15_by_pct_ge99)\n",
    "\n",
    "if county_col:\n",
    "    _write_csv(\"B7_county_top_by_n\", county_top)\n",
    "\n",
    "if base[\"rurality_3\"].notna().any():\n",
    "    _write_csv(\"B7_rurality_summary\", rurality_summary)\n",
    "    _write_csv(\"B7_rurality_by_period\", rurality_by_period)\n",
    "\n",
    "if discovery_col and report_col:\n",
    "    _write_csv(\"C8_mismatch_examples\", mismatch_examples)\n",
    "    _write_csv(\"C8_missingness_overall\", missingness_overall)\n",
    "    _write_csv(\"C8_missingness_by_period_type\", missingness_by_period_type)\n",
    "    _write_csv(\"C8_missingness_correlation\", missing_correlation)\n",
    "\n",
    "\n",
    "lines = []\n",
    "lines.append(\"# Step 1 measurement audit for reporting delay (Colorado spills)\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"This report diagnoses whether `delay_days` values (especially 99+) look like true delays vs. top-coding/placeholder/heaping, and where they concentrate.\")\n",
    "lines.append(\"**No filtering/capping/winsorizing is performed in this step.**\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"## Inputs & denominators\")\n",
    "lines.append(f\"- Rows total: {n_all:,}\")\n",
    "lines.append(f\"- Non-missing delay_days: {n_nonmissing_delay:,}\")\n",
    "lines.append(f\"- Non-integer delay_days (not exactly whole-day): {n_non_integer_delay:,}\")\n",
    "lines.append(f\"- Spill type column: `{spill_type_col}`\")\n",
    "lines.append(f\"- Operator column: `{operator_col}`\")\n",
    "lines.append(f\"- County column: `{county_col}`\")\n",
    "lines.append(f\"- RUCA/rurality source column: `{ruca_col}`\")\n",
    "if ruca_col and base[\"rurality_3\"].notna().any() and pd.to_numeric(base[ruca_col], errors='coerce').notna().mean() > 0.5:\n",
    "    lines.append(\"- Rurality mapping used (heuristic): RUCA 1â€“3=urban, 4â€“6=suburban, 7â€“10=rural\")\n",
    "lines.append(\"\")\n",
    "\n",
    "lines.append(\"## A) Heaping / spikes at key values\")\n",
    "lines.append(\"### A1) Exact-value frequency (key values)\")\n",
    "lines.append(_to_markdown_safe(heaping_key, index=False))\n",
    "lines.append(\"\")\n",
    "\n",
    "lines.append(\"### A2) Neighborhood around 99 (90â€“110)\")\n",
    "lines.append(_to_markdown_safe(neighborhood, index=False))\n",
    "lines.append(\"\")\n",
    "if plot_neighborhood_path:\n",
    "    lines.append(f\"Plot saved: `{plot_neighborhood_path}`\")\n",
    "lines.append(\"\")\n",
    "\n",
    "lines.append(\"### A3) Top 20 most frequent delay_days values (integer-only; includes <NA> bucket if many non-integers)\")\n",
    "lines.append(_to_markdown_safe(top20_values, index=False))\n",
    "lines.append(\"\")\n",
    "if plot_top20_path:\n",
    "    lines.append(f\"Plot saved: `{plot_top20_path}`\")\n",
    "lines.append(\"\")\n",
    "\n",
    "lines.append(\"## B) Where the 99+ values live (concentration)\")\n",
    "lines.append(\"### B4) Indicators\")\n",
    "lines.append(\"- `is_99 = (delay_days_int == 99)`\")\n",
    "lines.append(\"- `ge_99 = (delay_days >= 99)`\")\n",
    "lines.append(\"- `ge_90 = (delay_days >= 90)`\")\n",
    "lines.append(\"- `ge_30 = (delay_days >= 30)`\")\n",
    "lines.append(\"\")\n",
    "\n",
    "lines.append(\"### B5) Cross-tabs (n, % within-group, % of total rows)\")\n",
    "lines.append(\"#### is_99 by period\")\n",
    "lines.append(_to_markdown_safe(ct_is99_by_period, index=False))\n",
    "lines.append(\"\")\n",
    "lines.append(\"#### ge_99 by period\")\n",
    "lines.append(_to_markdown_safe(ct_ge99_by_period, index=False))\n",
    "lines.append(\"\")\n",
    "\n",
    "lines.append(\"#### is_99 by spill type\")\n",
    "lines.append(_to_markdown_safe(ct_is99_by_type, index=False))\n",
    "lines.append(\"\")\n",
    "lines.append(\"#### ge_99 by spill type\")\n",
    "lines.append(_to_markdown_safe(ct_ge99_by_type, index=False))\n",
    "lines.append(\"\")\n",
    "\n",
    "lines.append(\"#### is_99 by period Ã— spill type\")\n",
    "lines.append(_to_markdown_safe(ct_is99_by_period_type, index=False))\n",
    "lines.append(\"\")\n",
    "lines.append(\"#### ge_99 by period Ã— spill type\")\n",
    "lines.append(_to_markdown_safe(ct_ge99_by_period_type, index=False))\n",
    "lines.append(\"\")\n",
    "\n",
    "lines.append(\"### B6) By operator\")\n",
    "if operator_col:\n",
    "    lines.append(\"#### Top 25 operators by total spills\")\n",
    "    lines.append(_to_markdown_safe(operator_top25, index=False))\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"#### Top 15 operators by pct_ge_99 (min n >= 50)\")\n",
    "    lines.append(_to_markdown_safe(operator_top15_by_pct_ge99, index=False))\n",
    "else:\n",
    "    lines.append(\"Operator column not found; operator summaries skipped.\")\n",
    "lines.append(\"\")\n",
    "\n",
    "lines.append(\"### B7) By geography\")\n",
    "if county_col:\n",
    "    lines.append(\"#### Top counties by total spills\")\n",
    "    lines.append(_to_markdown_safe(county_top, index=False))\n",
    "else:\n",
    "    lines.append(\"County column not found; county summaries skipped.\")\n",
    "lines.append(\"\")\n",
    "\n",
    "if base[\"rurality_3\"].notna().any():\n",
    "    lines.append(\"#### Rurality summary\")\n",
    "    lines.append(_to_markdown_safe(rurality_summary, index=False))\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"#### Rurality Ã— period\")\n",
    "    lines.append(_to_markdown_safe(rurality_by_period, index=False))\n",
    "    lines.append(\"\")\n",
    "else:\n",
    "    lines.append(\"Rurality/RUCA not available (or not mappable); rurality summaries skipped.\")\n",
    "lines.append(\"\")\n",
    "\n",
    "lines.append(\"## C) Sanity checks on date construction\")\n",
    "if discovery_col and report_col:\n",
    "    lines.append(f\"Using discovery date column `{discovery_col}` and report date column `{report_col}`.\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(f\"- Random-sample mismatch rate (n up to 200; tol={tol}): {date_mismatch_rate:.3%}\" if date_mismatch_rate is not None else \"- Random-sample mismatch rate: NA\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"### C8) Mismatch examples (up to 20)\")\n",
    "    lines.append(_to_markdown_safe(mismatch_examples, index=False))\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"### C8) Missingness (overall)\")\n",
    "    lines.append(_to_markdown_safe(missingness_overall, index=False))\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"### C8) Missingness (by period Ã— spill type)\")\n",
    "    lines.append(_to_markdown_safe(missingness_by_period_type, index=False))\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"### C8) Missingness correlation with 99 / 99+\")\n",
    "    lines.append(_to_markdown_safe(missing_correlation, index=False))\n",
    "else:\n",
    "    lines.append(\"Discovery/report date columns not found; date-construction checks skipped.\")\n",
    "lines.append(\"\")\n",
    "\n",
    "lines.append(\"## D) Conclusions (computed bullets)\")\n",
    "for b in bullets[:10]:\n",
    "    lines.append(f\"- {b}\")\n",
    "\n",
    "lines.append(\"\")\n",
    "lines.append(\"## Outputs\")\n",
    "lines.append(f\"- Markdown report: `{REPORT_PATH}`\")\n",
    "lines.append(f\"- Outputs directory: `{OUTPUT_DIR}`\")\n",
    "if EXPORT_CSV:\n",
    "    lines.append(\"- CSVs written for each table (see outputs directory)\")\n",
    "if plot_neighborhood_path:\n",
    "    lines.append(f\"- PNG: `{plot_neighborhood_path}`\")\n",
    "if plot_top20_path:\n",
    "    lines.append(f\"- PNG: `{plot_top20_path}`\")\n",
    "\n",
    "REPORT_PATH.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Notebook outputs (tables)\n",
    "# -----------------------------\n",
    "\n",
    "print(f\"Wrote report: {REPORT_PATH}\")\n",
    "print(f\"Outputs dir: {OUTPUT_DIR}\")\n",
    "\n",
    "heaping_key, neighborhood, top20_values, ct_is99_by_period, ct_ge99_by_period, ct_is99_by_period_type, ct_ge99_by_period_type, operator_top25, operator_top15_by_pct_ge99, county_top, rurality_summary, rurality_by_period, mismatch_examples, missingness_by_period_type, missing_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33c127bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dadams/Repos/colorado_redux/.venv/lib/python3.13/site-packages/statsmodels/regression/linear_model.py:1884: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(np.diag(self.cov_params()))\n",
      "/home/dadams/Repos/colorado_redux/.venv/lib/python3.13/site-packages/statsmodels/regression/linear_model.py:1884: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(np.diag(self.cov_params()))\n",
      "/home/dadams/Repos/colorado_redux/.venv/lib/python3.13/site-packages/statsmodels/regression/linear_model.py:1884: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(np.diag(self.cov_params()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote report: /home/dadams/Repos/colorado_redux/step2_models_summary.md\n",
      "Wrote outputs under: /home/dadams/Repos/colorado_redux/analysis_postgis/step2_model_outputs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(      period  n_total  pct_historical  pct_recent\n",
       " 0  post_2020    15704          47.873      52.127\n",
       " 1   pre_2020    10672          35.382      64.618,\n",
       "            model                          term  log_odds        se  \\\n",
       " 0  H1_hist_logit                     Intercept -2.800939  0.124871   \n",
       " 1  H1_hist_logit  C(county_norm)[T.RIO BLANCO]  0.705167  0.092303   \n",
       " 2  H1_hist_logit        C(county_norm)[T.WELD]  1.554510  0.052259   \n",
       " 3  H1_hist_logit     C(rurality_3)[T.Suburban]  0.730929  0.198585   \n",
       " 4  H1_hist_logit        C(rurality_3)[T.Urban]  1.420127  0.035143   \n",
       " 5  H1_hist_logit                          post  0.499121  0.071400   \n",
       " \n",
       "    odds_ratio  or_ci95_lo  or_ci95_hi        p_value  \n",
       " 0    0.060753    0.047564    0.077600  1.979905e-111  \n",
       " 1    2.024185    1.689198    2.425605   2.176994e-14  \n",
       " 2    4.732768    4.272003    5.243229  1.932918e-194  \n",
       " 3    2.077009    1.407342    3.065329   2.326025e-04  \n",
       " 4    4.137647    3.862240    4.432692   0.000000e+00  \n",
       " 5    1.647273    1.432152    1.894708   2.738355e-12  ,\n",
       "             model             term      coef        se    z_or_t       p_value\n",
       " 146  H2_ols_log1p             post -0.219885  0.024052 -9.142088  6.125768e-20\n",
       " 147  H2_ols_log1p       historical  0.043168  0.153896  0.280499  7.790944e-01\n",
       " 148  H2_ols_log1p  post:historical  0.007845  0.028785  0.272523  7.852199e-01,\n",
       "              model             term  log_odds        se  odds_ratio  \\\n",
       " 5  H2_logit_late30             post  0.733227  0.645191    2.081787   \n",
       " 6  H2_logit_late30       historical  2.010974  0.620959    7.470592   \n",
       " 7  H2_logit_late30  post:historical -0.255636  0.571480    0.774424   \n",
       " \n",
       "    or_ci95_lo  or_ci95_hi   p_value  \n",
       " 5    0.587812    7.372827  0.255769  \n",
       " 6    2.211993   25.230522  0.001202  \n",
       " 7    0.252653    2.373737  0.654643  ,\n",
       "       period  spill_type      mean   ci95_lo   ci95_hi\n",
       " 0   pre_2020      Recent  0.003650  0.002516  0.005803\n",
       " 1   pre_2020  Historical  0.025917  0.011911  0.055095\n",
       " 2  post_2020      Recent  0.007531  0.001719  0.036008\n",
       " 3  post_2020  Historical  0.040456  0.025171  0.066593)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Modeling + robustness plan (post-2020 mission change; Colorado spills)\n",
    "# Goal: Test hypothesis families about (1) Historical/Recent mix, (2) reporting timeliness, (3) rural lag/heterogeneity.\n",
    "# Guardrails:\n",
    "# - Keep full sample (no outlier deletion in primary models).\n",
    "# - Use transformations/thresholds + robustness caps.\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 0) SETUP / VARIABLES\n",
    "# -----------------------------\n",
    "\n",
    "def _pick_col(df: pd.DataFrame, candidates: list[str]) -> str | None:\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "\n",
    "def _as_numeric(s: pd.Series) -> pd.Series:\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "\n",
    "def _ensure_period(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    if \"period\" in out.columns:\n",
    "        out[\"period\"] = out[\"period\"].astype(\"string\")\n",
    "        return out\n",
    "\n",
    "    if \"Before Report Year 2020\" in out.columns:\n",
    "        out[\"Before Report Year 2020\"] = out[\"Before Report Year 2020\"].astype(\"boolean\")\n",
    "        out[\"period\"] = out[\"Before Report Year 2020\"].map({True: \"pre_2020\", False: \"post_2020\"}).astype(\"string\")\n",
    "        return out\n",
    "\n",
    "    if \"Report Year\" in out.columns:\n",
    "        out[\"period\"] = np.where(out[\"Report Year\"].astype(\"Int64\") < 2020, \"pre_2020\", \"post_2020\")\n",
    "        out[\"period\"] = pd.Series(out[\"period\"]).astype(\"string\")\n",
    "        return out\n",
    "\n",
    "    raise KeyError(\"Need 'period' or ('Before Report Year 2020' or 'Report Year') to define pre/post.\")\n",
    "\n",
    "\n",
    "def _ensure_delay_days(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    delay_col = _pick_col(out, [\"delay_days\", \"report_delay_days\", \"Delay Days\"])\n",
    "    if delay_col is None:\n",
    "        raise KeyError(\"Need a delay column (delay_days or report_delay_days).\")\n",
    "    out[\"delay_days\"] = _as_numeric(out[delay_col])\n",
    "\n",
    "    # Delay integer for heaping checks (not required for modeling)\n",
    "    frac = (out[\"delay_days\"] - np.floor(out[\"delay_days\"])).abs()\n",
    "    out[\"delay_days_int\"] = pd.Series(pd.NA, index=out.index, dtype=\"Int64\")\n",
    "    mask_int = out[\"delay_days\"].notna() & (frac < 1e-9)\n",
    "    out.loc[mask_int, \"delay_days_int\"] = out.loc[mask_int, \"delay_days\"].astype(\"Int64\")\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def _safe_to_markdown(df: pd.DataFrame, index: bool = False) -> str:\n",
    "    try:\n",
    "        return df.to_markdown(index=index)\n",
    "    except Exception:\n",
    "        return \"```\\n\" + df.to_string(index=index) + \"\\n```\"\n",
    "\n",
    "\n",
    "def _coef_table(model, model_name: str) -> pd.DataFrame:\n",
    "    params = model.params.copy()\n",
    "    se = model.bse.copy()\n",
    "    out = pd.DataFrame({\n",
    "        \"term\": params.index,\n",
    "        \"coef\": params.values,\n",
    "        \"se\": se.values,\n",
    "        \"z_or_t\": (params / se).values,\n",
    "        \"p_value\": model.pvalues.values,\n",
    "    })\n",
    "    out.insert(0, \"model\", model_name)\n",
    "    return out\n",
    "\n",
    "\n",
    "def _coef_table_or(model, model_name: str) -> pd.DataFrame:\n",
    "    params = model.params.copy()\n",
    "    se = model.bse.copy()\n",
    "    out = pd.DataFrame({\n",
    "        \"term\": params.index,\n",
    "        \"log_odds\": params.values,\n",
    "        \"se\": se.values,\n",
    "        \"odds_ratio\": np.exp(params.values),\n",
    "        \"or_ci95_lo\": np.exp(params.values - 1.96 * se.values),\n",
    "        \"or_ci95_hi\": np.exp(params.values + 1.96 * se.values),\n",
    "        \"p_value\": model.pvalues.values,\n",
    "    })\n",
    "    out.insert(0, \"model\", model_name)\n",
    "    return out\n",
    "\n",
    "\n",
    "def _cov_fit_kwargs(df: pd.DataFrame, prefer: str, fallback: str) -> dict:\n",
    "    # kwargs for .fit(...)\n",
    "    if prefer in df.columns and df[prefer].notna().any():\n",
    "        return {\"cov_type\": \"cluster\", \"cov_kwds\": {\"groups\": df[prefer]}}\n",
    "    if fallback in df.columns and df[fallback].notna().any():\n",
    "        return {\"cov_type\": \"cluster\", \"cov_kwds\": {\"groups\": df[fallback]}}\n",
    "    return {\"cov_type\": \"HC1\"}\n",
    "\n",
    "\n",
    "def _fit_logit_safe(formulas: list[str], data: pd.DataFrame, model_name: str, prefer_cluster: str = \"county_norm\", fallback_cluster: str = \"operator\"):\n",
    "    last_err = None\n",
    "    cov_kwargs = _cov_fit_kwargs(data, prefer=prefer_cluster, fallback=fallback_cluster)\n",
    "    for f in formulas:\n",
    "        try:\n",
    "            # lbfgs avoids the singular-Hessian inversion path more often than newton\n",
    "            res = smf.logit(f, data=data).fit(disp=0, method=\"lbfgs\", maxiter=200, **cov_kwargs)\n",
    "            return res, f, cov_kwargs\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            continue\n",
    "    raise RuntimeError(f\"Failed to fit {model_name}; last error: {last_err}\")\n",
    "\n",
    "\n",
    "def _fit_ols_safe(formula: str, data: pd.DataFrame, prefer_cluster: str = \"county_norm\", fallback_cluster: str = \"operator\"):\n",
    "    cov_kwargs = _cov_fit_kwargs(data, prefer=prefer_cluster, fallback=fallback_cluster)\n",
    "    res = smf.ols(formula, data=data).fit(**cov_kwargs)\n",
    "    return res, cov_kwargs\n",
    "\n",
    "\n",
    "def _marginal_prediction_ci(model, df: pd.DataFrame, overrides: dict, is_logit: bool, draws: int = 300) -> pd.Series:\n",
    "    # Marginal standardization: override key vars, keep all other covariates as observed.\n",
    "    d = df.copy()\n",
    "    for k, v in overrides.items():\n",
    "        d[k] = v\n",
    "\n",
    "    pred = model.predict(d)\n",
    "    mean_pred = float(np.asarray(pred).mean())\n",
    "\n",
    "    # Parametric CI via coefficient simulation\n",
    "    try:\n",
    "        params = np.asarray(model.params)\n",
    "        cov = np.asarray(model.cov_params())\n",
    "        sims = np.random.multivariate_normal(mean=params, cov=cov, size=draws)\n",
    "\n",
    "        if hasattr(model.model, \"data\") and hasattr(model.model.data, \"design_info\"):\n",
    "            import patsy\n",
    "\n",
    "            design_info = model.model.data.design_info\n",
    "            exog_new = patsy.build_design_matrices([design_info], d, return_type=\"dataframe\")[0]\n",
    "            exog_new = np.asarray(exog_new)\n",
    "\n",
    "            lin = exog_new @ sims.T  # (n, draws)\n",
    "            if is_logit:\n",
    "                sim_pred = 1.0 / (1.0 + np.exp(-lin))\n",
    "            else:\n",
    "                sim_pred = lin\n",
    "            sim_means = sim_pred.mean(axis=0)\n",
    "\n",
    "            lo, hi = np.quantile(sim_means, [0.025, 0.975])\n",
    "            return pd.Series({\"mean\": mean_pred, \"ci95_lo\": float(lo), \"ci95_hi\": float(hi)})\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return pd.Series({\"mean\": mean_pred, \"ci95_lo\": float(\"nan\"), \"ci95_hi\": float(\"nan\")})\n",
    "\n",
    "\n",
    "if \"spills_gdf\" not in globals():\n",
    "    raise NameError(\"spills_gdf is not defined. Run earlier cells first.\")\n",
    "\n",
    "raw = spills_gdf.copy()\n",
    "raw = _ensure_period(raw)\n",
    "raw = _ensure_delay_days(raw)\n",
    "\n",
    "spill_type_col = _pick_col(raw, [\"spill_type\", \"Spill Type\", \"spilltype\"])\n",
    "operator_col = _pick_col(raw, [\"operator\", \"Operator Name\", \"Operator\", \"operator_name\", \"operator_id\"])\n",
    "county_col = _pick_col(raw, [\"county_norm\", \"county\", \"County\"])\n",
    "rurality_col = _pick_col(raw, [\"rurality_3\", \"rurality\", \"Rurality\", \"RUCA\", \"ruca\"])\n",
    "\n",
    "model_df = pd.DataFrame(index=raw.index)\n",
    "model_df[\"delay_days\"] = raw[\"delay_days\"].copy()\n",
    "model_df[\"period\"] = raw[\"period\"].astype(\"string\")\n",
    "model_df[\"post\"] = (model_df[\"period\"] == \"post_2020\").astype(int)\n",
    "\n",
    "if spill_type_col is None:\n",
    "    raise KeyError(\"Need spill type column (Historical/Recent).\")\n",
    "model_df[\"spill_type\"] = raw[spill_type_col].astype(\"string\")\n",
    "model_df[\"historical\"] = (model_df[\"spill_type\"] == \"Historical\").astype(int)\n",
    "\n",
    "model_df[\"operator\"] = raw[operator_col].astype(\"string\") if operator_col else pd.Series(pd.NA, index=model_df.index, dtype=\"string\")\n",
    "model_df[\"county_norm\"] = raw[county_col].astype(\"string\") if county_col else pd.Series(pd.NA, index=model_df.index, dtype=\"string\")\n",
    "\n",
    "model_df[\"rurality_3\"] = pd.Series(pd.NA, index=model_df.index, dtype=\"string\")\n",
    "if rurality_col:\n",
    "    r = raw[rurality_col]\n",
    "    r_num = pd.to_numeric(r, errors=\"coerce\")\n",
    "    if r_num.notna().mean() > 0.5 and r_num.dropna().between(1, 10).mean() > 0.9:\n",
    "        model_df[\"rurality_3\"] = pd.cut(r_num, bins=[0, 3, 6, 10], labels=[\"urban\", \"suburban\", \"rural\"], include_lowest=True).astype(\"string\")\n",
    "    else:\n",
    "        model_df[\"rurality_3\"] = r.astype(\"string\")\n",
    "\n",
    "model_df[\"log1p_delay\"] = np.log1p(model_df[\"delay_days\"])\n",
    "model_df[\"any_delay\"] = (model_df[\"delay_days\"] > 0).astype(int)\n",
    "model_df[\"late30\"] = (model_df[\"delay_days\"] >= 30).astype(int)\n",
    "model_df[\"late90\"] = (model_df[\"delay_days\"] >= 90).astype(int)\n",
    "model_df[\"extreme99\"] = (model_df[\"delay_days\"] >= 99).astype(int)\n",
    "\n",
    "model_df[\"delay_cap180\"] = np.minimum(model_df[\"delay_days\"], 180)\n",
    "model_df[\"log1p_delay_cap180\"] = np.log1p(model_df[\"delay_cap180\"])\n",
    "\n",
    "report_date_col = _pick_col(raw, [\"Initial Report Date\", \"report_date\", \"Report Date\", \"initial_report_date\"])\n",
    "model_df[\"report_date\"] = pd.to_datetime(raw[report_date_col], errors=\"coerce\") if report_date_col else pd.NaT\n",
    "if report_date_col:\n",
    "    model_df[\"report_month\"] = model_df[\"report_date\"].dt.to_period(\"M\").astype(\"string\")\n",
    "\n",
    "before_n = int(model_df.shape[0])\n",
    "model_df = model_df.loc[model_df[\"delay_days\"].notna()].copy()\n",
    "after_n = int(model_df.shape[0])\n",
    "\n",
    "operator_nunique = int(model_df[\"operator\"].nunique(dropna=True)) if operator_col else 0\n",
    "include_operator_fe_ols = bool(operator_col) and operator_nunique <= 250\n",
    "county_fe = model_df[\"county_norm\"].notna().any()\n",
    "\n",
    "cov_kwargs_main = _cov_fit_kwargs(model_df, prefer=\"county_norm\", fallback=\"operator\")\n",
    "\n",
    "OUTPUT_DIR = (Path.cwd().resolve().parent / \"analysis_postgis\" / \"step2_model_outputs\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "REPORT_PATH = Path.cwd().resolve().parent / \"step2_models_summary.md\"\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 1) H1: Historical vs Recent mix\n",
    "# -----------------------------\n",
    "\n",
    "def _mix_table(df: pd.DataFrame, group_cols: list[str]) -> pd.DataFrame:\n",
    "    g = df.groupby(group_cols, dropna=False)\n",
    "    out = g.agg(\n",
    "        n_total=(\"historical\", \"size\"),\n",
    "        pct_historical=(\"historical\", lambda s: float(s.mean() * 100.0)),\n",
    "        pct_recent=(\"historical\", lambda s: float((1 - s.mean()) * 100.0)),\n",
    "    ).reset_index()\n",
    "    out[\"pct_historical\"] = out[\"pct_historical\"].round(3)\n",
    "    out[\"pct_recent\"] = out[\"pct_recent\"].round(3)\n",
    "    return out\n",
    "\n",
    "mix_overall = _mix_table(model_df, [\"period\"]).sort_values(\"period\")\n",
    "\n",
    "mix_by_county = pd.DataFrame()\n",
    "if county_fe:\n",
    "    mix_by_county = _mix_table(model_df, [\"county_norm\", \"period\"]).sort_values([\"county_norm\", \"period\"])\n",
    "\n",
    "mix_by_operator_top25 = pd.DataFrame()\n",
    "if operator_col:\n",
    "    top_ops = model_df[\"operator\"].value_counts().head(25).index\n",
    "    mix_by_operator_top25 = _mix_table(model_df[model_df[\"operator\"].isin(top_ops)], [\"operator\", \"period\"]).sort_values([\"operator\", \"period\"])\n",
    "\n",
    "plot_pct_hist_path_out = None\n",
    "pct_hist_over_time = pd.DataFrame()\n",
    "if report_date_col and model_df[\"report_month\"].notna().any():\n",
    "    tmp = model_df.dropna(subset=[\"report_month\"]).copy()\n",
    "    pct_hist_over_time = tmp.groupby(\"report_month\")[\"historical\"].mean().rename(\"pct_historical\").reset_index()\n",
    "    pct_hist_over_time[\"pct_historical\"] = (pct_hist_over_time[\"pct_historical\"] * 100.0).round(3)\n",
    "    pct_hist_over_time = pct_hist_over_time.sort_values(\"report_month\")\n",
    "\n",
    "    plot_pct_hist_path = OUTPUT_DIR / \"pct_historical_over_time.png\"\n",
    "    try:\n",
    "        fig, ax = plt.subplots(figsize=(10, 4))\n",
    "        ax.plot(pct_hist_over_time[\"report_month\"], pct_hist_over_time[\"pct_historical\"], marker=\"o\", linewidth=1)\n",
    "        ax.axvline(\"2020-01\", color=\"black\", linewidth=1)\n",
    "        ax.set_title(\"Pct Historical over time (monthly)\")\n",
    "        ax.set_xlabel(\"report_month\")\n",
    "        ax.set_ylabel(\"pct_historical\")\n",
    "        ax.tick_params(axis=\"x\", rotation=60)\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(plot_pct_hist_path, dpi=150)\n",
    "        plt.close(fig)\n",
    "        plot_pct_hist_path_out = plot_pct_hist_path\n",
    "    except Exception:\n",
    "        plot_pct_hist_path_out = None\n",
    "\n",
    "formulas_hist = []\n",
    "base = \"historical ~ post\"\n",
    "if county_fe and model_df[\"rurality_3\"].notna().any():\n",
    "    formulas_hist.append(base + \" + C(county_norm) + C(rurality_3)\")\n",
    "if county_fe:\n",
    "    formulas_hist.append(base + \" + C(county_norm)\")\n",
    "if model_df[\"rurality_3\"].notna().any():\n",
    "    formulas_hist.append(base + \" + C(rurality_3)\")\n",
    "formulas_hist.append(base)\n",
    "\n",
    "m_hist_logit, formula_hist_used, cov_kwargs_hist = _fit_logit_safe(formulas_hist, model_df, model_name=\"H1_hist_logit\")\n",
    "hist_logit_or = _coef_table_or(m_hist_logit, \"H1_hist_logit\")\n",
    "\n",
    "its_model = None\n",
    "its_coef = pd.DataFrame()\n",
    "if not pct_hist_over_time.empty:\n",
    "    pct_hist_over_time[\"t\"] = np.arange(len(pct_hist_over_time))\n",
    "    pct_hist_over_time[\"post\"] = (pct_hist_over_time[\"report_month\"] >= \"2020-01\").astype(int)\n",
    "    pct_hist_over_time[\"post_t\"] = pct_hist_over_time[\"post\"] * pct_hist_over_time[\"t\"]\n",
    "\n",
    "    if pct_hist_over_time[\"t\"].max() >= 24:\n",
    "        pct_hist_over_time[\"month_of_year\"] = pd.PeriodIndex(pct_hist_over_time[\"report_month\"], freq=\"M\").month\n",
    "        its_formula = \"pct_historical ~ t + post + post_t + C(month_of_year)\"\n",
    "        its_model = smf.ols(its_formula, data=pct_hist_over_time).fit(cov_type=\"HC1\")\n",
    "        its_coef = _coef_table(its_model, \"H1_its_monthly_pct_hist\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) H2: reporting delay\n",
    "# -----------------------------\n",
    "\n",
    "formula_delay_ols = \"log1p_delay ~ post + historical + post:historical\"\n",
    "if model_df[\"rurality_3\"].notna().any():\n",
    "    formula_delay_ols += \" + C(rurality_3)\"\n",
    "if county_fe:\n",
    "    formula_delay_ols += \" + C(county_norm)\"\n",
    "if include_operator_fe_ols:\n",
    "    formula_delay_ols += \" + C(operator)\"\n",
    "\n",
    "m_delay_ols, cov_kwargs_delay = _fit_ols_safe(formula_delay_ols, model_df)\n",
    "delay_ols_coef = _coef_table(m_delay_ols, \"H2_ols_log1p\")\n",
    "\n",
    "formulas_any_delay = [\"any_delay ~ post + historical + post:historical\"]\n",
    "if model_df[\"rurality_3\"].notna().any():\n",
    "    formulas_any_delay = [formulas_any_delay[0] + \" + C(rurality_3)\"] + formulas_any_delay\n",
    "if county_fe:\n",
    "    formulas_any_delay = [f + \" + C(county_norm)\" for f in formulas_any_delay] + formulas_any_delay\n",
    "\n",
    "m_any_delay, formula_any_used, cov_kwargs_any = _fit_logit_safe(formulas_any_delay, model_df, model_name=\"H2_logit_any_delay\")\n",
    "any_delay_or = _coef_table_or(m_any_delay, \"H2_logit_any_delay\")\n",
    "\n",
    "model_df_pos = model_df[model_df[\"delay_days\"] > 0].copy()\n",
    "m_delay_pos, cov_kwargs_delay_pos = _fit_ols_safe(formula_delay_ols, model_df_pos)\n",
    "delay_pos_coef = _coef_table(m_delay_pos, \"H2_ols_log1p_cond_delay_gt0\")\n",
    "\n",
    "\n",
    "def _build_thresh_formulas(dv: str) -> list[str]:\n",
    "    f0 = f\"{dv} ~ post + historical + post:historical\"\n",
    "    out = [f0]\n",
    "    if model_df[\"rurality_3\"].notna().any():\n",
    "        out = [f0 + \" + C(rurality_3)\"] + out\n",
    "    if county_fe:\n",
    "        out = [f + \" + C(county_norm)\" for f in out] + out\n",
    "    return out\n",
    "\n",
    "m_late30, formula_late30_used, cov_kwargs_late30 = _fit_logit_safe(_build_thresh_formulas(\"late30\"), model_df, model_name=\"H2_logit_late30\")\n",
    "m_late90, formula_late90_used, cov_kwargs_late90 = _fit_logit_safe(_build_thresh_formulas(\"late90\"), model_df, model_name=\"H2_logit_late90\")\n",
    "m_extreme99, formula_extreme99_used, cov_kwargs_ext = _fit_logit_safe(_build_thresh_formulas(\"extreme99\"), model_df, model_name=\"H2_logit_extreme99\")\n",
    "\n",
    "late30_or = _coef_table_or(m_late30, \"H2_logit_late30\")\n",
    "late90_or = _coef_table_or(m_late90, \"H2_logit_late90\")\n",
    "extreme99_or = _coef_table_or(m_extreme99, \"H2_logit_extreme99\")\n",
    "\n",
    "pred_grid = []\n",
    "for post_val, period in [(0, \"pre_2020\"), (1, \"post_2020\")]:\n",
    "    for hist_val, label in [(0, \"Recent\"), (1, \"Historical\")]:\n",
    "        s = _marginal_prediction_ci(m_late30, model_df, {\"post\": post_val, \"historical\": hist_val}, is_logit=True)\n",
    "        pred_grid.append({\"period\": period, \"spill_type\": label, **s.to_dict()})\n",
    "\n",
    "pred_late30 = pd.DataFrame(pred_grid)\n",
    "\n",
    "plot_pred_late30_path_out = None\n",
    "plot_pred_late30_path = OUTPUT_DIR / \"pred_late30_pre_post_by_type.png\"\n",
    "try:\n",
    "    fig, ax = plt.subplots(figsize=(7, 4))\n",
    "    order = [\"Recent\", \"Historical\"]\n",
    "    x = np.arange(len(order))\n",
    "\n",
    "    pre_vals = pred_late30[pred_late30[\"period\"] == \"pre_2020\"].set_index(\"spill_type\").loc[order]\n",
    "    post_vals = pred_late30[pred_late30[\"period\"] == \"post_2020\"].set_index(\"spill_type\").loc[order]\n",
    "\n",
    "    ax.errorbar(x - 0.1, pre_vals[\"mean\"], yerr=[pre_vals[\"mean\"] - pre_vals[\"ci95_lo\"], pre_vals[\"ci95_hi\"] - pre_vals[\"mean\"]], fmt=\"o\", capsize=3, label=\"pre_2020\")\n",
    "    ax.errorbar(x + 0.1, post_vals[\"mean\"], yerr=[post_vals[\"mean\"] - post_vals[\"ci95_lo\"], post_vals[\"ci95_hi\"] - post_vals[\"mean\"]], fmt=\"o\", capsize=3, label=\"post_2020\")\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(order)\n",
    "    ax.set_ylabel(\"Predicted P(late30)\")\n",
    "    ax.set_title(\"Predicted P(delay_days â‰¥ 30) by spill type and period\")\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(plot_pred_late30_path, dpi=150)\n",
    "    plt.close(fig)\n",
    "    plot_pred_late30_path_out = plot_pred_late30_path\n",
    "except Exception:\n",
    "    plot_pred_late30_path_out = None\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) H3: rural heterogeneity (if available)\n",
    "# -----------------------------\n",
    "\n",
    "late30_rural_or = pd.DataFrame()\n",
    "plot_pred_late30_rural_path_out = None\n",
    "pred_late30_rural = pd.DataFrame()\n",
    "\n",
    "if model_df[\"rurality_3\"].notna().any():\n",
    "    formula_late30_rural_full = \"late30 ~ post + historical + post:historical + C(rurality_3) + post:C(rurality_3)\"\n",
    "    formulas_rural = [formula_late30_rural_full]\n",
    "    if county_fe:\n",
    "        formulas_rural.insert(0, formula_late30_rural_full + \" + C(county_norm)\")\n",
    "\n",
    "    m_late30_rural, formula_late30_rural_used, cov_kwargs_rural = _fit_logit_safe(formulas_rural, model_df, model_name=\"H3_logit_late30_post_x_rurality\")\n",
    "    late30_rural_or = _coef_table_or(m_late30_rural, \"H3_logit_late30_post_x_rurality\")\n",
    "\n",
    "    rural_levels = sorted(model_df[\"rurality_3\"].dropna().unique().tolist())\n",
    "    grid = []\n",
    "    for r in rural_levels:\n",
    "        for post_val, period in [(0, \"pre_2020\"), (1, \"post_2020\")]:\n",
    "            s = _marginal_prediction_ci(m_late30_rural, model_df, {\"post\": post_val, \"historical\": 0, \"rurality_3\": r}, is_logit=True)\n",
    "            grid.append({\"rurality_3\": r, \"period\": period, **s.to_dict()})\n",
    "    pred_late30_rural = pd.DataFrame(grid)\n",
    "\n",
    "    plot_pred_late30_rural_path = OUTPUT_DIR / \"pred_late30_pre_post_by_rurality_recent.png\"\n",
    "    try:\n",
    "        fig, ax = plt.subplots(figsize=(8, 4))\n",
    "        for period, dx in [(\"pre_2020\", -0.1), (\"post_2020\", 0.1)]:\n",
    "            d = pred_late30_rural[pred_late30_rural[\"period\"] == period].copy()\n",
    "            x = np.arange(len(rural_levels)) + dx\n",
    "            y = d.set_index(\"rurality_3\").loc[rural_levels][\"mean\"].values\n",
    "            lo = d.set_index(\"rurality_3\").loc[rural_levels][\"ci95_lo\"].values\n",
    "            hi = d.set_index(\"rurality_3\").loc[rural_levels][\"ci95_hi\"].values\n",
    "            ax.errorbar(x, y, yerr=[y - lo, hi - y], fmt=\"o\", capsize=3, label=period)\n",
    "\n",
    "        ax.set_xticks(np.arange(len(rural_levels)))\n",
    "        ax.set_xticklabels(rural_levels)\n",
    "        ax.set_ylabel(\"Predicted P(late30) (Recent spills)\")\n",
    "        ax.set_title(\"Predicted P(delay_days â‰¥ 30) by rurality and period (Recent)\")\n",
    "        ax.legend()\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(plot_pred_late30_rural_path, dpi=150)\n",
    "        plt.close(fig)\n",
    "        plot_pred_late30_rural_path_out = plot_pred_late30_rural_path\n",
    "    except Exception:\n",
    "        plot_pred_late30_rural_path_out = None\n",
    "\n",
    "model_df[\"p_late30_hat\"] = m_late30.predict(model_df)\n",
    "\n",
    "adjusted_operator_late30 = pd.DataFrame()\n",
    "if operator_col:\n",
    "    op = model_df.groupby(\"operator\", dropna=False).agg(n_total=(\"late30\", \"size\"), p_late30_adj=(\"p_late30_hat\", \"mean\")).reset_index()\n",
    "    op = op[op[\"n_total\"] >= 50].copy()\n",
    "    op[\"p_late30_adj\"] = (op[\"p_late30_adj\"] * 100.0).round(3)\n",
    "    adjusted_operator_late30 = op.sort_values(\"p_late30_adj\", ascending=False)\n",
    "\n",
    "adjusted_county_late30 = pd.DataFrame()\n",
    "if county_fe:\n",
    "    cty = model_df.groupby(\"county_norm\", dropna=False).agg(n_total=(\"late30\", \"size\"), p_late30_adj=(\"p_late30_hat\", \"mean\")).reset_index()\n",
    "    cty = cty[cty[\"n_total\"] >= 50].copy()\n",
    "    cty[\"p_late30_adj\"] = (cty[\"p_late30_adj\"] * 100.0).round(3)\n",
    "    adjusted_county_late30 = cty.sort_values(\"p_late30_adj\", ascending=False)\n",
    "\n",
    "op_top15 = adjusted_operator_late30.head(15)\n",
    "op_bottom15 = adjusted_operator_late30.tail(15).sort_values(\"p_late30_adj\", ascending=True)\n",
    "cty_top = adjusted_county_late30.head(15)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Robustness\n",
    "# -----------------------------\n",
    "\n",
    "m_delay_ols_cap180, _ = _fit_ols_safe(formula_delay_ols.replace(\"log1p_delay\", \"log1p_delay_cap180\"), model_df)\n",
    "\n",
    "removed_pct = float((model_df[\"extreme99\"] == 1).mean() * 100.0)\n",
    "model_df_no_extreme99 = model_df[model_df[\"extreme99\"] == 0].copy()\n",
    "\n",
    "m_delay_ols_no_extreme99, _ = _fit_ols_safe(formula_delay_ols, model_df_no_extreme99)\n",
    "\n",
    "robust_ols_compare = pd.concat([\n",
    "    _coef_table(m_delay_ols, \"robust_ols_baseline\"),\n",
    "    _coef_table(m_delay_ols_cap180, \"robust_ols_cap180\"),\n",
    "    _coef_table(m_delay_ols_no_extreme99, \"robust_ols_exclude_extreme99\"),\n",
    "], ignore_index=True)\n",
    "\n",
    "core_terms = [\"post\", \"historical\", \"post:historical\"]\n",
    "robust_ols_core = robust_ols_compare[robust_ols_compare[\"term\"].isin(core_terms)].copy()\n",
    "\n",
    "m_late30_no_extreme99, formula_late30_no_extreme_used, _ = _fit_logit_safe(\n",
    "    _build_thresh_formulas(\"late30\"),\n",
    "    model_df_no_extreme99,\n",
    "    model_name=\"robust_late30_exclude_extreme99\",\n",
    ")\n",
    "\n",
    "robust_late30_compare = pd.concat([\n",
    "    _coef_table_or(m_late30, \"robust_late30_baseline\"),\n",
    "    _coef_table_or(m_late30_no_extreme99, \"robust_late30_exclude_extreme99\"),\n",
    "], ignore_index=True)\n",
    "robust_late30_core = robust_late30_compare[robust_late30_compare[\"term\"].isin(core_terms)].copy()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Outputs\n",
    "# -----------------------------\n",
    "\n",
    "mix_overall.to_csv(OUTPUT_DIR / \"H1_mix_overall.csv\", index=False)\n",
    "if not mix_by_county.empty:\n",
    "    mix_by_county.to_csv(OUTPUT_DIR / \"H1_mix_by_county.csv\", index=False)\n",
    "if not mix_by_operator_top25.empty:\n",
    "    mix_by_operator_top25.to_csv(OUTPUT_DIR / \"H1_mix_by_operator_top25.csv\", index=False)\n",
    "\n",
    "hist_logit_or.to_csv(OUTPUT_DIR / \"H1_hist_logit_or.csv\", index=False)\n",
    "if not its_coef.empty:\n",
    "    its_coef.to_csv(OUTPUT_DIR / \"H1_its_monthly_coef.csv\", index=False)\n",
    "\n",
    "pd.concat([delay_ols_coef, delay_pos_coef], ignore_index=True).to_csv(OUTPUT_DIR / \"H2_ols_coef.csv\", index=False)\n",
    "any_delay_or.to_csv(OUTPUT_DIR / \"H2_any_delay_or.csv\", index=False)\n",
    "late30_or.to_csv(OUTPUT_DIR / \"H2_late30_or.csv\", index=False)\n",
    "late90_or.to_csv(OUTPUT_DIR / \"H2_late90_or.csv\", index=False)\n",
    "extreme99_or.to_csv(OUTPUT_DIR / \"H2_extreme99_or.csv\", index=False)\n",
    "pred_late30.to_csv(OUTPUT_DIR / \"H2_pred_late30_prepost_by_type.csv\", index=False)\n",
    "\n",
    "if not late30_rural_or.empty:\n",
    "    late30_rural_or.to_csv(OUTPUT_DIR / \"H3_late30_rural_or.csv\", index=False)\n",
    "if not pred_late30_rural.empty:\n",
    "    pred_late30_rural.to_csv(OUTPUT_DIR / \"H3_pred_late30_prepost_by_rurality_recent.csv\", index=False)\n",
    "\n",
    "if not adjusted_operator_late30.empty:\n",
    "    adjusted_operator_late30.to_csv(OUTPUT_DIR / \"H3_operator_adjusted_late30.csv\", index=False)\n",
    "if not adjusted_county_late30.empty:\n",
    "    adjusted_county_late30.to_csv(OUTPUT_DIR / \"H3_county_adjusted_late30.csv\", index=False)\n",
    "\n",
    "robust_ols_core.to_csv(OUTPUT_DIR / \"robust_ols_core_terms.csv\", index=False)\n",
    "robust_late30_core.to_csv(OUTPUT_DIR / \"robust_late30_core_terms.csv\", index=False)\n",
    "\n",
    "plot_delay_dist_path_out = None\n",
    "plot_delay_dist_path = OUTPUT_DIR / \"delay_distribution_log1p.png\"\n",
    "try:\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    ax.hist(model_df[\"log1p_delay\"].dropna().values, bins=50)\n",
    "    ax.set_title(\"Distribution of log(1 + delay_days)\")\n",
    "    ax.set_xlabel(\"log(1 + delay_days)\")\n",
    "    ax.set_ylabel(\"count\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(plot_delay_dist_path, dpi=150)\n",
    "    plt.close(fig)\n",
    "    plot_delay_dist_path_out = plot_delay_dist_path\n",
    "except Exception:\n",
    "    plot_delay_dist_path_out = None\n",
    "\n",
    "lines = []\n",
    "lines.append(\"# Step 2 modeling + robustness summary (Colorado spills)\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"This report evaluates three hypothesis families: (1) Historical vs Recent mix shifts after 2020, (2) reporting timeliness changes (especially for Recent), (3) rural lag and heterogeneous improvements.\")\n",
    "lines.append(\"Primary models keep the full sample; sensitivity includes capping and an explicit exclusion check.\")\n",
    "lines.append(\"\")\n",
    "\n",
    "lines.append(\"## Setup\")\n",
    "lines.append(f\"- Rows before delay non-missing filter: {before_n:,}\")\n",
    "lines.append(f\"- Rows used in models (delay_days non-missing): {after_n:,}\")\n",
    "lines.append(f\"- Operator FE included in OLS: {include_operator_fe_ols} (unique operators={operator_nunique:,})\")\n",
    "lines.append(f\"- Covariance (main): {cov_kwargs_main.get('cov_type')}\")\n",
    "lines.append(\"\")\n",
    "\n",
    "lines.append(\"## H1) Agency outputs: Historical vs Recent mix\")\n",
    "lines.append(\"### Descriptives\")\n",
    "lines.append(_safe_to_markdown(mix_overall, index=False))\n",
    "lines.append(\"\")\n",
    "if not mix_by_county.empty:\n",
    "    lines.append(\"Top counties (pre/post rows):\")\n",
    "    lines.append(_safe_to_markdown(mix_by_county.head(30), index=False))\n",
    "    lines.append(\"\")\n",
    "if not mix_by_operator_top25.empty:\n",
    "    lines.append(\"Top 25 operators (pre/post rows):\")\n",
    "    lines.append(_safe_to_markdown(mix_by_operator_top25.head(50), index=False))\n",
    "    lines.append(\"\")\n",
    "if plot_pct_hist_path_out:\n",
    "    lines.append(f\"Plot saved: `{plot_pct_hist_path_out}`\")\n",
    "lines.append(\"\")\n",
    "\n",
    "lines.append(\"### Model\")\n",
    "lines.append(f\"Spill-level logit formula used: `{formula_hist_used}`\")\n",
    "lines.append(_safe_to_markdown(hist_logit_or[hist_logit_or[\"term\"].isin([\"post\"])], index=False))\n",
    "lines.append(\"\")\n",
    "if its_model is not None:\n",
    "    lines.append(\"Interrupted time series (monthly pct_historical): pct_historical ~ t + post + post_t + month-of-year FE\")\n",
    "    lines.append(_safe_to_markdown(its_coef[its_coef[\"term\"].isin([\"post\", \"post_t\"])], index=False))\n",
    "    lines.append(\"\")\n",
    "\n",
    "lines.append(\"## H2) Timeliness: reporting delay\")\n",
    "lines.append(\"### Continuous DV: OLS on log1p_delay\")\n",
    "lines.append(_safe_to_markdown(delay_ols_coef[delay_ols_coef[\"term\"].isin([\"post\", \"historical\", \"post:historical\"])], index=False))\n",
    "lines.append(\"\")\n",
    "lines.append(\"### Two-part model\")\n",
    "lines.append(f\"Any delay (logit) formula used: `{formula_any_used}`\")\n",
    "lines.append(_safe_to_markdown(any_delay_or[any_delay_or[\"term\"].isin([\"post\", \"historical\", \"post:historical\"])], index=False))\n",
    "lines.append(\"\")\n",
    "lines.append(\"Delay length | delay>0 (OLS log1p_delay):\")\n",
    "lines.append(_safe_to_markdown(delay_pos_coef[delay_pos_coef[\"term\"].isin([\"post\", \"historical\", \"post:historical\"])], index=False))\n",
    "lines.append(\"\")\n",
    "\n",
    "lines.append(\"### Threshold models (odds ratios)\")\n",
    "lines.append(f\"late30 formula used: `{formula_late30_used}`\")\n",
    "lines.append(_safe_to_markdown(late30_or[late30_or[\"term\"].isin([\"post\", \"historical\", \"post:historical\"])], index=False))\n",
    "lines.append(\"\")\n",
    "lines.append(f\"late90 formula used: `{formula_late90_used}`\")\n",
    "lines.append(_safe_to_markdown(late90_or[late90_or[\"term\"].isin([\"post\", \"historical\", \"post:historical\"])], index=False))\n",
    "lines.append(\"\")\n",
    "lines.append(f\"extreme99 formula used: `{formula_extreme99_used}`\")\n",
    "lines.append(_safe_to_markdown(extreme99_or[extreme99_or[\"term\"].isin([\"post\", \"historical\", \"post:historical\"])], index=False))\n",
    "lines.append(\"\")\n",
    "\n",
    "lines.append(\"Predicted P(late30) by spill type Ã— period (marginal standardization):\")\n",
    "lines.append(_safe_to_markdown(pred_late30, index=False))\n",
    "if plot_pred_late30_path_out:\n",
    "    lines.append(f\"Plot saved: `{plot_pred_late30_path_out}`\")\n",
    "lines.append(\"\")\n",
    "\n",
    "lines.append(\"## H3) Rural lag + heterogeneity\")\n",
    "if not late30_rural_or.empty:\n",
    "    lines.append(\"late30 with postÃ—rurality interaction (core + interaction terms):\")\n",
    "    keep_terms = [\"post\", \"historical\", \"post:historical\"]\n",
    "    show = late30_rural_or[late30_rural_or[\"term\"].isin(keep_terms) | late30_rural_or[\"term\"].str.contains(r\"post:C\\(rurality_3\\)\")].copy()\n",
    "    lines.append(_safe_to_markdown(show, index=False))\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"Predicted P(late30) by rurality Ã— period for Recent (historical=0):\")\n",
    "    lines.append(_safe_to_markdown(pred_late30_rural, index=False))\n",
    "    if plot_pred_late30_rural_path_out:\n",
    "        lines.append(f\"Plot saved: `{plot_pred_late30_rural_path_out}`\")\n",
    "    lines.append(\"\")\n",
    "else:\n",
    "    lines.append(\"Rurality not available; rural heterogeneity models skipped.\")\n",
    "    lines.append(\"\")\n",
    "\n",
    "if not op_top15.empty:\n",
    "    lines.append(\"Top 15 operators by adjusted predicted P(late30) (min n>=50):\")\n",
    "    lines.append(_safe_to_markdown(op_top15, index=False))\n",
    "    lines.append(\"\")\n",
    "if not op_bottom15.empty:\n",
    "    lines.append(\"Bottom 15 operators by adjusted predicted P(late30) (min n>=50):\")\n",
    "    lines.append(_safe_to_markdown(op_bottom15, index=False))\n",
    "    lines.append(\"\")\n",
    "if not cty_top.empty:\n",
    "    lines.append(\"Top 15 counties by adjusted predicted P(late30) (min n>=50):\")\n",
    "    lines.append(_safe_to_markdown(cty_top, index=False))\n",
    "    lines.append(\"\")\n",
    "\n",
    "lines.append(\"## Robustness / sensitivity\")\n",
    "lines.append(\"### OLS (log1p_delay): baseline vs cap180 vs exclude extreme99\")\n",
    "lines.append(_safe_to_markdown(robust_ols_core, index=False))\n",
    "lines.append(\"\")\n",
    "lines.append(f\"Exclude-extreme99 variant removes {removed_pct:.3f}% of rows (extreme99==1).\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"### late30 (logit): baseline vs exclude extreme99\")\n",
    "lines.append(_safe_to_markdown(robust_late30_core, index=False))\n",
    "lines.append(\"\")\n",
    "\n",
    "lines.append(\"## Outputs\")\n",
    "lines.append(f\"- Report: `{REPORT_PATH}`\")\n",
    "lines.append(f\"- Output directory: `{OUTPUT_DIR}`\")\n",
    "\n",
    "REPORT_PATH.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Wrote report: {REPORT_PATH}\")\n",
    "print(f\"Wrote outputs under: {OUTPUT_DIR}\")\n",
    "\n",
    "mix_overall, hist_logit_or.head(10), delay_ols_coef[delay_ols_coef['term'].isin(['post','historical','post:historical'])], late30_or[late30_or['term'].isin(['post','historical','post:historical'])], pred_late30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f761bfdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c469436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote report: /home/dadams/Repos/colorado_redux/step2_final_models.md\n",
      "Wrote outputs under: /home/dadams/Repos/colorado_redux/analysis_postgis/step2_model_outputs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(      period  n_total  pct_historical\n",
       " 0  post_2020    15704          47.873\n",
       " 1   pre_2020    10672          35.382,\n",
       "       period  pred_pct_historical\n",
       " 0   pre_2020               36.826\n",
       " 1  post_2020               46.762,\n",
       "    spill_type     period  P_late30  P_late90\n",
       " 0      Recent   pre_2020  0.003650  0.000239\n",
       " 1      Recent  post_2020  0.007531  0.003581\n",
       " 2  Historical   pre_2020  0.025917  0.008854\n",
       " 3  Historical  post_2020  0.040456  0.021834,\n",
       "    spill_type     period  P_any_delay  E_delay_if_pos\n",
       " 0      Recent   pre_2020     0.745056           2.030\n",
       " 1      Recent  post_2020     0.572983           1.862\n",
       " 2  Historical   pre_2020     0.702362           2.376\n",
       " 3  Historical  post_2020     0.449043           2.904,\n",
       "    spill_type     p_pre    p_post  risk_ratio  risk_diff\n",
       " 0      Recent  0.095476  0.302575    3.169117   0.207099\n",
       " 1  Historical  0.374337  0.706926    1.888477   0.332589)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2B: Final defensible model set (H1â€“H3) + rare-events fix for extreme99\n",
    "# Keeps full sample; avoids separation in extreme tail models by using ridge logistic (if sklearn available).\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n",
    "def _pick_col(df: pd.DataFrame, candidates: list[str]) -> str | None:\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "\n",
    "def _as_numeric(s: pd.Series) -> pd.Series:\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "\n",
    "def _cov_kwargs(df: pd.DataFrame, cluster_by: str, min_clusters: int = 20) -> dict:\n",
    "    if cluster_by in df.columns and df[cluster_by].notna().any():\n",
    "        g = df[cluster_by]\n",
    "        n_clust = int(g.nunique(dropna=True))\n",
    "        if n_clust >= min_clusters:\n",
    "            return {\"cov_type\": \"cluster\", \"cov_kwds\": {\"groups\": g}}\n",
    "    return {\"cov_type\": \"HC1\"}\n",
    "\n",
    "\n",
    "def _fit_ols_safe(formula: str, data: pd.DataFrame, cluster_by: str):\n",
    "    # Fit OLS; prefer cluster-robust SE when it looks stable, otherwise fallback to HC1.\n",
    "    cov = _cov_kwargs(data, cluster_by=cluster_by)\n",
    "    res = smf.ols(formula, data=data).fit(**cov)\n",
    "    try:\n",
    "        diag = np.diag(res.cov_params())\n",
    "        if (not np.all(np.isfinite(diag))) or np.any(diag < 0):\n",
    "            res = smf.ols(formula, data=data).fit(cov_type=\"HC1\")\n",
    "    except Exception:\n",
    "        res = smf.ols(formula, data=data).fit(cov_type=\"HC1\")\n",
    "    return res\n",
    "\n",
    "\n",
    "def _safe_to_markdown(df: pd.DataFrame, index: bool = False) -> str:\n",
    "    try:\n",
    "        return df.to_markdown(index=index)\n",
    "    except Exception:\n",
    "        return \"```\\n\" + df.to_string(index=index) + \"\\n```\"\n",
    "\n",
    "\n",
    "def _coef_table_ols(res, model_name: str) -> pd.DataFrame:\n",
    "    out = pd.DataFrame({\n",
    "        \"term\": res.params.index,\n",
    "        \"coef\": res.params.values,\n",
    "        \"se\": res.bse.values,\n",
    "        \"t\": (res.params / res.bse).values,\n",
    "        \"p_value\": res.pvalues.values,\n",
    "    })\n",
    "    out.insert(0, \"model\", model_name)\n",
    "    return out\n",
    "\n",
    "\n",
    "def _coef_table_or(res, model_name: str) -> pd.DataFrame:\n",
    "    p = res.params\n",
    "    se = res.bse\n",
    "    out = pd.DataFrame({\n",
    "        \"term\": p.index,\n",
    "        \"log_odds\": p.values,\n",
    "        \"se\": se.values,\n",
    "        \"odds_ratio\": np.exp(p.values),\n",
    "        \"or_ci95_lo\": np.exp(p.values - 1.96 * se.values),\n",
    "        \"or_ci95_hi\": np.exp(p.values + 1.96 * se.values),\n",
    "        \"p_value\": res.pvalues.values,\n",
    "    })\n",
    "    out.insert(0, \"model\", model_name)\n",
    "    return out\n",
    "\n",
    "\n",
    "def _fit_logit_safe(formulas: list[str], data: pd.DataFrame, cov_kwargs: dict, model_name: str):\n",
    "    last_err = None\n",
    "    for f in formulas:\n",
    "        try:\n",
    "            model = smf.logit(f, data=data)\n",
    "            res = model.fit(disp=0, method=\"lbfgs\", maxiter=600, **cov_kwargs)\n",
    "            try:\n",
    "                converged = bool(getattr(res, \"mle_retvals\", {}).get(\"converged\", True))\n",
    "            except Exception:\n",
    "                converged = True\n",
    "            if not converged:\n",
    "                for meth in [\"newton\", \"bfgs\"]:\n",
    "                    try:\n",
    "                        res2 = model.fit(disp=0, method=meth, maxiter=1200, **cov_kwargs)\n",
    "                        try:\n",
    "                            conv2 = bool(getattr(res2, \"mle_retvals\", {}).get(\"converged\", True))\n",
    "                        except Exception:\n",
    "                            conv2 = True\n",
    "                        res = res2\n",
    "                        if conv2:\n",
    "                            break\n",
    "                    except Exception:\n",
    "                        continue\n",
    "            return res, f\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            continue\n",
    "    raise RuntimeError(f\"Failed to fit {model_name}; last error: {last_err}\")\n",
    "\n",
    "\n",
    "def _marginal_mean_pred(res, df: pd.DataFrame, overrides: dict) -> float:\n",
    "    d = df.copy()\n",
    "    for k, v in overrides.items():\n",
    "        d[k] = v\n",
    "    pred = res.predict(d)\n",
    "    return float(np.asarray(pred).mean())\n",
    "\n",
    "\n",
    "def _marginal_mean_exp_delay_from_log1p(res, df: pd.DataFrame, overrides: dict) -> float:\n",
    "    # Interpreted as model-implied median on original scale for each row then averaged: exp(lp)-1.\n",
    "    d = df.copy()\n",
    "    for k, v in overrides.items():\n",
    "        d[k] = v\n",
    "    lp = res.predict(d)\n",
    "    return float(np.asarray(np.exp(lp) - 1.0).mean())\n",
    "\n",
    "\n",
    "if \"spills_gdf\" not in globals():\n",
    "    raise NameError(\"spills_gdf is not defined. Run the earlier setup cells first.\")\n",
    "\n",
    "raw = spills_gdf.copy()\n",
    "\n",
    "# Standardize period\n",
    "if \"period\" not in raw.columns:\n",
    "    if \"Before Report Year 2020\" in raw.columns:\n",
    "        raw[\"period\"] = raw[\"Before Report Year 2020\"].astype(bool).map({True: \"pre_2020\", False: \"post_2020\"})\n",
    "    elif \"Report Year\" in raw.columns:\n",
    "        raw[\"period\"] = np.where(raw[\"Report Year\"].astype(\"Int64\") < 2020, \"pre_2020\", \"post_2020\")\n",
    "    else:\n",
    "        raise KeyError(\"Need 'period' or a way to construct it ('Before Report Year 2020' or 'Report Year').\")\n",
    "raw[\"period\"] = raw[\"period\"].astype(\"string\")\n",
    "\n",
    "# Delay\n",
    "delay_col = _pick_col(raw, [\"delay_days\", \"report_delay_days\", \"Delay Days\"])\n",
    "if delay_col is None:\n",
    "    raise KeyError(\"Need a delay column (delay_days or report_delay_days).\")\n",
    "raw[\"delay_days\"] = _as_numeric(raw[delay_col])\n",
    "\n",
    "# Core fields\n",
    "spill_type_col = _pick_col(raw, [\"spill_type\", \"Spill Type\", \"spilltype\"])\n",
    "operator_col = _pick_col(raw, [\"operator\", \"Operator Name\", \"Operator\", \"operator_name\", \"operator_id\"])\n",
    "county_col = _pick_col(raw, [\"county_norm\", \"county\", \"County\"])\n",
    "rurality_col = _pick_col(raw, [\"rurality_3\", \"rurality\", \"Rurality\", \"RUCA\", \"ruca\"])\n",
    "report_date_col = _pick_col(raw, [\"Initial Report Date\", \"report_date\", \"Report Date\", \"initial_report_date\"])\n",
    "\n",
    "if spill_type_col is None:\n",
    "    raise KeyError(\"Need spill type column (Historical/Recent).\")\n",
    "\n",
    "df = pd.DataFrame(index=raw.index)\n",
    "df[\"delay_days\"] = raw[\"delay_days\"]\n",
    "df[\"period\"] = raw[\"period\"]\n",
    "df[\"post\"] = (df[\"period\"] == \"post_2020\").astype(int)\n",
    "df[\"spill_type\"] = raw[spill_type_col].astype(\"string\")\n",
    "df[\"historical\"] = (df[\"spill_type\"] == \"Historical\").astype(int)\n",
    "df[\"operator\"] = raw[operator_col].astype(\"string\") if operator_col else pd.Series(pd.NA, index=df.index, dtype=\"string\")\n",
    "df[\"county_norm\"] = raw[county_col].astype(\"string\") if county_col else pd.Series(pd.NA, index=df.index, dtype=\"string\")\n",
    "\n",
    "# rurality_3 (standardize)\n",
    "df[\"rurality_3\"] = pd.Series(pd.NA, index=df.index, dtype=\"string\")\n",
    "if rurality_col:\n",
    "    r = raw[rurality_col]\n",
    "    r_num = pd.to_numeric(r, errors=\"coerce\")\n",
    "    if r_num.notna().mean() > 0.5 and r_num.dropna().between(1, 10).mean() > 0.9:\n",
    "        df[\"rurality_3\"] = pd.cut(r_num, bins=[0, 3, 6, 10], labels=[\"rural\", \"suburban\", \"urban\"], include_lowest=True).astype(\"string\")\n",
    "    else:\n",
    "        df[\"rurality_3\"] = r.astype(\"string\")\n",
    "\n",
    "# time\n",
    "df[\"report_date\"] = pd.to_datetime(raw[report_date_col], errors=\"coerce\") if report_date_col else pd.NaT\n",
    "if report_date_col:\n",
    "    df[\"report_month\"] = df[\"report_date\"].dt.to_period(\"M\").astype(\"string\")\n",
    "    df[\"report_quarter\"] = df[\"report_date\"].dt.to_period(\"Q\").astype(\"string\")\n",
    "\n",
    "# outcomes\n",
    "df[\"log1p_delay\"] = np.log1p(df[\"delay_days\"])\n",
    "df[\"delay_cap180\"] = np.minimum(df[\"delay_days\"], 180)\n",
    "df[\"log1p_delay_cap180\"] = np.log1p(df[\"delay_cap180\"])\n",
    "df[\"any_delay\"] = (df[\"delay_days\"] > 0).astype(int)\n",
    "df[\"late30\"] = (df[\"delay_days\"] >= 30).astype(int)\n",
    "df[\"late90\"] = (df[\"delay_days\"] >= 90).astype(int)\n",
    "df[\"extreme99\"] = (df[\"delay_days\"] >= 99).astype(int)\n",
    "\n",
    "# keep full sample except missing delay\n",
    "df = df[df[\"delay_days\"].notna()].copy()\n",
    "\n",
    "# top counties\n",
    "top_counties = df[\"county_norm\"].value_counts().head(3).index.tolist() if df[\"county_norm\"].notna().any() else []\n",
    "df[\"county_top3\"] = np.where(df[\"county_norm\"].isin(top_counties), df[\"county_norm\"], \"OTHER\")\n",
    "df[\"county_top3\"] = pd.Series(df[\"county_top3\"]).astype(\"string\")\n",
    "\n",
    "OUTPUT_DIR = (Path.cwd().resolve().parent / \"analysis_postgis\" / \"step2_model_outputs\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "REPORT_PATH = Path.cwd().resolve().parent / \"step2_final_models.md\"\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 1) H1: Historical vs Recent mix\n",
    "# ================================\n",
    "h1_lines = []\n",
    "h1_lines.append(\"## H1: Historical vs Recent mix (agency outputs)\\n\")\n",
    "\n",
    "mix_overall = df.groupby(\"period\")[\"historical\"].agg([\"size\", \"mean\"]).reset_index()\n",
    "mix_overall.rename(columns={\"size\": \"n_total\", \"mean\": \"pct_historical\"}, inplace=True)\n",
    "mix_overall[\"pct_historical\"] = (mix_overall[\"pct_historical\"] * 100.0).round(3)\n",
    "h1_lines.append(\"### Descriptive mix\\n\")\n",
    "h1_lines.append(_safe_to_markdown(mix_overall, index=False))\n",
    "h1_lines.append(\"\")\n",
    "\n",
    "# A) Baseline logit (cluster by county as default; falls back to HC1 if missing)\n",
    "cov_h1 = _cov_kwargs(df, cluster_by=\"county_norm\")\n",
    "formulas_h1 = []\n",
    "f_base = \"historical ~ post\"\n",
    "if df[\"county_norm\"].notna().any() and df[\"rurality_3\"].notna().any():\n",
    "    formulas_h1.append(\"historical ~ post + C(county_norm) + C(rurality_3)\")\n",
    "if df[\"county_norm\"].notna().any():\n",
    "    formulas_h1.append(\"historical ~ post + C(county_norm)\")\n",
    "if df[\"rurality_3\"].notna().any():\n",
    "    formulas_h1.append(\"historical ~ post + C(rurality_3)\")\n",
    "formulas_h1.append(f_base)\n",
    "\n",
    "m_h1_logit, h1_logit_formula = _fit_logit_safe(formulas_h1, df, cov_h1, \"H1 baseline logit\")\n",
    "h1_logit_tab = _coef_table_or(m_h1_logit, \"H1_logit_baseline\")\n",
    "h1_lines.append(\"### Models\\n\")\n",
    "h1_lines.append(f\"Baseline spill-level logit formula used: `{h1_logit_formula}`\\n\")\n",
    "h1_lines.append(_safe_to_markdown(h1_logit_tab[h1_logit_tab[\"term\"].isin([\"post\"])], index=False))\n",
    "h1_lines.append(\"\")\n",
    "\n",
    "# marginal predicted pct_historical pre/post\n",
    "pred_h1 = []\n",
    "for post_val, period in [(0, \"pre_2020\"), (1, \"post_2020\")]:\n",
    "    ph = _marginal_mean_pred(m_h1_logit, df, {\"post\": post_val})\n",
    "    pred_h1.append({\"period\": period, \"pred_pct_historical\": round(ph * 100.0, 3)})\n",
    "pred_h1 = pd.DataFrame(pred_h1)\n",
    "h1_lines.append(\"Marginal predicted pct_historical (pre vs post):\\n\")\n",
    "h1_lines.append(_safe_to_markdown(pred_h1, index=False))\n",
    "h1_lines.append(\"\")\n",
    "\n",
    "# B) Operator structure: LPM with operator FE, clustered by operator (robustness)\n",
    "h1_lpm_tab = pd.DataFrame()\n",
    "h1_lpm_formula = None\n",
    "if operator_col and df[\"operator\"].notna().any():\n",
    "    # LPM is stable and directly supports high-D FE; interpret as robustness only.\n",
    "    h1_lpm_formula = \"historical ~ post\"\n",
    "    if df[\"county_norm\"].notna().any():\n",
    "        h1_lpm_formula += \" + C(county_norm)\"\n",
    "    if df[\"rurality_3\"].notna().any():\n",
    "        h1_lpm_formula += \" + C(rurality_3)\"\n",
    "    # operator FE\n",
    "    h1_lpm_formula += \" + C(operator)\"\n",
    "    m_h1_lpm = _fit_ols_safe(h1_lpm_formula, df, cluster_by=\"operator\")\n",
    "    h1_lpm_tab = _coef_table_ols(m_h1_lpm, \"H1_LPM_operator_FE\")\n",
    "    h1_lines.append(f\"Operator-structure robustness: LPM with operator FE, clustered by operator. Formula: `{h1_lpm_formula}`\\n\")\n",
    "    h1_lines.append(_safe_to_markdown(h1_lpm_tab[h1_lpm_tab[\"term\"].isin([\"post\"])], index=False))\n",
    "    h1_lines.append(\"\")\n",
    "\n",
    "# Plot monthly/quarterly pct_historical overall + top 3 counties\n",
    "plot_h1_month_path = None\n",
    "plot_h1_county_path = None\n",
    "if report_date_col and df[\"report_month\"].notna().any():\n",
    "    tmp = df.dropna(subset=[\"report_month\"]).copy()\n",
    "    overall_ts = tmp.groupby(\"report_month\")[\"historical\"].mean().reset_index()\n",
    "    overall_ts[\"pct_historical\"] = overall_ts[\"historical\"] * 100.0\n",
    "    overall_ts = overall_ts.sort_values(\"report_month\")\n",
    "    plot_h1_month_path = OUTPUT_DIR / \"h1_pct_historical_monthly.png\"\n",
    "    try:\n",
    "        fig, ax = plt.subplots(figsize=(10, 4))\n",
    "        ax.plot(overall_ts[\"report_month\"], overall_ts[\"pct_historical\"], marker=\"o\", linewidth=1)\n",
    "        ax.axvline(\"2020-01\", color=\"black\", linewidth=1)\n",
    "        ax.set_title(\"H1: Pct Historical over time (monthly)\")\n",
    "        ax.set_xlabel(\"report_month\")\n",
    "        ax.set_ylabel(\"pct_historical\")\n",
    "        ax.tick_params(axis=\"x\", rotation=60)\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(plot_h1_month_path, dpi=150)\n",
    "        plt.close(fig)\n",
    "    except Exception:\n",
    "        plot_h1_month_path = None\n",
    "\n",
    "    if top_counties:\n",
    "        county_ts = tmp[tmp[\"county_norm\"].isin(top_counties)].copy()\n",
    "        county_ts = county_ts.groupby([\"county_norm\", \"report_month\"])[\"historical\"].mean().reset_index()\n",
    "        county_ts[\"pct_historical\"] = county_ts[\"historical\"] * 100.0\n",
    "        plot_h1_county_path = OUTPUT_DIR / \"h1_pct_historical_monthly_top3counties.png\"\n",
    "        try:\n",
    "            fig, ax = plt.subplots(figsize=(10, 4))\n",
    "            for c in top_counties:\n",
    "                d = county_ts[county_ts[\"county_norm\"] == c].sort_values(\"report_month\")\n",
    "                ax.plot(d[\"report_month\"], d[\"pct_historical\"], marker=\"o\", linewidth=1, label=str(c))\n",
    "            ax.axvline(\"2020-01\", color=\"black\", linewidth=1)\n",
    "            ax.set_title(\"H1: Pct Historical over time (monthly) â€” top 3 counties\")\n",
    "            ax.set_xlabel(\"report_month\")\n",
    "            ax.set_ylabel(\"pct_historical\")\n",
    "            ax.tick_params(axis=\"x\", rotation=60)\n",
    "            ax.legend()\n",
    "            fig.tight_layout()\n",
    "            fig.savefig(plot_h1_county_path, dpi=150)\n",
    "            plt.close(fig)\n",
    "        except Exception:\n",
    "            plot_h1_county_path = None\n",
    "\n",
    "h1_lines.append(\"Plots saved:\\n\")\n",
    "if plot_h1_month_path:\n",
    "    h1_lines.append(f\"- `{plot_h1_month_path}`\")\n",
    "if plot_h1_county_path:\n",
    "    h1_lines.append(f\"- `{plot_h1_county_path}`\")\n",
    "h1_lines.append(\"\")\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 2) H2: Reporting timeliness\n",
    "# ================================\n",
    "h2_lines = []\n",
    "h2_lines.append(\"## H2: Reporting timeliness (regulated entities)\\n\")\n",
    "\n",
    "# A) Primary continuous DV with operator FE; cluster by operator (main) and county (robustness)\n",
    "h2_ols_formula = \"log1p_delay ~ post + historical + post:historical\"\n",
    "if df[\"county_norm\"].notna().any():\n",
    "    h2_ols_formula += \" + C(county_norm)\"\n",
    "if df[\"rurality_3\"].notna().any():\n",
    "    h2_ols_formula += \" + C(rurality_3)\"\n",
    "if operator_col and df[\"operator\"].notna().any():\n",
    "    h2_ols_formula += \" + C(operator)\"\n",
    "\n",
    "m_h2_ols_op = _fit_ols_safe(h2_ols_formula, df, cluster_by=\"operator\")\n",
    "m_h2_ols_cty = _fit_ols_safe(h2_ols_formula, df, cluster_by=\"county_norm\")\n",
    "h2_ols_tab_op = _coef_table_ols(m_h2_ols_op, \"H2_OLS_log1p_cluster_operator\")\n",
    "h2_ols_tab_cty = _coef_table_ols(m_h2_ols_cty, \"H2_OLS_log1p_cluster_county\")\n",
    "\n",
    "h2_lines.append(\"### A) Continuous DV (primary)\\n\")\n",
    "h2_lines.append(f\"OLS formula: `{h2_ols_formula}`\\n\")\n",
    "h2_lines.append(\"Cluster by operator (main):\\n\")\n",
    "h2_lines.append(_safe_to_markdown(h2_ols_tab_op[h2_ols_tab_op[\"term\"].isin([\"post\", \"historical\", \"post:historical\"])], index=False))\n",
    "h2_lines.append(\"\")\n",
    "h2_lines.append(\"Cluster by county (robustness):\\n\")\n",
    "h2_lines.append(_safe_to_markdown(h2_ols_tab_cty[h2_ols_tab_cty[\"term\"].isin([\"post\", \"historical\", \"post:historical\"])], index=False))\n",
    "h2_lines.append(\"\")\n",
    "\n",
    "# B) Two-part model\n",
    "h2_lines.append(\"### B) Two-part model\\n\")\n",
    "\n",
    "# 1) any_delay logit: county FE + rurality + (operator FE if feasible)\n",
    "form_any = \"any_delay ~ post + historical + post:historical\"\n",
    "if df[\"rurality_3\"].notna().any():\n",
    "    form_any += \" + C(rurality_3)\"\n",
    "if df[\"county_norm\"].notna().any():\n",
    "    form_any += \" + C(county_norm)\"\n",
    "\n",
    "# try operator FE only if it doesn't explode\n",
    "form_any_try = []\n",
    "if operator_col and df[\"operator\"].notna().any() and df[\"operator\"].nunique(dropna=True) <= 200:\n",
    "    form_any_try.append(form_any + \" + C(operator)\")\n",
    "form_any_try.append(form_any)\n",
    "\n",
    "m_any, form_any_used = _fit_logit_safe(form_any_try, df, _cov_kwargs(df, cluster_by=\"operator\"), \"H2 any_delay logit\")\n",
    "any_tab = _coef_table_or(m_any, \"H2_logit_any_delay\")\n",
    "h2_lines.append(f\"any_delay logit formula used: `{form_any_used}`\\n\")\n",
    "h2_lines.append(_safe_to_markdown(any_tab[any_tab[\"term\"].isin([\"post\", \"historical\", \"post:historical\"])], index=False))\n",
    "h2_lines.append(\"\")\n",
    "\n",
    "# 2) conditional positive delay: OLS on log1p_delay among delay>0 with same predictors; cluster operator\n",
    "df_pos = df[df[\"delay_days\"] > 0].copy()\n",
    "m_pos = _fit_ols_safe(h2_ols_formula, df_pos, cluster_by=\"operator\")\n",
    "pos_tab = _coef_table_ols(m_pos, \"H2_OLS_log1p_delay_gt0_cluster_operator\")\n",
    "h2_lines.append(\"Conditional positive delay model (delay>0), OLS on log1p_delay; clustered by operator:\\n\")\n",
    "h2_lines.append(_safe_to_markdown(pos_tab[pos_tab[\"term\"].isin([\"post\", \"historical\", \"post:historical\"])], index=False))\n",
    "h2_lines.append(\"\")\n",
    "\n",
    "# Mechanism figure: P(any_delay) and E[delay|delay>0] pre/post by spill type\n",
    "mech = []\n",
    "for hist_val, label in [(0, \"Recent\"), (1, \"Historical\")]:\n",
    "    for post_val, period in [(0, \"pre_2020\"), (1, \"post_2020\")]:\n",
    "        p_any = _marginal_mean_pred(m_any, df, {\"historical\": hist_val, \"post\": post_val})\n",
    "        e_pos = _marginal_mean_exp_delay_from_log1p(m_pos, df_pos, {\"historical\": hist_val, \"post\": post_val})\n",
    "        mech.append({\"spill_type\": label, \"period\": period, \"P_any_delay\": p_any, \"E_delay_if_pos\": e_pos})\n",
    "mech = pd.DataFrame(mech)\n",
    "mech[\"P_any_delay\"] = mech[\"P_any_delay\"].round(6)\n",
    "mech[\"E_delay_if_pos\"] = mech[\"E_delay_if_pos\"].round(3)\n",
    "h2_lines.append(\"Mechanism decomposition table (marginal standardized):\\n\")\n",
    "h2_lines.append(_safe_to_markdown(mech, index=False))\n",
    "h2_lines.append(\"\")\n",
    "\n",
    "plot_mech_path = OUTPUT_DIR / \"h2_mechanism_decomposition.png\"\n",
    "try:\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    x = np.arange(2)\n",
    "    width = 0.18\n",
    "    for i, label in enumerate([\"Recent\", \"Historical\"]):\n",
    "        pre = mech[(mech[\"spill_type\"] == label) & (mech[\"period\"] == \"pre_2020\")].iloc[0]\n",
    "        post = mech[(mech[\"spill_type\"] == label) & (mech[\"period\"] == \"post_2020\")].iloc[0]\n",
    "        ax.bar(i - width, pre[\"P_any_delay\"], width, label=f\"{label} pre: P(any)\")\n",
    "        ax.bar(i, post[\"P_any_delay\"], width, label=f\"{label} post: P(any)\")\n",
    "    ax.set_xticks([0, 1])\n",
    "    ax.set_xticklabels([\"Recent\", \"Historical\"])\n",
    "    ax.set_title(\"H2 mechanism: predicted P(any_delay) pre vs post\")\n",
    "    ax.set_ylabel(\"probability\")\n",
    "    ax.legend(fontsize=8)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(plot_mech_path, dpi=150)\n",
    "    plt.close(fig)\n",
    "except Exception:\n",
    "    plot_mech_path = None\n",
    "\n",
    "# C) Threshold models late30 and late90 with reduced county burden\n",
    "h2_lines.append(\"### C) Threshold models\\n\")\n",
    "\n",
    "def _threshold_formula(dv: str) -> str:\n",
    "    f = f\"{dv} ~ post + historical + post:historical\"\n",
    "    if df[\"rurality_3\"].notna().any():\n",
    "        f += \" + C(rurality_3)\"\n",
    "    # reduced county FE: only top3 counties + OTHER\n",
    "    if df[\"county_norm\"].notna().any() and len(top_counties) > 0:\n",
    "        f += \" + C(county_top3)\"\n",
    "    return f\n",
    "\n",
    "f_late30 = _threshold_formula(\"late30\")\n",
    "f_late90 = _threshold_formula(\"late90\")\n",
    "\n",
    "m_l30, f_l30_used = _fit_logit_safe([f_late30, \"late30 ~ post + historical + post:historical\"], df, _cov_kwargs(df, cluster_by=\"operator\"), \"late30 logit\")\n",
    "m_l90, f_l90_used = _fit_logit_safe([f_late90, \"late90 ~ post + historical + post:historical\"], df, _cov_kwargs(df, cluster_by=\"operator\"), \"late90 logit\")\n",
    "\n",
    "l30_tab = _coef_table_or(m_l30, \"H2_logit_late30\")\n",
    "l90_tab = _coef_table_or(m_l90, \"H2_logit_late90\")\n",
    "\n",
    "h2_lines.append(f\"late30 formula used: `{f_l30_used}`\\n\")\n",
    "h2_lines.append(_safe_to_markdown(l30_tab[l30_tab[\"term\"].isin([\"post\", \"historical\", \"post:historical\"])], index=False))\n",
    "h2_lines.append(\"\")\n",
    "h2_lines.append(f\"late90 formula used: `{f_l90_used}`\\n\")\n",
    "h2_lines.append(_safe_to_markdown(l90_tab[l90_tab[\"term\"].isin([\"post\", \"historical\", \"post:historical\"])], index=False))\n",
    "h2_lines.append(\"\")\n",
    "\n",
    "pred_thresh = []\n",
    "for label, hist_val in [(\"Recent\", 0), (\"Historical\", 1)]:\n",
    "    for period, post_val in [(\"pre_2020\", 0), (\"post_2020\", 1)]:\n",
    "        p30 = _marginal_mean_pred(m_l30, df, {\"historical\": hist_val, \"post\": post_val})\n",
    "        p90 = _marginal_mean_pred(m_l90, df, {\"historical\": hist_val, \"post\": post_val})\n",
    "        pred_thresh.append({\"spill_type\": label, \"period\": period, \"P_late30\": p30, \"P_late90\": p90})\n",
    "pred_thresh = pd.DataFrame(pred_thresh)\n",
    "pred_thresh[[\"P_late30\", \"P_late90\"]] = pred_thresh[[\"P_late30\", \"P_late90\"]].round(6)\n",
    "h2_lines.append(\"Predicted P(late30) and P(late90) pre vs post (marginal standardized):\\n\")\n",
    "h2_lines.append(_safe_to_markdown(pred_thresh, index=False))\n",
    "h2_lines.append(\"\")\n",
    "\n",
    "plot_thresh_path = OUTPUT_DIR / \"h2_pred_late30_late90_prepost_by_type.png\"\n",
    "try:\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    order = [\"Recent\", \"Historical\"]\n",
    "    x = np.arange(len(order))\n",
    "    pre = pred_thresh[pred_thresh[\"period\"] == \"pre_2020\"].set_index(\"spill_type\").loc[order]\n",
    "    post = pred_thresh[pred_thresh[\"period\"] == \"post_2020\"].set_index(\"spill_type\").loc[order]\n",
    "    ax.plot(x, pre[\"P_late30\"], marker=\"o\", label=\"pre late30\")\n",
    "    ax.plot(x, post[\"P_late30\"], marker=\"o\", label=\"post late30\")\n",
    "    ax.plot(x, pre[\"P_late90\"], marker=\"x\", label=\"pre late90\")\n",
    "    ax.plot(x, post[\"P_late90\"], marker=\"x\", label=\"post late90\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(order)\n",
    "    ax.set_ylabel(\"predicted probability\")\n",
    "    ax.set_title(\"H2: Predicted tail probabilities\")\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(plot_thresh_path, dpi=150)\n",
    "    plt.close(fig)\n",
    "except Exception:\n",
    "    plot_thresh_path = None\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 3) Extreme tail (>=99): ridge logistic or fallback\n",
    "# ============================================\n",
    "rare_lines = []\n",
    "rare_lines.append(\"## Extreme tail (>=99 days): rare-events strategy\\n\")\n",
    "\n",
    "ridge_available = True\n",
    "try:\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.linear_model import LogisticRegressionCV\n",
    "except Exception:\n",
    "    ridge_available = False\n",
    "\n",
    "ridge_summary = {}\n",
    "pred_extreme99 = pd.DataFrame()\n",
    "plot_extreme99_path = None\n",
    "\n",
    "if ridge_available:\n",
    "    # Ridge logistic with cross-validated penalty; keep FE out of the model by using a small county category (top3+OTHER)\n",
    "    # and operator as a random-effect proxy via one-hot only if not too sparse (we keep it out by default).\n",
    "    features = [\"post\", \"historical\"]\n",
    "    cat_features = []\n",
    "    if df[\"rurality_3\"].notna().any():\n",
    "        cat_features.append(\"rurality_3\")\n",
    "    if df[\"county_norm\"].notna().any() and len(top_counties) > 0:\n",
    "        cat_features.append(\"county_top3\")\n",
    "    # NOTE: operator is intentionally excluded to avoid ultra-sparse one-hot and leakage/separation.\n",
    "\n",
    "    X = df[features + cat_features].copy()\n",
    "    y = df[\"extreme99\"].astype(int).values\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", \"passthrough\", features),\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_features),\n",
    "        ]\n",
    "    )\n",
    "    clf = LogisticRegressionCV(\n",
    "        Cs=np.logspace(-3, 3, 12),\n",
    "        cv=5,\n",
    "        penalty=\"l2\",\n",
    "        solver=\"lbfgs\",\n",
    "        max_iter=5000,\n",
    "        scoring=\"neg_log_loss\",\n",
    "        class_weight=\"balanced\",\n",
    "        n_jobs=None,\n",
    "        refit=True,\n",
    "    )\n",
    "    pipe = Pipeline([(\"pre\", pre), (\"clf\", clf)])\n",
    "    pipe.fit(X, y)\n",
    "    ridge_summary = {\n",
    "        \"chosen_C\": float(pipe.named_steps[\"clf\"].C_[0]),\n",
    "        \"event_rate\": float(np.mean(y)),\n",
    "        \"n\": int(len(y)),\n",
    "    }\n",
    "    rare_lines.append(\"Used ridge (L2) logistic regression with CV on C, class_weight='balanced'.\\n\")\n",
    "    rare_lines.append(_safe_to_markdown(pd.DataFrame([ridge_summary]), index=False))\n",
    "    rare_lines.append(\"\")\n",
    "\n",
    "    def _ridge_marginal_pred(overrides: dict) -> float:\n",
    "        d = X.copy()\n",
    "        for k, v in overrides.items():\n",
    "            d[k] = v\n",
    "        p = pipe.predict_proba(d)[:, 1]\n",
    "        return float(np.mean(p))\n",
    "\n",
    "    grid = []\n",
    "    for label, hist_val in [(\"Recent\", 0), (\"Historical\", 1)]:\n",
    "        p_pre = _ridge_marginal_pred({\"historical\": hist_val, \"post\": 0})\n",
    "        p_post = _ridge_marginal_pred({\"historical\": hist_val, \"post\": 1})\n",
    "        rr = (p_post / p_pre) if p_pre > 0 else np.nan\n",
    "        rd = p_post - p_pre\n",
    "        grid.append({\"spill_type\": label, \"p_pre\": p_pre, \"p_post\": p_post, \"risk_ratio\": rr, \"risk_diff\": rd})\n",
    "    pred_extreme99 = pd.DataFrame(grid)\n",
    "    pred_extreme99[[\"p_pre\", \"p_post\", \"risk_ratio\", \"risk_diff\"]] = pred_extreme99[[\"p_pre\", \"p_post\", \"risk_ratio\", \"risk_diff\"]].round(6)\n",
    "    rare_lines.append(\"Stable predicted probabilities (marginal standardized) and pre/post risk ratios:\\n\")\n",
    "    rare_lines.append(_safe_to_markdown(pred_extreme99, index=False))\n",
    "    rare_lines.append(\"\")\n",
    "\n",
    "    plot_extreme99_path = OUTPUT_DIR / \"extreme99_ridge_pred_prepost_by_type.png\"\n",
    "    try:\n",
    "        fig, ax = plt.subplots(figsize=(7, 4))\n",
    "        x = np.arange(pred_extreme99.shape[0])\n",
    "        ax.bar(x - 0.15, pred_extreme99[\"p_pre\"], width=0.3, label=\"pre\")\n",
    "        ax.bar(x + 0.15, pred_extreme99[\"p_post\"], width=0.3, label=\"post\")\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(pred_extreme99[\"spill_type\"].tolist())\n",
    "        ax.set_ylabel(\"Predicted P(extreme99)\")\n",
    "        ax.set_title(\"Extreme tail (>=99d): ridge predicted probabilities\")\n",
    "        ax.legend()\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(plot_extreme99_path, dpi=150)\n",
    "        plt.close(fig)\n",
    "    except Exception:\n",
    "        plot_extreme99_path = None\n",
    "else:\n",
    "    rare_lines.append(\"Ridge logistic regression unavailable (sklearn not installed).\\n\")\n",
    "    rare_lines.append(\"Per spec, we avoid extreme99 inference; treat extreme99 descriptively and rely on late90 as inferential tail.\\n\")\n",
    "    desc = df.groupby([\"period\", \"spill_type\"])[\"extreme99\"].agg([\"size\", \"mean\"]).reset_index()\n",
    "    desc.rename(columns={\"size\": \"n\", \"mean\": \"pct_extreme99\"}, inplace=True)\n",
    "    desc[\"pct_extreme99\"] = (desc[\"pct_extreme99\"] * 100.0).round(3)\n",
    "    rare_lines.append(_safe_to_markdown(desc, index=False))\n",
    "    rare_lines.append(\"\")\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 4) H3: Rural lag + interactions\n",
    "# ================================\n",
    "h3_lines = []\n",
    "h3_lines.append(\"## H3: Rural lag and heterogeneous improvements\\n\")\n",
    "\n",
    "has_rural = df[\"rurality_3\"].notna().any()\n",
    "if not has_rural:\n",
    "    h3_lines.append(\"Rurality not available; H3 interaction tests skipped.\\n\")\n",
    "else:\n",
    "    # any_delay, log1p_delay, late90 with post*rurality\n",
    "    h3_any_formula = \"any_delay ~ post + historical + post:historical + C(rurality_3) + post:C(rurality_3)\"\n",
    "    if df[\"county_norm\"].notna().any():\n",
    "        h3_any_formula += \" + C(county_norm)\"\n",
    "    m_h3_any, h3_any_used = _fit_logit_safe([h3_any_formula, \"any_delay ~ post + historical + post:historical + C(rurality_3) + post:C(rurality_3)\"], df, _cov_kwargs(df, cluster_by=\"operator\"), \"H3 any_delay\")\n",
    "    any_h3_tab = _coef_table_or(m_h3_any, \"H3_logit_any_delay_post_x_rurality\")\n",
    "\n",
    "    h3_delay_formula = \"log1p_delay ~ post + historical + post:historical + C(rurality_3) + post:C(rurality_3)\"\n",
    "    if df[\"county_norm\"].notna().any():\n",
    "        h3_delay_formula += \" + C(county_norm)\"\n",
    "    if operator_col and df[\"operator\"].notna().any():\n",
    "        h3_delay_formula += \" + C(operator)\"\n",
    "    m_h3_delay = _fit_ols_safe(h3_delay_formula, df, cluster_by=\"operator\")\n",
    "    delay_h3_tab = _coef_table_ols(m_h3_delay, \"H3_OLS_log1p_post_x_rurality_cluster_operator\")\n",
    "\n",
    "    h3_l90_formula = \"late90 ~ post + historical + post:historical + C(rurality_3) + post:C(rurality_3)\"\n",
    "    # reduced county FE to avoid sparseness\n",
    "    if df[\"county_norm\"].notna().any() and len(top_counties) > 0:\n",
    "        h3_l90_formula += \" + C(county_top3)\"\n",
    "    m_h3_l90, h3_l90_used = _fit_logit_safe([h3_l90_formula, \"late90 ~ post + historical + post:historical + C(rurality_3) + post:C(rurality_3)\"], df, _cov_kwargs(df, cluster_by=\"operator\"), \"H3 late90\")\n",
    "    l90_h3_tab = _coef_table_or(m_h3_l90, \"H3_logit_late90_post_x_rurality\")\n",
    "\n",
    "    # Joint tests: jointly test postÃ—rurality terms with correct restriction matrix.\n",
    "    # Targets e.g. post:C(rurality_3)[T.Suburban] and post:C(rurality_3)[T.Urban] (and any others if coding differs).\n",
    "    def _post_x_rural_terms(param_names: list[str]) -> list[str]:\n",
    "        terms = []\n",
    "        for t in param_names:\n",
    "            if \"rurality_3\" not in t or \"post\" not in t:\n",
    "                continue\n",
    "            # patsy can name interactions as post:C(rurality_3)[T.x] or C(rurality_3)[T.x]:post\n",
    "            if (\"post:C(rurality_3)[T.\" in t) or (\"C(rurality_3)[T.\" in t and t.endswith(\":post\")):\n",
    "                terms.append(t)\n",
    "        # de-duplicate while preserving order\n",
    "        seen = set()\n",
    "        out = []\n",
    "        for t in terms:\n",
    "            if t not in seen:\n",
    "                out.append(t)\n",
    "                seen.add(t)\n",
    "        return out\n",
    "\n",
    "    def _restriction_matrix(param_names: list[str], terms: list[str]) -> np.ndarray:\n",
    "        name_to_ix = {n: i for i, n in enumerate(param_names)}\n",
    "        k = len(param_names)\n",
    "        q = len(terms)\n",
    "        R = np.zeros((q, k))\n",
    "        for r_i, term in enumerate(terms):\n",
    "            if term in name_to_ix:\n",
    "                R[r_i, name_to_ix[term]] = 1.0\n",
    "        return R\n",
    "\n",
    "    def _joint_post_x_rural_test(res, model_kind: str) -> str:\n",
    "        param_names = list(res.params.index)\n",
    "        terms = _post_x_rural_terms(param_names)\n",
    "        if not terms:\n",
    "            return \"Joint test postÃ—rurality: (no interaction terms found)\"\n",
    "        R = _restriction_matrix(param_names, terms)\n",
    "        if R.shape != (len(terms), len(param_names)):\n",
    "            return f\"Joint test postÃ—rurality failed: bad R shape {R.shape}, expected {(len(terms), len(param_names))}\"\n",
    "        try:\n",
    "            if model_kind == \"ols\":\n",
    "                ft = res.f_test(R)\n",
    "                stat = float(np.asarray(ft.fvalue).squeeze())\n",
    "                df_num = int(getattr(ft, \"df_num\", len(terms)))\n",
    "                df_denom = int(getattr(ft, \"df_denom\", np.nan))\n",
    "                p = float(np.asarray(ft.pvalue).squeeze())\n",
    "                return f\"Joint test postÃ—rurality (F): F={stat:.3f}, df=({df_num}, {df_denom}), p={p:.6g}; terms={terms}\"\n",
    "            else:\n",
    "                wt = res.wald_test(R)\n",
    "                stat = float(np.asarray(wt.statistic).squeeze())\n",
    "                df_num = int(getattr(wt, \"df_num\", len(terms)))\n",
    "                p = float(np.asarray(wt.pvalue).squeeze())\n",
    "                return f\"Joint test postÃ—rurality (Ï‡Â²): chi2={stat:.3f}, df={df_num}, p={p:.6g}; terms={terms}\"\n",
    "        except Exception as e:\n",
    "            return f\"Joint test postÃ—rurality failed: {e}; terms={terms}\"\n",
    "\n",
    "    h3_lines.append(\"### Interaction models\\n\")\n",
    "    h3_lines.append(f\"any_delay model formula used: `{h3_any_used}`\\n\")\n",
    "    h3_lines.append(_safe_to_markdown(any_h3_tab[any_h3_tab[\"term\"].str.contains(\"rurality_3\") & any_h3_tab[\"term\"].str.contains(\"post\") | any_h3_tab[\"term\"].isin([\"post\", \"post:historical\"])], index=False))\n",
    "    h3_lines.append(_joint_post_x_rural_test(m_h3_any, model_kind=\"logit\"))\n",
    "    h3_lines.append(\"\")\n",
    "\n",
    "    h3_lines.append(f\"log1p_delay model formula: `{h3_delay_formula}`\\n\")\n",
    "    h3_lines.append(_safe_to_markdown(delay_h3_tab[delay_h3_tab[\"term\"].str.contains(\"rurality_3\") & delay_h3_tab[\"term\"].str.contains(\"post\") | delay_h3_tab[\"term\"].isin([\"post\", \"post:historical\"])], index=False))\n",
    "    h3_lines.append(_joint_post_x_rural_test(m_h3_delay, model_kind=\"ols\"))\n",
    "    h3_lines.append(\"\")\n",
    "\n",
    "    h3_lines.append(f\"late90 model formula used: `{h3_l90_used}`\\n\")\n",
    "    h3_lines.append(_safe_to_markdown(l90_h3_tab[l90_h3_tab[\"term\"].str.contains(\"rurality_3\") & l90_h3_tab[\"term\"].str.contains(\"post\") | l90_h3_tab[\"term\"].isin([\"post\", \"post:historical\"])], index=False))\n",
    "    h3_lines.append(_joint_post_x_rural_test(m_h3_l90, model_kind=\"logit\"))\n",
    "    h3_lines.append(\"\")\n",
    "\n",
    "    # Rurality gradient table (Recent only)\n",
    "    recent = df[df[\"historical\"] == 0].copy()\n",
    "    levels = sorted(recent[\"rurality_3\"].dropna().unique().tolist())\n",
    "    grad = []\n",
    "    for r in levels:\n",
    "        sub = recent[recent[\"rurality_3\"] == r].copy()\n",
    "        if sub.empty:\n",
    "            continue\n",
    "        for period, post_val in [(\"pre_2020\", 0), (\"post_2020\", 1)]:\n",
    "            p_any = _marginal_mean_pred(m_h3_any, sub, {\"post\": post_val, \"historical\": 0})\n",
    "            p30 = _marginal_mean_pred(m_l30, sub, {\"post\": post_val, \"historical\": 0})\n",
    "            p90 = _marginal_mean_pred(m_l90, sub, {\"post\": post_val, \"historical\": 0})\n",
    "            med_delay = _marginal_mean_exp_delay_from_log1p(m_h3_delay, sub, {\"post\": post_val, \"historical\": 0})\n",
    "            grad.append({\"rurality_3\": r, \"period\": period, \"P_any_delay\": p_any, \"median_delay_implied\": med_delay, \"P_late30\": p30, \"P_late90\": p90})\n",
    "    grad = pd.DataFrame(grad)\n",
    "    if not grad.empty:\n",
    "        grad[[\"P_any_delay\", \"P_late30\", \"P_late90\"]] = grad[[\"P_any_delay\", \"P_late30\", \"P_late90\"]].round(6)\n",
    "        grad[\"median_delay_implied\"] = grad[\"median_delay_implied\"].round(3)\n",
    "        h3_lines.append(\"### Rurality gradient (Recent spills only; model-implied median delay = exp(lp)-1)\\n\")\n",
    "        h3_lines.append(_safe_to_markdown(grad, index=False))\n",
    "        h3_lines.append(\"\")\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# 5) Operator + geography deliverables\n",
    "# ====================================\n",
    "var_lines = []\n",
    "var_lines.append(\"## Operator + geography variation (Aim 3 deliverables)\\n\")\n",
    "\n",
    "# Operator adjusted predicted probabilities for late30 and any_delay (min n>=50)\n",
    "df[\"p_late30_hat\"] = m_l30.predict(df)\n",
    "df[\"p_any_hat\"] = m_any.predict(df)\n",
    "\n",
    "op_tables = []\n",
    "if operator_col and df[\"operator\"].notna().any():\n",
    "    op = df.groupby(\"operator\").agg(n_total=(\"late30\", \"size\"), p_late30_adj=(\"p_late30_hat\", \"mean\"), p_any_adj=(\"p_any_hat\", \"mean\")).reset_index()\n",
    "    op = op[op[\"n_total\"] >= 50].copy()\n",
    "    op[\"p_late30_adj\"] = (op[\"p_late30_adj\"] * 100.0).round(3)\n",
    "    op[\"p_any_adj\"] = (op[\"p_any_adj\"] * 100.0).round(3)\n",
    "    op_sorted_l30 = op.sort_values(\"p_late30_adj\", ascending=False)\n",
    "    op_sorted_any = op.sort_values(\"p_any_adj\", ascending=False)\n",
    "\n",
    "    op_top15_l30 = op_sorted_l30.head(15)\n",
    "    op_bottom15_l30 = op_sorted_l30.tail(15).sort_values(\"p_late30_adj\", ascending=True)\n",
    "    op_top15_any = op_sorted_any.head(15)\n",
    "    op_bottom15_any = op_sorted_any.tail(15).sort_values(\"p_any_adj\", ascending=True)\n",
    "\n",
    "    (OUTPUT_DIR / \"operator_adjusted\").mkdir(exist_ok=True)\n",
    "    op.to_csv(OUTPUT_DIR / \"operator_adjusted\" / \"operator_adjusted_pred_probs.csv\", index=False)\n",
    "    op_top15_l30.to_csv(OUTPUT_DIR / \"operator_adjusted\" / \"operator_top15_late30.csv\", index=False)\n",
    "    op_bottom15_l30.to_csv(OUTPUT_DIR / \"operator_adjusted\" / \"operator_bottom15_late30.csv\", index=False)\n",
    "    op_top15_any.to_csv(OUTPUT_DIR / \"operator_adjusted\" / \"operator_top15_any_delay.csv\", index=False)\n",
    "    op_bottom15_any.to_csv(OUTPUT_DIR / \"operator_adjusted\" / \"operator_bottom15_any_delay.csv\", index=False)\n",
    "\n",
    "    var_lines.append(\"### Operators (min n>=50): adjusted predicted probabilities\\n\")\n",
    "    var_lines.append(\"Top/bottom 15 for late30 and any_delay saved as CSVs under `step2_model_outputs/operator_adjusted/`.\\n\")\n",
    "else:\n",
    "    var_lines.append(\"Operator column not available; operator variation deliverables skipped.\\n\")\n",
    "\n",
    "# County adjusted predicted probabilities for late30 and late90 (min n>=50)\n",
    "if df[\"county_norm\"].notna().any():\n",
    "    df[\"p_late90_hat\"] = m_l90.predict(df)\n",
    "    cty = df.groupby(\"county_norm\").agg(n_total=(\"late30\", \"size\"), p_late30_adj=(\"p_late30_hat\", \"mean\"), p_late90_adj=(\"p_late90_hat\", \"mean\")).reset_index()\n",
    "    cty = cty[cty[\"n_total\"] >= 50].copy()\n",
    "    cty[\"p_late30_adj\"] = (cty[\"p_late30_adj\"] * 100.0).round(3)\n",
    "    cty[\"p_late90_adj\"] = (cty[\"p_late90_adj\"] * 100.0).round(3)\n",
    "    cty_sorted = cty.sort_values(\"p_late30_adj\", ascending=False)\n",
    "    cty.to_csv(OUTPUT_DIR / \"county_adjusted_pred_probs.csv\", index=False)\n",
    "    var_lines.append(\"### Counties (min n>=50): adjusted predicted P(late30) and P(late90)\\n\")\n",
    "    var_lines.append(_safe_to_markdown(cty_sorted.head(20), index=False))\n",
    "    var_lines.append(\"\\nChoropleth map skipped (no county geometry join detected in this notebook).\\n\")\n",
    "else:\n",
    "    var_lines.append(\"County column not available; county variation deliverables skipped.\\n\")\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 6) Robustness set\n",
    "# ================================\n",
    "rob_lines = []\n",
    "rob_lines.append(\"## Robustness set (must include)\\n\")\n",
    "\n",
    "# Continuous model variants\n",
    "rob_ols = []\n",
    "m_base_op = m_h2_ols_op\n",
    "rob_ols.append({\"variant\": \"baseline\", \"post\": m_base_op.params.get(\"post\", np.nan), \"post_se\": m_base_op.bse.get(\"post\", np.nan), \"post:historical\": m_base_op.params.get(\"post:historical\", np.nan), \"int_se\": m_base_op.bse.get(\"post:historical\", np.nan)})\n",
    "\n",
    "df_cap = df.copy()\n",
    "df_cap[\"log1p_delay\"] = df_cap[\"log1p_delay_cap180\"]\n",
    "m_cap_op = _fit_ols_safe(h2_ols_formula.replace(\"log1p_delay\", \"log1p_delay\"), df_cap, cluster_by=\"operator\")\n",
    "rob_ols.append({\"variant\": \"cap180\", \"post\": m_cap_op.params.get(\"post\", np.nan), \"post_se\": m_cap_op.bse.get(\"post\", np.nan), \"post:historical\": m_cap_op.params.get(\"post:historical\", np.nan), \"int_se\": m_cap_op.bse.get(\"post:historical\", np.nan)})\n",
    "\n",
    "df_no99 = df[df[\"extreme99\"] == 0].copy()\n",
    "m_no99_op = _fit_ols_safe(h2_ols_formula, df_no99, cluster_by=\"operator\")\n",
    "rob_ols.append({\"variant\": \"exclude_extreme99\", \"post\": m_no99_op.params.get(\"post\", np.nan), \"post_se\": m_no99_op.bse.get(\"post\", np.nan), \"post:historical\": m_no99_op.params.get(\"post:historical\", np.nan), \"int_se\": m_no99_op.bse.get(\"post:historical\", np.nan)})\n",
    "\n",
    "rob_ols = pd.DataFrame(rob_ols)\n",
    "rob_ols[[\"post\", \"post_se\", \"post:historical\", \"int_se\"]] = rob_ols[[\"post\", \"post_se\", \"post:historical\", \"int_se\"]].round(6)\n",
    "rob_lines.append(\"### Continuous model (cluster by operator): compare key terms\\n\")\n",
    "rob_lines.append(_safe_to_markdown(rob_ols, index=False))\n",
    "rob_lines.append(\"\")\n",
    "\n",
    "# Key logit: late30 variants\n",
    "rob_l30 = []\n",
    "rob_l30.append({\"variant\": \"baseline\", \"post\": m_l30.params.get(\"post\", np.nan), \"post_se\": m_l30.bse.get(\"post\", np.nan), \"post:historical\": m_l30.params.get(\"post:historical\", np.nan), \"int_se\": m_l30.bse.get(\"post:historical\", np.nan)})\n",
    "\n",
    "m_l30_cap, _ = _fit_logit_safe([f_late30, \"late30 ~ post + historical + post:historical\"], df_cap, _cov_kwargs(df_cap, cluster_by=\"operator\"), \"late30 cap180\")\n",
    "rob_l30.append({\"variant\": \"cap180 (affects DV only via delay; late30 unchanged)\", \"post\": m_l30_cap.params.get(\"post\", np.nan), \"post_se\": m_l30_cap.bse.get(\"post\", np.nan), \"post:historical\": m_l30_cap.params.get(\"post:historical\", np.nan), \"int_se\": m_l30_cap.bse.get(\"post:historical\", np.nan)})\n",
    "\n",
    "m_l30_no99, _ = _fit_logit_safe([f_late30, \"late30 ~ post + historical + post:historical\"], df_no99, _cov_kwargs(df_no99, cluster_by=\"operator\"), \"late30 exclude99\")\n",
    "rob_l30.append({\"variant\": \"exclude_extreme99\", \"post\": m_l30_no99.params.get(\"post\", np.nan), \"post_se\": m_l30_no99.bse.get(\"post\", np.nan), \"post:historical\": m_l30_no99.params.get(\"post:historical\", np.nan), \"int_se\": m_l30_no99.bse.get(\"post:historical\", np.nan)})\n",
    "\n",
    "rob_l30 = pd.DataFrame(rob_l30)\n",
    "rob_l30[[\"post\", \"post_se\", \"post:historical\", \"int_se\"]] = rob_l30[[\"post\", \"post_se\", \"post:historical\", \"int_se\"]].round(6)\n",
    "rob_lines.append(\"### Key logit (late30): compare key terms\\n\")\n",
    "rob_lines.append(_safe_to_markdown(rob_l30, index=False))\n",
    "rob_lines.append(\"\")\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 7) Write final report + save key tables\n",
    "# ================================\n",
    "lines = []\n",
    "lines.append(\"# Step 2B final models (Colorado spills mission-change study)\\n\")\n",
    "lines.append(\"This notebook cell produces the final, defensible model set for H1â€“H3 while keeping the full sample and avoiding separation in the extreme tail (>=99 days).\\n\")\n",
    "lines.append(\"\\n\")\n",
    "lines.extend(h1_lines)\n",
    "lines.append(\"\\n\")\n",
    "lines.extend(h2_lines)\n",
    "lines.append(\"\\n\")\n",
    "lines.extend(h3_lines)\n",
    "lines.append(\"\\n\")\n",
    "lines.extend(rare_lines)\n",
    "lines.append(\"\\n\")\n",
    "lines.extend(var_lines)\n",
    "lines.append(\"\\n\")\n",
    "lines.extend(rob_lines)\n",
    "lines.append(\"\\n\")\n",
    "lines.append(\"## Outputs\\n\")\n",
    "lines.append(f\"- Report: `{REPORT_PATH}`\\n\")\n",
    "lines.append(f\"- Plots/tables: `{OUTPUT_DIR}`\\n\")\n",
    "\n",
    "REPORT_PATH.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "\n",
    "# Save key tables\n",
    "h1_logit_tab.to_csv(OUTPUT_DIR / \"h1_baseline_logit_or.csv\", index=False)\n",
    "if not h1_lpm_tab.empty:\n",
    "    h1_lpm_tab.to_csv(OUTPUT_DIR / \"h1_lpm_operator_fe.csv\", index=False)\n",
    "h2_ols_tab_op.to_csv(OUTPUT_DIR / \"h2_ols_log1p_cluster_operator.csv\", index=False)\n",
    "h2_ols_tab_cty.to_csv(OUTPUT_DIR / \"h2_ols_log1p_cluster_county.csv\", index=False)\n",
    "any_tab.to_csv(OUTPUT_DIR / \"h2_any_delay_logit_or.csv\", index=False)\n",
    "pos_tab.to_csv(OUTPUT_DIR / \"h2_conditional_delay_ols.csv\", index=False)\n",
    "l30_tab.to_csv(OUTPUT_DIR / \"h2_late30_logit_or.csv\", index=False)\n",
    "l90_tab.to_csv(OUTPUT_DIR / \"h2_late90_logit_or.csv\", index=False)\n",
    "pred_thresh.to_csv(OUTPUT_DIR / \"h2_pred_late30_late90_prepost_by_type.csv\", index=False)\n",
    "mech.to_csv(OUTPUT_DIR / \"h2_mechanism_decomposition.csv\", index=False)\n",
    "rob_ols.to_csv(OUTPUT_DIR / \"robustness_continuous_key_terms.csv\", index=False)\n",
    "rob_l30.to_csv(OUTPUT_DIR / \"robustness_late30_key_terms.csv\", index=False)\n",
    "if ridge_available and not pred_extreme99.empty:\n",
    "    pred_extreme99.to_csv(OUTPUT_DIR / \"extreme99_ridge_pred_prepost_by_type.csv\", index=False)\n",
    "\n",
    "print(f\"Wrote report: {REPORT_PATH}\")\n",
    "print(f\"Wrote outputs under: {OUTPUT_DIR}\")\n",
    "\n",
    "# Return a couple headline tables\n",
    "mix_overall, pred_h1, pred_thresh, mech, (pred_extreme99 if ridge_available else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4e39e8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote paper tables: /home/dadams/Repos/colorado_redux/step2_paper_tables.md\n",
      "Read CSV artifacts from: /home/dadams/Repos/colorado_redux/analysis_postgis/step2_model_outputs\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import io\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# --- Paths (reuse variables from earlier cells if present) ---\n",
    "repo_root = globals().get(\"repo_root\", Path.cwd())\n",
    "OUTPUT_DIR = globals().get(\n",
    "    \"OUTPUT_DIR\", repo_root / \"analysis_postgis\" / \"step2_model_outputs\"\n",
    ")\n",
    "REPORT_PATH = globals().get(\"REPORT_PATH\", repo_root / \"step2_final_models.md\")\n",
    "TABLES_PATH = repo_root / \"step2_paper_tables.md\"\n",
    "\n",
    "\n",
    "# --- Formatting helpers (journal-friendly) ---\n",
    "TERM_LABELS = {\n",
    "    \"post\": \"Post\",\n",
    "    \"historical\": \"Historical\",\n",
    "    \"post:historical\": \"Post Ã— Historical\",\n",
    "}\n",
    "\n",
    "\n",
    "def fmt_int(x) -> str:\n",
    "    if pd.isna(x):\n",
    "        return \"\"\n",
    "    return str(int(round(float(x))))\n",
    "\n",
    "\n",
    "def fmt_num(x, nd: int = 3) -> str:\n",
    "    if pd.isna(x):\n",
    "        return \"\"\n",
    "    return f\"{float(x):.{nd}f}\"\n",
    "\n",
    "\n",
    "def fmt_pct(x, nd: int = 1) -> str:\n",
    "    if pd.isna(x):\n",
    "        return \"\"\n",
    "    return f\"{float(x):.{nd}f}\"\n",
    "\n",
    "\n",
    "def fmt_p(p) -> str:\n",
    "    if pd.isna(p):\n",
    "        return \"\"\n",
    "    p = float(p)\n",
    "    if p < 0.001:\n",
    "        return \"<0.001\"\n",
    "    return f\"{p:.3f}\"\n",
    "\n",
    "\n",
    "def stars(p) -> str:\n",
    "    if pd.isna(p):\n",
    "        return \"\"\n",
    "    p = float(p)\n",
    "    if p < 0.01:\n",
    "        return \"***\"\n",
    "    if p < 0.05:\n",
    "        return \"**\"\n",
    "    if p < 0.10:\n",
    "        return \"*\"\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "# --- Lightweight Markdown table writer (no tabulate dependency) ---\n",
    "def df_to_md(df: pd.DataFrame, *, index: bool = False, max_rows: int | None = None) -> str:\n",
    "    if df is None or df.empty:\n",
    "        return \"\"\n",
    "    d = df.copy()\n",
    "    if not index:\n",
    "        d = d.reset_index(drop=True)\n",
    "    if max_rows is not None:\n",
    "        d = d.head(max_rows)\n",
    "\n",
    "    cols = list(map(str, d.columns))\n",
    "    header = \"| \" + \" | \".join(cols) + \" |\"\n",
    "    sep = \"| \" + \" | \".join([\"---\"] * len(cols)) + \" |\"\n",
    "    rows = [\n",
    "        \"| \" + \" | \".join(\"\" if pd.isna(v) else str(v) for v in row) + \" |\"\n",
    "        for row in d.itertuples(index=False, name=None)\n",
    "    ]\n",
    "    return \"\\n\".join([header, sep, *rows])\n",
    "\n",
    "\n",
    "def _read_csv(filename: str) -> pd.DataFrame | None:\n",
    "    p = OUTPUT_DIR / filename\n",
    "    if not p.exists():\n",
    "        return None\n",
    "    return pd.read_csv(p)\n",
    "\n",
    "\n",
    "def _core_terms(df: pd.DataFrame, *, term_col: str = \"term\") -> pd.DataFrame:\n",
    "    if df is None or df.empty:\n",
    "        return df\n",
    "    keep = {\"post\", \"historical\", \"post:historical\"}\n",
    "    if term_col in df.columns:\n",
    "        return df[df[term_col].isin(keep)].copy()\n",
    "    return df\n",
    "\n",
    "\n",
    "def format_coef_table(df: pd.DataFrame, *, term_col: str = \"term\") -> pd.DataFrame:\n",
    "    if df is None or df.empty:\n",
    "        return df\n",
    "    d = df.copy()\n",
    "    if term_col in d.columns:\n",
    "        d[\"Term\"] = d[term_col].map(lambda t: TERM_LABELS.get(str(t), str(t)))\n",
    "    else:\n",
    "        d[\"Term\"] = \"\"\n",
    "\n",
    "    if {\"coef\", \"se\"}.issubset(d.columns):\n",
    "        pcol = \"p\" if \"p\" in d.columns else (\"p_value\" if \"p_value\" in d.columns else None)\n",
    "        pvals = d[pcol] if pcol else pd.Series([pd.NA] * len(d))\n",
    "        d[\"Estimate (SE)\"] = [\n",
    "            f\"{fmt_num(b, 3)}{stars(p)} ({fmt_num(s, 3)})\" for b, s, p in zip(d[\"coef\"], d[\"se\"], pvals)\n",
    "        ]\n",
    "        d[\"p\"] = [fmt_p(p) for p in pvals]\n",
    "        return d[[\"Term\", \"Estimate (SE)\", \"p\"]].copy()\n",
    "\n",
    "    return d\n",
    "\n",
    "\n",
    "def format_or_table(df: pd.DataFrame, *, term_col: str = \"term\") -> pd.DataFrame:\n",
    "    if df is None or df.empty:\n",
    "        return df\n",
    "    d = df.copy()\n",
    "    if term_col in d.columns:\n",
    "        d[\"Term\"] = d[term_col].map(lambda t: TERM_LABELS.get(str(t), str(t)))\n",
    "    else:\n",
    "        d[\"Term\"] = \"\"\n",
    "\n",
    "    pcol = \"p\" if \"p\" in d.columns else (\"p_value\" if \"p_value\" in d.columns else None)\n",
    "    pvals = d[pcol] if pcol else pd.Series([pd.NA] * len(d))\n",
    "\n",
    "    if {\"OR\", \"CI95_lo\", \"CI95_hi\"}.issubset(d.columns):\n",
    "        orv, lo, hi = d[\"OR\"], d[\"CI95_lo\"], d[\"CI95_hi\"]\n",
    "    elif {\"odds_ratio\", \"or_ci95_lo\", \"or_ci95_hi\"}.issubset(d.columns):\n",
    "        orv, lo, hi = d[\"odds_ratio\"], d[\"or_ci95_lo\"], d[\"or_ci95_hi\"]\n",
    "    else:\n",
    "        return d\n",
    "\n",
    "    d[\"OR [95% CI]\"] = [\n",
    "        f\"{fmt_num(o, 3)}{stars(p)} [{fmt_num(l, 3)}, {fmt_num(h, 3)}]\" for o, l, h, p in zip(orv, lo, hi, pvals)\n",
    "    ]\n",
    "    d[\"p\"] = [fmt_p(p) for p in pvals]\n",
    "    return d[[\"Term\", \"OR [95% CI]\", \"p\"]].copy()\n",
    "\n",
    "\n",
    "def format_simple_pct_table(df: pd.DataFrame, cols_pct: list[str], cols_int: list[str] | None = None) -> pd.DataFrame:\n",
    "    if df is None or df.empty:\n",
    "        return df\n",
    "    d = df.copy()\n",
    "    cols_int = cols_int or []\n",
    "    for c in cols_pct:\n",
    "        if c in d.columns:\n",
    "            d[c] = d[c].map(lambda v: fmt_pct(v, 1))\n",
    "    for c in cols_int:\n",
    "        if c in d.columns:\n",
    "            d[c] = d[c].map(fmt_int)\n",
    "    return d\n",
    "\n",
    "\n",
    "# --- Descriptives (traditional Table 1 style) ---\n",
    "def _pick_col(df: pd.DataFrame, candidates: list[str | None]) -> str | None:\n",
    "    for c in candidates:\n",
    "        if c and c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "\n",
    "def _get_base_df() -> pd.DataFrame | None:\n",
    "    candidates: list[tuple[str, object]] = [\n",
    "        (\"spills_gdf\", globals().get(\"spills_gdf\")),\n",
    "        (\"df_spills\", globals().get(\"df_spills\")),\n",
    "        (\"df\", globals().get(\"df\")),\n",
    "        (\"raw\", globals().get(\"raw\")),\n",
    "        (\"data\", globals().get(\"data\")),\n",
    "    ]\n",
    "\n",
    "    delay_hint = globals().get(\"delay_col\")\n",
    "\n",
    "    def usable(d: pd.DataFrame) -> bool:\n",
    "        delay_col = _pick_col(\n",
    "            d,\n",
    "            [delay_hint, \"delay_days\", \"report_delay_days\", \"delay\", \"delay_day\", \"days_delay\"],\n",
    "        )\n",
    "        period_col = _pick_col(d, [\"period\", \"Period\"])\n",
    "        return (delay_col is not None) and (period_col is not None)\n",
    "\n",
    "    for _, obj in candidates:\n",
    "        if obj is None:\n",
    "            continue\n",
    "        try:\n",
    "            d = pd.DataFrame(obj)\n",
    "        except Exception:\n",
    "            continue\n",
    "        if usable(d):\n",
    "            return d.copy()\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def _descriptives_table(df: pd.DataFrame) -> dict[str, pd.DataFrame]:\n",
    "    period_col = _pick_col(df, [\"period\", \"Period\"])\n",
    "\n",
    "    spill_type_hint = globals().get(\"spill_type_col\")\n",
    "    spill_type_col = _pick_col(df, [spill_type_hint, \"spill_type\", \"spill_type_col\"])\n",
    "\n",
    "    rurality_hint = globals().get(\"rurality_col\")\n",
    "    rurality_col = _pick_col(df, [rurality_hint, \"rurality_3\", \"ruca_group\", \"rurality\"])\n",
    "\n",
    "    county_hint = globals().get(\"county_col\")\n",
    "    county_col = _pick_col(df, [county_hint, \"county_norm\", \"county\"])\n",
    "\n",
    "    operator_hint = globals().get(\"operator_col\")\n",
    "    operator_col = _pick_col(df, [operator_hint, \"operator\"])\n",
    "\n",
    "    delay_hint = globals().get(\"delay_col\")\n",
    "    delay_col = _pick_col(\n",
    "        df,\n",
    "        [delay_hint, \"delay_days\", \"report_delay_days\", \"delay\", \"delay_day\", \"days_delay\"],\n",
    "    )\n",
    "\n",
    "    if period_col is None or delay_col is None:\n",
    "        return {\"D1_period\": pd.DataFrame(), \"D1_type_period\": pd.DataFrame()}\n",
    "\n",
    "    if spill_type_col is not None:\n",
    "        df[spill_type_col] = df[spill_type_col].astype(str)\n",
    "\n",
    "    df[delay_col] = pd.to_numeric(df[delay_col], errors=\"coerce\")\n",
    "    d = df[df[delay_col].notna()].copy()\n",
    "\n",
    "    def summarize(g: pd.DataFrame) -> dict:\n",
    "        x = g[delay_col]\n",
    "        mean = float(x.mean())\n",
    "        sd = float(x.std(ddof=1))\n",
    "        median = float(x.median())\n",
    "        p25 = float(x.quantile(0.25))\n",
    "        p75 = float(x.quantile(0.75))\n",
    "\n",
    "        out = {\n",
    "            \"n\": int(len(g)),\n",
    "            \"mean (SD)\": f\"{mean:.2f} ({sd:.2f})\",\n",
    "            \"median [IQR]\": f\"{median:.0f} [{p25:.0f}, {p75:.0f}]\",\n",
    "            \"pct_delay_0\": 100.0 * float((x == 0).mean()),\n",
    "            \"pct_delay_le1\": 100.0 * float((x <= 1).mean()),\n",
    "            \"pct_any_delay\": 100.0 * float((x > 0).mean()),\n",
    "            \"pct_late30\": 100.0 * float((x >= 30).mean()),\n",
    "            \"pct_late90\": 100.0 * float((x >= 90).mean()),\n",
    "            \"pct_ge99\": 100.0 * float((x >= 99).mean()),\n",
    "        }\n",
    "        if spill_type_col is not None and spill_type_col in g.columns:\n",
    "            out[\"pct_historical\"] = 100.0 * float((g[spill_type_col] == \"Historical\").mean())\n",
    "        if operator_col is not None and operator_col in g.columns:\n",
    "            out[\"n_operators\"] = int(g[operator_col].nunique(dropna=True))\n",
    "        if county_col is not None and county_col in g.columns:\n",
    "            out[\"n_counties\"] = int(g[county_col].nunique(dropna=True))\n",
    "        if rurality_col is not None and rurality_col in g.columns:\n",
    "            out[\"n_rurality\"] = int(g[rurality_col].nunique(dropna=True))\n",
    "        return out\n",
    "\n",
    "    D1_period = (\n",
    "        d.groupby(period_col, dropna=False)\n",
    "        .apply(lambda g: pd.Series(summarize(g)), include_groups=False)\n",
    "        .reset_index()\n",
    "        .rename(columns={period_col: \"period\"})\n",
    "    )\n",
    "\n",
    "    if spill_type_col is not None and spill_type_col in d.columns:\n",
    "        D1_type_period = (\n",
    "            d.groupby([spill_type_col, period_col], dropna=False)\n",
    "            .apply(lambda g: pd.Series(summarize(g)), include_groups=False)\n",
    "            .reset_index()\n",
    "            .rename(columns={spill_type_col: \"spill_type\", period_col: \"period\"})\n",
    "        )\n",
    "        if \"pct_historical\" in D1_type_period.columns:\n",
    "            D1_type_period = D1_type_period.drop(columns=[\"pct_historical\"])\n",
    "    else:\n",
    "        D1_type_period = pd.DataFrame()\n",
    "\n",
    "    prefer = [\n",
    "        \"spill_type\",\n",
    "        \"period\",\n",
    "        \"n\",\n",
    "        \"pct_historical\",\n",
    "        \"mean (SD)\",\n",
    "        \"median [IQR]\",\n",
    "        \"pct_delay_0\",\n",
    "        \"pct_delay_le1\",\n",
    "        \"pct_any_delay\",\n",
    "        \"pct_late30\",\n",
    "        \"pct_late90\",\n",
    "        \"pct_ge99\",\n",
    "        \"n_operators\",\n",
    "        \"n_counties\",\n",
    "        \"n_rurality\",\n",
    "    ]\n",
    "\n",
    "    def reorder(df_: pd.DataFrame) -> pd.DataFrame:\n",
    "        if df_ is None or df_.empty:\n",
    "            return df_\n",
    "        cols = [c for c in prefer if c in df_.columns] + [c for c in df_.columns if c not in prefer]\n",
    "        out = df_[cols].copy()\n",
    "        for c in [c for c in out.columns if c.startswith(\"pct_\")]:\n",
    "            out[c] = out[c].map(lambda v: fmt_pct(v, 1))\n",
    "        if \"n\" in out.columns:\n",
    "            out[\"n\"] = out[\"n\"].map(fmt_int)\n",
    "        for c in [\"n_operators\", \"n_counties\", \"n_rurality\"]:\n",
    "            if c in out.columns:\n",
    "                out[c] = out[c].map(fmt_int)\n",
    "        return out\n",
    "\n",
    "    return {\"D1_period\": reorder(D1_period), \"D1_type_period\": reorder(D1_type_period)}\n",
    "\n",
    "\n",
    "# --- Parsers (fallback when a specific table was not exported as CSV) ---\n",
    "def _parse_pred_pct_historical_from_report(report_path: Path) -> pd.DataFrame | None:\n",
    "    if not report_path.exists():\n",
    "        return None\n",
    "    txt = report_path.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines()\n",
    "    for i, line in enumerate(txt):\n",
    "        if \"pred_pct_historical\" in line:\n",
    "            block = [line]\n",
    "            j = i + 1\n",
    "            while j < len(txt) and txt[j].strip() and \"```\" not in txt[j]:\n",
    "                block.append(txt[j])\n",
    "                if len(block) >= 4:\n",
    "                    break\n",
    "                j += 1\n",
    "            raw = \"\\n\".join(block)\n",
    "            try:\n",
    "                d = pd.read_fwf(io.StringIO(raw))\n",
    "                if \"pred_pct_historical\" in d.columns:\n",
    "                    if \"period\" in d.columns:\n",
    "                        out = d[[\"period\", \"pred_pct_historical\"]].copy()\n",
    "                    else:\n",
    "                        cols = list(d.columns)\n",
    "                        out = d[[cols[1], \"pred_pct_historical\"]].copy()\n",
    "                        out.columns = [\"period\", \"pred_pct_historical\"]\n",
    "                    return out\n",
    "            except Exception:\n",
    "                continue\n",
    "    return None\n",
    "\n",
    "\n",
    "def _parse_h3_joint_tests_from_report(report_path: Path) -> pd.DataFrame | None:\n",
    "    if not report_path.exists():\n",
    "        return None\n",
    "\n",
    "    current = None\n",
    "    rows = []\n",
    "    lines = report_path.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines()\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith(\"any_delay model formula\"):\n",
    "            current = \"any_delay\"\n",
    "        elif line.startswith(\"log1p_delay model formula\"):\n",
    "            current = \"log1p_delay\"\n",
    "        elif line.startswith(\"late90 model formula\"):\n",
    "            current = \"late90\"\n",
    "\n",
    "        if line.startswith(\"Joint test postÃ—rurality\") and current is not None:\n",
    "            m = re.search(r\"\\(([^)]+)\\):\\s*(.+)$\", line)\n",
    "            if not m:\n",
    "                continue\n",
    "            test_type = m.group(1).strip()\n",
    "            rest = m.group(2)\n",
    "\n",
    "            stat = None\n",
    "            df = None\n",
    "            p = None\n",
    "\n",
    "            m_chi2 = re.search(r\"chi2=([^,]+)\", rest)\n",
    "            m_F = re.search(r\"F=([^,]+)\", rest)\n",
    "            m_df = re.search(r\"df=(.*?),\\s*p=\", rest)\n",
    "            m_p = re.search(r\"p=([^,;]+)\", rest)\n",
    "\n",
    "            if m_chi2:\n",
    "                stat = m_chi2.group(1).strip()\n",
    "            elif m_F:\n",
    "                stat = m_F.group(1).strip()\n",
    "\n",
    "            if m_df:\n",
    "                df = m_df.group(1).strip()\n",
    "            if m_p:\n",
    "                p = m_p.group(1).strip()\n",
    "\n",
    "            rows.append({\"Outcome\": current, \"Test\": test_type, \"Statistic\": stat, \"df\": df, \"p\": p})\n",
    "\n",
    "    if not rows:\n",
    "        return None\n",
    "\n",
    "    out = pd.DataFrame(rows)\n",
    "    out[\"Outcome\"] = out[\"Outcome\"].map(lambda x: {\"any_delay\": \"Any delay\", \"log1p_delay\": \"log(1+delay)\", \"late90\": \"Late â‰¥90\"}.get(x, x))\n",
    "    out[\"Statistic\"] = out[\"Statistic\"].map(lambda v: fmt_num(v, 3) if v not in (None, \"\") else \"\")\n",
    "    out[\"p\"] = out[\"p\"].map(lambda v: fmt_p(float(v)) if v not in (None, \"\") else \"\")\n",
    "    return out\n",
    "\n",
    "\n",
    "def _parse_h3_recent_gradient_from_report(report_path: Path) -> pd.DataFrame | None:\n",
    "    if not report_path.exists():\n",
    "        return None\n",
    "\n",
    "    lines = report_path.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines()\n",
    "    start = None\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.strip().startswith(\"rurality_3\") and \"median_delay_implied\" in line:\n",
    "            start = i\n",
    "            break\n",
    "    if start is None:\n",
    "        return None\n",
    "\n",
    "    block = []\n",
    "    for j in range(start, len(lines)):\n",
    "        if lines[j].strip().startswith(\"```\"):\n",
    "            break\n",
    "        block.append(lines[j])\n",
    "\n",
    "    raw = \"\\n\".join(block)\n",
    "    try:\n",
    "        d = pd.read_fwf(io.StringIO(raw))\n",
    "        if \"median_delay_implied\" in d.columns:\n",
    "            d = d.rename(columns={\"median_delay_implied\": \"implied_mean_delay\"})\n",
    "        # Journal-friendly rounding\n",
    "        if \"implied_mean_delay\" in d.columns:\n",
    "            d[\"implied_mean_delay\"] = d[\"implied_mean_delay\"].map(lambda v: fmt_num(v, 2))\n",
    "        for c in [\"p_any_delay\", \"p_late90\", \"median_any_delay\", \"median_late90\"]:\n",
    "            if c in d.columns:\n",
    "                d[c] = d[c].map(lambda v: fmt_pct(v, 1))\n",
    "        return d\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Load CSV-backed tables ---\n",
    "H1A = _read_csv(\"H1_mix_overall.csv\")\n",
    "H1B_or = _core_terms(_read_csv(\"h1_baseline_logit_or.csv\"))\n",
    "H1C = _core_terms(_read_csv(\"h1_lpm_operator_fe.csv\"))\n",
    "\n",
    "H2A_op = _core_terms(_read_csv(\"h2_ols_log1p_cluster_operator.csv\"))\n",
    "H2A_cty = _core_terms(_read_csv(\"h2_ols_log1p_cluster_county.csv\"))\n",
    "R1 = _read_csv(\"robustness_continuous_key_terms.csv\")\n",
    "\n",
    "H2B_any = _core_terms(_read_csv(\"h2_any_delay_logit_or.csv\"))\n",
    "H2B_pos = _core_terms(_read_csv(\"h2_conditional_delay_ols.csv\"), term_col=\"term\")\n",
    "H2C = _read_csv(\"h2_mechanism_decomposition.csv\")\n",
    "\n",
    "H2D = _read_csv(\"h2_pred_late30_late90_prepost_by_type.csv\")\n",
    "H2E_l30 = _core_terms(_read_csv(\"h2_late30_logit_or.csv\"))\n",
    "H2E_l90 = _core_terms(_read_csv(\"h2_late90_logit_or.csv\"))\n",
    "\n",
    "E99 = _read_csv(\"extreme99_ridge_pred_prepost_by_type.csv\")\n",
    "\n",
    "VAR_A = _read_csv(\"county_adjusted_pred_probs.csv\")\n",
    "OP_TOP_ANY = _read_csv(\"operator_adjusted/operator_top15_any_delay.csv\")\n",
    "OP_BOT_ANY = _read_csv(\"operator_adjusted/operator_bottom15_any_delay.csv\")\n",
    "OP_TOP_L30 = _read_csv(\"operator_adjusted/operator_top15_late30.csv\")\n",
    "OP_BOT_L30 = _read_csv(\"operator_adjusted/operator_bottom15_late30.csv\")\n",
    "\n",
    "\n",
    "# --- Fallbacks (from report) ---\n",
    "H1B_pred = _parse_pred_pct_historical_from_report(REPORT_PATH)\n",
    "H3A = _parse_h3_joint_tests_from_report(REPORT_PATH)\n",
    "H3B = _parse_h3_recent_gradient_from_report(REPORT_PATH)\n",
    "\n",
    "\n",
    "# --- Build descriptives tables from in-memory data ---\n",
    "_base_df = _get_base_df()\n",
    "D1 = _descriptives_table(_base_df) if _base_df is not None else {\"D1_period\": pd.DataFrame(), \"D1_type_period\": pd.DataFrame()}\n",
    "D1_period = D1.get(\"D1_period\")\n",
    "D1_type_period = D1.get(\"D1_type_period\")\n",
    "\n",
    "\n",
    "# --- Format tables into journal-friendly presentation ---\n",
    "# H1-A\n",
    "if H1A is not None and {\"period\", \"n_total\", \"pct_historical\"}.issubset(H1A.columns):\n",
    "    H1A = H1A[[\"period\", \"n_total\", \"pct_historical\"]].copy()\n",
    "    H1A[\"n_total\"] = H1A[\"n_total\"].map(fmt_int)\n",
    "    H1A[\"pct_historical\"] = H1A[\"pct_historical\"].map(lambda v: fmt_pct(v, 1))\n",
    "\n",
    "# H1-B OR (post only)\n",
    "if H1B_or is not None and not H1B_or.empty and \"term\" in H1B_or.columns:\n",
    "    H1B_or = H1B_or[H1B_or[\"term\"].eq(\"post\")].copy()\n",
    "    H1B_or = H1B_or.rename(\n",
    "        columns={\"odds_ratio\": \"OR\", \"or_ci95_lo\": \"CI95_lo\", \"or_ci95_hi\": \"CI95_hi\", \"p_value\": \"p\"}\n",
    "    )\n",
    "    H1B_or = format_or_table(H1B_or)\n",
    "\n",
    "# H1-B predicted % historical\n",
    "if H1B_pred is not None and not H1B_pred.empty:\n",
    "    if \"pred_pct_historical\" in H1B_pred.columns:\n",
    "        H1B_pred[\"pred_pct_historical\"] = H1B_pred[\"pred_pct_historical\"].map(lambda v: fmt_pct(v, 1))\n",
    "\n",
    "# H1-C LPM (post only)\n",
    "if H1C is not None and not H1C.empty and \"term\" in H1C.columns:\n",
    "    H1C = H1C[H1C[\"term\"].eq(\"post\")].copy()\n",
    "    if \"p_value\" in H1C.columns and \"p\" not in H1C.columns:\n",
    "        H1C = H1C.rename(columns={\"p_value\": \"p\"})\n",
    "    H1C = format_coef_table(H1C)\n",
    "\n",
    "# H2-A / H2-A'\n",
    "for name in [\"H2A_op\", \"H2A_cty\"]:\n",
    "    d = globals().get(name)\n",
    "    if d is not None and not d.empty:\n",
    "        if \"p_value\" in d.columns and \"p\" not in d.columns:\n",
    "            d = d.rename(columns={\"p_value\": \"p\"})\n",
    "        d = format_coef_table(d)\n",
    "        globals()[name] = d\n",
    "\n",
    "# Robustness R1\n",
    "if R1 is not None and not R1.empty:\n",
    "    for c in [\"post\", \"post_se\", \"post:historical\", \"int_se\"]:\n",
    "        if c in R1.columns:\n",
    "            R1[c] = R1[c].map(lambda v: fmt_num(v, 3))\n",
    "\n",
    "# H2-B any_delay ORs\n",
    "if H2B_any is not None and not H2B_any.empty:\n",
    "    H2B_any = H2B_any.rename(\n",
    "        columns={\"odds_ratio\": \"OR\", \"or_ci95_lo\": \"CI95_lo\", \"or_ci95_hi\": \"CI95_hi\", \"p_value\": \"p\"}\n",
    "    )\n",
    "    H2B_any = format_or_table(H2B_any)\n",
    "\n",
    "# H2-B conditional OLS\n",
    "if H2B_pos is not None and not H2B_pos.empty:\n",
    "    if \"p_value\" in H2B_pos.columns and \"p\" not in H2B_pos.columns:\n",
    "        H2B_pos = H2B_pos.rename(columns={\"p_value\": \"p\"})\n",
    "    H2B_pos = format_coef_table(H2B_pos)\n",
    "\n",
    "# H2-C mechanism decomposition (probabilities & implied mean delay)\n",
    "if H2C is not None and not H2C.empty:\n",
    "    keep = [\n",
    "        c\n",
    "        for c in [\"spill_type\", \"period\", \"P_any_delay\", \"E_delay_if_pos\", \"median_delay_implied\"]\n",
    "        if c in H2C.columns\n",
    "    ]\n",
    "    if keep:\n",
    "        H2C = H2C[keep].copy()\n",
    "        if \"median_delay_implied\" in H2C.columns:\n",
    "            H2C = H2C.rename(columns={\"median_delay_implied\": \"implied_mean_delay\"})\n",
    "        for c in [\"P_any_delay\"]:\n",
    "            if c in H2C.columns:\n",
    "                H2C[c] = H2C[c].map(lambda v: fmt_num(v, 3))\n",
    "        for c in [\"E_delay_if_pos\", \"implied_mean_delay\"]:\n",
    "            if c in H2C.columns:\n",
    "                H2C[c] = H2C[c].map(lambda v: fmt_num(v, 2))\n",
    "\n",
    "# H2-D predicted probs (late30/late90)\n",
    "if H2D is not None and not H2D.empty:\n",
    "    keep = [c for c in [\"spill_type\", \"period\", \"P_late30\", \"P_late90\"] if c in H2D.columns]\n",
    "    if keep:\n",
    "        H2D = H2D[keep].copy()\n",
    "        for c in [\"P_late30\", \"P_late90\"]:\n",
    "            if c in H2D.columns:\n",
    "                H2D[c] = H2D[c].map(lambda v: fmt_num(v, 6))\n",
    "\n",
    "# H2-E OR tables\n",
    "if H2E_l30 is not None and not H2E_l30.empty:\n",
    "    H2E_l30 = H2E_l30.rename(\n",
    "        columns={\"odds_ratio\": \"OR\", \"or_ci95_lo\": \"CI95_lo\", \"or_ci95_hi\": \"CI95_hi\", \"p_value\": \"p\"}\n",
    "    )\n",
    "    H2E_l30 = format_or_table(H2E_l30)\n",
    "\n",
    "if H2E_l90 is not None and not H2E_l90.empty:\n",
    "    H2E_l90 = H2E_l90.rename(\n",
    "        columns={\"odds_ratio\": \"OR\", \"or_ci95_lo\": \"CI95_lo\", \"or_ci95_hi\": \"CI95_hi\", \"p_value\": \"p\"}\n",
    "    )\n",
    "    H2E_l90 = format_or_table(H2E_l90)\n",
    "\n",
    "# E99\n",
    "if E99 is not None and not E99.empty:\n",
    "    for c in E99.columns:\n",
    "        if c.lower().startswith(\"p_\") or \"prob\" in c.lower() or \"pct\" in c.lower():\n",
    "            E99[c] = E99[c].map(lambda v: fmt_pct(v, 1))\n",
    "        if \"rr\" in c.lower() or \"risk\" in c.lower():\n",
    "            E99[c] = E99[c].map(lambda v: fmt_num(v, 2))\n",
    "\n",
    "# VAR-A county adjusted predicted probs\n",
    "if VAR_A is not None and not VAR_A.empty:\n",
    "    cols = [c for c in [\"county_norm\", \"n_total\", \"p_late30_adj\", \"p_late90_adj\"] if c in VAR_A.columns]\n",
    "    VAR_A = VAR_A[cols].copy()\n",
    "    if \"n_total\" in VAR_A.columns:\n",
    "        VAR_A[\"n_total\"] = VAR_A[\"n_total\"].map(fmt_int)\n",
    "    for c in [\"p_late30_adj\", \"p_late90_adj\"]:\n",
    "        if c in VAR_A.columns:\n",
    "            VAR_A[c] = VAR_A[c].map(lambda v: fmt_pct(v, 2))\n",
    "\n",
    "# Operator risk profiles\n",
    "\n",
    "def format_operator_risk(df: pd.DataFrame, *, outcome: str) -> pd.DataFrame:\n",
    "    if df is None or df.empty:\n",
    "        return df\n",
    "    d = df.copy()\n",
    "    d.insert(0, \"Rank\", range(1, len(d) + 1))\n",
    "    if \"n_total\" in d.columns:\n",
    "        d[\"n_total\"] = d[\"n_total\"].map(fmt_int)\n",
    "\n",
    "    # These are already in percent units in the exported files\n",
    "    for c in [\"p_any_adj\", \"p_late30_adj\", \"p_late90_adj\"]:\n",
    "        if c in d.columns:\n",
    "            d[c] = d[c].map(lambda v: fmt_pct(v, 1))\n",
    "\n",
    "    if outcome == \"any_delay\":\n",
    "        cols = [c for c in [\"Rank\", \"operator\", \"n_total\", \"p_any_adj\"] if c in d.columns]\n",
    "        d = d[cols].copy()\n",
    "        d = d.rename(columns={\"p_any_adj\": \"Adj. P(any delay) (%)\", \"operator\": \"Operator\", \"n_total\": \"N\"})\n",
    "    elif outcome == \"late30\":\n",
    "        cols = [c for c in [\"Rank\", \"operator\", \"n_total\", \"p_late30_adj\", \"p_any_adj\"] if c in d.columns]\n",
    "        d = d[cols].copy()\n",
    "        d = d.rename(\n",
    "            columns={\n",
    "                \"p_late30_adj\": \"Adj. P(late â‰¥30) (%)\",\n",
    "                \"p_any_adj\": \"Adj. P(any delay) (%)\",\n",
    "                \"operator\": \"Operator\",\n",
    "                \"n_total\": \"N\",\n",
    "            }\n",
    "        )\n",
    "    return d\n",
    "\n",
    "\n",
    "OP_TOP_ANY = format_operator_risk(OP_TOP_ANY, outcome=\"any_delay\")\n",
    "OP_BOT_ANY = format_operator_risk(OP_BOT_ANY, outcome=\"any_delay\")\n",
    "OP_TOP_L30 = format_operator_risk(OP_TOP_L30, outcome=\"late30\")\n",
    "OP_BOT_L30 = format_operator_risk(OP_BOT_L30, outcome=\"late30\")\n",
    "\n",
    "\n",
    "# --- Write paper tables markdown ---\n",
    "lines: list[str] = []\n",
    "lines.append(\"# Paper Tables (journal-ready markdown)\")\n",
    "lines.append(\"\")\n",
    "lines.append(f\"Source folder (CSVs): `{OUTPUT_DIR}`\")\n",
    "lines.append(f\"Generated: `{TABLES_PATH}`\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"Significance: * p<0.10, ** p<0.05, *** p<0.01\")\n",
    "lines.append(\"\")\n",
    "\n",
    "lines.append(\"## Table D1: Traditional descriptives\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"(Delay is in days; cells report mean (SD) and median [IQR]. Percentages are percent of spills in group.)\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"### Table D1-A: Delay descriptives by period\")\n",
    "lines.append(df_to_md(D1_period) if D1_period is not None and not D1_period.empty else \"(Could not build descriptives table.)\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"### Table D1-B: Delay descriptives by spill type Ã— period\")\n",
    "lines.append(df_to_md(D1_type_period) if D1_type_period is not None and not D1_type_period.empty else \"(Could not build spill_typeÃ—period descriptives.)\")\n",
    "lines.append(\"\")\n",
    "\n",
    "# Claim 1\n",
    "lines.append(\"## Claim 1: Post-2020 shifts spill composition toward inspection-identified (Historical) spills\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"### Table H1-A: Descriptive mix by period\")\n",
    "lines.append(df_to_md(H1A) if H1A is not None else \"(Missing: H1_mix_overall.csv)\")\n",
    "lines.append(\"\")\n",
    "\n",
    "lines.append(\"### Table H1-B: Spill-level logit (post effect) + predicted pct_historical\")\n",
    "lines.append(\"**Panel 1: Logit OR for Post**\")\n",
    "lines.append(df_to_md(H1B_or) if H1B_or is not None else \"(Missing: h1_baseline_logit_or.csv)\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"**Panel 2: Model-implied pct_historical (marginal)**\")\n",
    "lines.append(df_to_md(H1B_pred) if H1B_pred is not None else \"(Missing: could not parse predicted pct from step2_final_models.md)\")\n",
    "lines.append(\"\")\n",
    "\n",
    "lines.append(\"### Table H1-C (Appendix): Operator FE LPM (post coefficient)\")\n",
    "lines.append(df_to_md(H1C) if H1C is not None else \"(Missing: h1_lpm_operator_fe.csv)\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"Note: Standard errors are clustered by operator.\")\n",
    "lines.append(\"\")\n",
    "\n",
    "# Claim 2\n",
    "lines.append(\"## Claim 2: Typical reporting timeliness improves post-2020\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"### Table H2-A: OLS on log(1+delay) (core terms)\")\n",
    "lines.append(df_to_md(H2A_op) if H2A_op is not None else \"(Missing: h2_ols_log1p_cluster_operator.csv)\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"Note: Standard errors are clustered by operator; model includes operator fixed effects and county/rurality controls.\")\n",
    "lines.append(\"\")\n",
    "\n",
    "lines.append(\"### Table H2-Aâ€² (Appendix): Same model clustered by county\")\n",
    "lines.append(df_to_md(H2A_cty) if H2A_cty is not None else \"(Missing: h2_ols_log1p_cluster_county.csv)\")\n",
    "lines.append(\"\")\n",
    "\n",
    "lines.append(\"### Table R1 (Appendix): Robustness (key terms)\")\n",
    "lines.append(df_to_md(R1) if R1 is not None else \"(Missing: robustness_continuous_key_terms.csv)\")\n",
    "lines.append(\"\")\n",
    "\n",
    "# Claim 3\n",
    "lines.append(\"## Claim 3: Mechanism is mostly fewer any-delay cases; tail behaves differently for Historical\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"### Table H2-B: Two-part model (core terms)\")\n",
    "lines.append(\"**Panel 1: any_delay logit (ORs)**\")\n",
    "lines.append(df_to_md(H2B_any) if H2B_any is not None else \"(Missing: h2_any_delay_logit_or.csv)\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"**Panel 2: conditional delay>0 OLS (coefficients)**\")\n",
    "lines.append(df_to_md(H2B_pos) if H2B_pos is not None else \"(Missing: h2_conditional_delay_ols.csv)\")\n",
    "lines.append(\"\")\n",
    "\n",
    "lines.append(\"### Table H2-C: Mechanism decomposition (by spill type Ã— period)\")\n",
    "lines.append(df_to_md(H2C) if H2C is not None else \"(Missing: h2_mechanism_decomposition.csv)\")\n",
    "lines.append(\"\")\n",
    "\n",
    "# Claim 4\n",
    "lines.append(\"## Claim 4: Extreme delays remain rare but increase post-2020 and are concentrated in Historical cases\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"### Table H2-D: Predicted probabilities for late30 and late90 (by spill type Ã— period)\")\n",
    "lines.append(df_to_md(H2D) if H2D is not None else \"(Missing: h2_pred_late30_late90_prepost_by_type.csv)\")\n",
    "lines.append(\"\")\n",
    "\n",
    "lines.append(\"### Table H2-E (Appendix): late30 and late90 logits (core terms)\")\n",
    "lines.append(\"**Panel 1: late30 logit (ORs)**\")\n",
    "lines.append(df_to_md(H2E_l30) if H2E_l30 is not None else \"(Missing: h2_late30_logit_or.csv)\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"**Panel 2: late90 logit (ORs)**\")\n",
    "lines.append(df_to_md(H2E_l90) if H2E_l90 is not None else \"(Missing: h2_late90_logit_or.csv)\")\n",
    "lines.append(\"\")\n",
    "\n",
    "lines.append(\"### Table E99 (Appendix): Ridge extreme99 predicted probabilities\")\n",
    "lines.append(df_to_md(E99) if E99 is not None else \"(Missing: extreme99_ridge_pred_prepost_by_type.csv)\")\n",
    "lines.append(\"\")\n",
    "\n",
    "# Claim 5\n",
    "lines.append(\"## Claim 5: Geography moderates improvement (tempered rural lag)\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"### Table H3-A: Joint tests for postÃ—rurality\")\n",
    "lines.append(df_to_md(H3A) if H3A is not None else \"(Missing: could not parse H3 joint tests from step2_final_models.md)\")\n",
    "lines.append(\"\")\n",
    "\n",
    "lines.append(\"### Table H3-B: Rurality gradient (Recent spills only)\")\n",
    "lines.append(df_to_md(H3B) if H3B is not None else \"(Missing: could not parse rurality gradient from step2_final_models.md)\")\n",
    "lines.append(\"\")\n",
    "\n",
    "# Claim 6\n",
    "lines.append(\"## Claim 6: Operator + geography variation (profiling deliverables)\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"### Table VAR-A: County adjusted predicted probabilities\")\n",
    "lines.append(df_to_md(VAR_A) if VAR_A is not None else \"(Missing: county_adjusted_pred_probs.csv)\")\n",
    "lines.append(\"\")\n",
    "\n",
    "lines.append(\"### Operator risk profiles (Appendix): Top/Bottom 15\")\n",
    "lines.append(\"**Top 15 any_delay**\")\n",
    "lines.append(df_to_md(OP_TOP_ANY) if OP_TOP_ANY is not None else \"(Missing: operator_adjusted/operator_top15_any_delay.csv)\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"**Bottom 15 any_delay**\")\n",
    "lines.append(df_to_md(OP_BOT_ANY) if OP_BOT_ANY is not None else \"(Missing: operator_adjusted/operator_bottom15_any_delay.csv)\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"**Top 15 late30**\")\n",
    "lines.append(df_to_md(OP_TOP_L30) if OP_TOP_L30 is not None else \"(Missing: operator_adjusted/operator_top15_late30.csv)\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"**Bottom 15 late30**\")\n",
    "lines.append(df_to_md(OP_BOT_L30) if OP_BOT_L30 is not None else \"(Missing: operator_adjusted/operator_bottom15_late30.csv)\")\n",
    "lines.append(\"\")\n",
    "\n",
    "TABLES_PATH.write_text(\"\\n\".join(lines) + \"\\n\", encoding=\"utf-8\")\n",
    "print(f\"Wrote paper tables: {TABLES_PATH}\")\n",
    "print(f\"Read CSV artifacts from: {OUTPUT_DIR}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
